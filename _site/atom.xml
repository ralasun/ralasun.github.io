<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Ralasun Resarch Blog</title>
 <link href="http://localhost:4000/atom.xml" rel="self"/>
 <link href="http://localhost:4000/"/>
 <updated>2021-06-30T17:45:56+09:00</updated>
 <id>http://localhost:4000</id>
 <author>
   <name>Seonhwa Lee</name>
   <email></email>
 </author>

 
 <entry>
   <title>Short-time Fourier Transform(STFT) 과 Discrete Wavelet Transform(DWT)</title>
   <link href="http://localhost:4000/signal%20analysis/2021/06/21/stft-dwt/"/>
   <updated>2021-06-21T00:00:00+09:00</updated>
   <id>http://localhost:4000/signal%20analysis/2021/06/21/stft-dwt</id>
   <content type="html">&lt;p&gt;Fourier Transform의 단점은 무한히 흘러가는 파동에 대한 주파수 분석만 가능하기 때문에 국소적인 시간 부분 단위로는 주파수 분석을 할 수가 없습니다. 따라서 시간-주파수 영역 모두 분석할 수 있는 방법으로 Short-term Fourier Transform과 Discrete Wavelet Transform이 있습니다.&lt;/p&gt;

&lt;h1&gt;Limitation of Fourier Transform&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img width=&quot;500&quot; src=&quot;https://imgur.com/2Xr5hU4.png&quot; /&gt;&lt;/p&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 1. Fourier Transform&lt;/figcaption&gt;

&lt;p&gt;&amp;lt;그림 1.&amp;gt;을 보면, 시간에 따라 주파수가 변화는 신호에 대해서 푸리에 변환은 시간에 따른 변화 정보를 담지 못합니다. 일정한 속도로 진동하는 정현파(sine함수와 cosine함수)가 아니라 갑자기 sharp한 포인트를 가지는 파동같은 경우에도 이러한 변화를 푸리에 변환을 통해선 확인할 수가 없습니다.&lt;/p&gt;

&lt;p&gt;따라서, 일정한 시간 블럭(window)로 나눠서 각각 블럭에 대해 푸리에 변환을 적용한다면 이러한 푸리에 변환의 단점을 어느정도 완화시킬 수 있습니다. 이 방법이 바로 국소 푸리에 변환(Short-Time Fourier Transform, STFT)입니다.&lt;/p&gt;

&lt;h1&gt;Short-Time Fourier Transform(STFT)&lt;/h1&gt;

\[\hat f(t, u) = \int_{-\infty}^{\infty}f(t')w(t'-t)e^{-i2\pi t'u}dt' \tag{1}\]

&lt;p&gt;국소 푸리에 변환은 신호를 슬라이딩 윈도우 기법처럼 특정 길이를 가진 윈도우를 시그널 위에 움직이면서, 각각 윈도우에서 푸리에 변환을 하는 것입니다. 특정 시간 t에 대해서 푸리에 변환을 여러번 계산하게 되는데, 계산한 횟수만큼 평균을 구해서 특정 시간 t에서의 주파수 스펙트럼을 구합니다.&lt;/p&gt;

&lt;p&gt;그렇다면 &lt;span style=&quot;text-decoration: underline&quot;&gt;국소 푸리에 변환의 결과에 영향을 주는 변수&lt;/span&gt;는 어떤걸까요? 바로 &lt;span style=&quot;text-decoration: underline&quot;&gt;&lt;b&gt;슬라이딩 윈도우&lt;/b&gt;&lt;/span&gt;의 크기입니다.&lt;/p&gt;

&lt;h2&gt;Trade-offs between frequency resolution and time resolution&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img width=&quot;500&quot; src=&quot;https://imgur.com/7DE3e69.png&quot; /&gt;&lt;/p&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 2. Trade-Offs between Frequency and Time in STFT&lt;/figcaption&gt;

&lt;p&gt;윈도우 크기가 너무 작으면(narrow window), 주파수 영역의 해상도가 떨어집니다. 조금만 생각하면 이해하기 쉽습니다. 푸리에 변환하는 대상인 윈도우를 크기를 작게해서 변환하는 윈도우 갯수를 늘린다면, 비슷한 시각 근처의 윈도우들의 분석 결과가 대부분 유사할 것입니다. 따라서, 특정 시간 t 주변에 frequency 스펙트럼이 비슷하게 그려지기 때문에, &amp;lt;그림 2.&amp;gt; 처럼 frequency 축으로 ‘spread-out’ 되어 해상도가 떨어집니다. 반대로 윈도우 크기가 너무 크면(broad window), 반대로 주파수 영역 해상도는 올라가지만 time 영역 해상도가 떨어집니다. &amp;lt;그림 2.&amp;gt;를 보시면, 왼쪽그림으로 갈수록 시간 영역에 흐릿한 영역이 넓어지는 것을 확인할 수 있습니다.&lt;/p&gt;

&lt;p&gt;그렇다면, 어떻게 해야 시간 영역과 주파수 영역 모두 해상도를 높게 가져갈 수 있을까요 ? &lt;span style=&quot;text-decoration: underline&quot;&gt;&amp;lt;그림 1.&amp;gt;의 신호에서 진동이 빠른 부분(높은 주파수)은 윈도우 크기를 작게 가져가고, 진동이 느린 부분(낮은 주파수)은 윈도우 크기를 넓게 가져가는 것입니다.&lt;/span&gt; 그러나, STFT는 고정된 윈도우 사이즈에 대해서만 계산이 가능합니다. 이의 단점을 해결한 것이 바로 &lt;b&gt;웨이블릿 변환(Wavelet Transform)&lt;/b&gt;입니다.&lt;/p&gt;

&lt;h1&gt;Wavelet Transform&lt;/h1&gt;

&lt;p&gt;푸리에 변환은 cosine과 sine으로 구성된 기저함수로의 분해입니다. 여기서 cosine과 sine은 정현파로, 시간에 따라 변하지 않고 일정한 속도와 크기로 움직입니다. 반면에 웨이블릿 변환은 ‘웨이블릿(wavelet)’이라는 기저함수로 분해됩니다. Wave는 파동, let은 ‘작다’ 라는 의미로, 작은 파동을 뜻합니다. &lt;span style=&quot;text-decoration: underline&quot;&gt;웨이블릿 변환은 사전에 정의된 웨이블릿 기저함수들로 분해&lt;/span&gt;하는 것입니다. 그렇다면 웨이블릿 변환은 왜 시간과 주파수 영역 둘 다에서 높은 해상도를 가지는 걸까요 ? 웨이블릿의 특징을 살펴보면 알 수 있습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://miro.medium.com/max/1400/1*Ioee_j_s29XVULQVUN_OmA.png&quot; /&gt;&lt;/p&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 3. Example Wavelets&lt;/figcaption&gt;

&lt;p&gt;&amp;lt;그림 1.&amp;gt;은 웨이블릿 함수의 예입니다. 웨이블릿은 국소 시간 부분에만 파동이 존재하고, 대부분은 0의 값을 가지는 파동입니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;A Wavelet is wave-like oscillation that is localized in time&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;웨이블릿은 2가지 파라미터가 있습니다. scale과 location입니다. Scale은   웨이블릿 파동을 늘이거나 줄이는데 관여합니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://miro.medium.com/max/1400/1*F4yPDvEePSWVLb7C9rRuag.png&quot; /&gt;&lt;/p&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 4. 웨이블릿의 scale변수&lt;/figcaption&gt;

&lt;p&gt;scale값이 크면, 웨이블릿은 늘어난 형태로 즉 작은 주파수를 가지게 되고, 국소 시간 부분의 크기가 증가합니다. 반대로 scale이 작아지면 웨이블릿은 큰 주파수를 가지게 되고 국소 시간 부분의 크기가 감소합니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://miro.medium.com/max/1400/1*QUAYlxYNrdRX0f4gRjTLtA.png&quot; /&gt;&lt;/p&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 5. 웨이블릿의 location변수&lt;/figcaption&gt;

&lt;p&gt;location변수는 wavelet의 이동과 관련됩니다. 주어진 location 변수만큼 웨이블릿은 주어진 신호를 슬라이딩하면서 변환을 계산하는 것입니다. 이러한 특징 때문에 웨이블릿 변환은 &lt;b&gt;합성곱(convolution)&lt;/b&gt;으로 볼 수 있습니다.&lt;/p&gt;

&lt;p algin=&quot;center&quot;&gt;&lt;img src=&quot;https://miro.medium.com/max/1200/1*4fXf0Yy8TMLSk7LXoZDDWw.gif&quot; /&gt;&lt;/p&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 6. 웨이블릿 변환 애니메이션&lt;/figcaption&gt;

&lt;p&gt;따라서, &lt;span style=&quot;text-decoration: underline&quot;&gt;웨이블릿 변환은 작은 주파수에 대해선 넓은 윈도우 크기를 가지고, 큰 주파수에 대해선 좁은 윈도우 크기를 가지기 때문에 시간과 주파수 두 영역 모두에서 높은 해상도를 가질 수 있게 되는 것&lt;/span&gt;입니다.&lt;/p&gt;

&lt;h2&gt;Differences among FT, STFT, and WT&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/ueD0HKr.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 5. FT, STFT 와 WT 비교&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;&amp;lt;그림 5.&amp;gt;는 푸리에 변환, 국소 푸리에 변환과 웨이블릿 변환을 비교한 그림입니다. FT는 time domain영역에 대한 주파수 변화는 볼 수 없고, STFT는 모두 동일한 윈도우 크기에 대해서만 주파수 영역을 분석할 수 있습니다. 반면에 WT는 주파수 크기에 따른 유연한 윈도우 크기를 설정하여 시간과 주파수 영역에 대한 해상도를 높일 수 있습니다.&lt;/p&gt;

&lt;h2&gt;Various kinds of Wavelets&lt;/h2&gt;

&lt;p&gt;아래는 다양한 웨이블릿 함수입니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://miro.medium.com/max/1400/1*mkdL9Wjoj2MjbPtkrpoZjA.png&quot; /&gt;&lt;/p&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 6. 웨이블릿 종류들&lt;/figcaption&gt;

&lt;p&gt;그림에서 다양한 웨이블릿인 것처럼, 어떤 웨이블릿을 선택하느냐에 따라 다른 변환 결과를 갖게 될 것입니다. 따라서, 주어진 신호에서 어떠한 특징을 뽑고 싶은지를 판단해야 하므로, 각 웨이블릿의 특징들을 살펴봐야 합니다. 여기서는 생략하도록 하겠습니다.&lt;/p&gt;

&lt;h2&gt;Discrete Wavelet Transform&lt;/h2&gt;

&lt;p&gt;좀 더 들어가서, 이산 웨이블릿 변환의 계산과정에 대해 살펴보겠습니다. 웨이블릿 변환은 해당 신호가 주어진 scale과 location 변수를 가진 wavelet과 얼마만큼 닮았는지에 대한 양에 해당되는 계수를 구하는 과정입니다. 따라서 이 계수들은 filter-bank를 반복적으로 적용하는 형태로 순차적으로 계산됩니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/7KNw6xP.png&quot; /&gt;&lt;/p&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 7. Filter bank view of wavelet decomposition&lt;/figcaption&gt;

&lt;p&gt;이산 웨이블릿 변환에서는 두 변수 scale(a)과 location(b)은 2의 배수씩 증가시켜서 다양한 웨이블릿 형태를 얻습니다.&lt;/p&gt;

\[a^j = 2^j , \,\,\,\, b_{j,k} = 2^jk \triangle t, \\where \,\, j = 1, 2, \dots, \infty, \\k=-\infty, \dots, -2, -1, 0, 1, 2, \dots, \infty\]

&lt;p&gt;직관적으로 이해하면, scale이 커질수록 주파수가 작아지기 때문에 국소 시간 부분의 크기가 증가합니다. 따라서, 여기에 맞춰서 scale이 커질수록 shift되는 정도도 큼직해야하고, 반면에 샘플링 정도는 작아져야 합니다. 그렇게 되어야 시간-주파수 영역에서의 해상도를 유지할 수 있습니다(그림 8.).&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/eCoFXbq.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 8. dyadic sampling&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;따라서 처음에 적용하는 웨이블릿은 scale이 가장 작기 때문에, 높은 주파수 영역대의 웨이블릿과의 신호와의 합성곱의 결과로 해당 웨이블릿과 유산한 양이 계산됩니다. 그렇다면 나머지 주파수 영역대에 대해서도 그 다음 scale인 2배된 scale을 통과해야 하는데 이는 마치 high-pass filter를 통과하고 난 나머지 부분에 대해서 처음 적용했던 필터보단 낮지만 높은 주파수 영역대를 살펴보는 또다른 high-pass filter를 통과시키는 것과 같은 과정을 거치게 됩니다. 따라서 결국엔 recursive한 형태를 보이게 되는 겁니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/WN4GtSe.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 9. filter bank view of wavelet decomposition(2)&lt;/figcaption&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;이상으로 포스팅을 마치겠습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;ol&gt;
  &lt;li&gt;The Wavelet Transform, &lt;a href=&quot;https://towardsdatascience.com/the-wavelet-transform-e9cfa85d7b34&quot;&gt;https://towardsdatascience.com/the-wavelet-transform-e9cfa85d7b34&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Lecture 16 : Limitations of the Fourier Transform: STFT, &lt;a href=&quot;https://qiml.radiology.wisc.edu/wp-content/uploads/sites/760/2020/10/notes_016_stft.pdf&quot;&gt;https://qiml.radiology.wisc.edu/wp-content/uploads/sites/760/2020/10/notes_016_stft.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Lecture 17 : Wavelets: Motivation and Description, &lt;a href=&quot;https://qiml.radiology.wisc.edu/wp-content/uploads/sites/760/2020/10/notes_017_wavelets_intro.pdf&quot;&gt;https://qiml.radiology.wisc.edu/wp-content/uploads/sites/760/2020/10/notes_017_wavelets_intro.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</content>
 </entry>
 
 <entry>
   <title>Discrete Fourier Transform에 대하여</title>
   <link href="http://localhost:4000/signal%20analysis/2021/06/18/ft-vs-wt/"/>
   <updated>2021-06-18T00:00:00+09:00</updated>
   <id>http://localhost:4000/signal%20analysis/2021/06/18/ft-vs-wt</id>
   <content type="html">&lt;p&gt;이산 푸리에 변환에 대해 알아보도록 하겠습니다.&lt;/p&gt;

&lt;h1&gt; Fourier transform &lt;/h1&gt;

&lt;p&gt;푸리에 변환이란 &lt;span style=&quot;text-decoration: underline&quot;&gt;&lt;b&gt;임의의 입력 신호를 다양한 주파수를 갖는 주기함수들의 합으로 분해하여 표현&lt;/b&gt;&lt;/span&gt;한 것입니다. 여러 주기함수가 혼합되어 있는 신호를 봤을 땐 신호의 특성을 살피기 어려우나, 푸리에 변환은 아래 그림처럼 혼합된 신호(빨간색)을 여러 종류의 주파수를 갖는 주기함수들(파란색)로 분해할 수 있기 때문에, 신호의 특징을 살펴볼 수 있습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/9967FA3359B63D8122&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt; 그림 1. 푸리에 변환 &lt;/figcaption&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img width=&quot;500&quot; src=&quot;https://i.imgur.com/GrX9rSd.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt; 그림 2. 푸리에 변환(2) &lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;푸리에 변환의 수학적 의미는 Time Domain(x축 : 시간, y축 : 진폭)을 Frequency Domain으로 변환(x축 : Frequency, y축 : 푸리에 변환 결과에 해당되는 계수)하는 것입니다. 아래는 일반 신호를 푸리에 변환한 결과(Spectogram)를 나타냅니다. Input 신호는 두 개의 주파수가 메인인 신호의 합성파입니다. 이처럼 푸리에 변환을 통해서 raw 데이터에서 볼 수 없는 특징을 찾아낼 수 있습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img width=&quot;500&quot; src=&quot;https://imgur.com/gXJGsN2.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt; 그림 3. 푸리에 변환 결과 &lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;일반적으로 Audio나 EEG 등 signal 데이터는 연속적일 수 없습니다. 왜냐하면, 기계를 통해 신호가 수집(sampling)이 되기 때문에 이산(Discrete)적인 특징을 띄고 있습니다. 예를 들어 256Hz로 샘플링 되는 신호라는 뜻은 1초에 256개 신호 sample을 수집한다는 뜻입니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/OQw4chB.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 4. 연속신호(continuous signal)와 디지털 신호(discretized signal)&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;따라서, 이산적인 특징을 다룰 수 있는 이산 푸리에 변환(Discrete Fourier Transform)을 사용합니다. 연속 푸리에 변환과 이산 푸리에 변환식은 아래와 같습니다.&lt;/p&gt;

\[\hat{f}(\xi) = \int_{\mathbf{R}^d} f(x)e^{2\pi ix\xi} \,dx \tag{1}\]

&lt;figcaption align=&quot;center&quot;&gt;수식 1. 연속 푸리에 변환&lt;/figcaption&gt;

\[\mathnormal{X}_k = \sum_{n=0}^{N-1}x_n\cdot e^{\frac{-2\pi i}{N}kn} \tag{2}\]

&lt;figcaption align=&quot;center&quot;&gt;수식 2. 이산 푸리에 변환&lt;/figcaption&gt;

&lt;h2&gt;Concept of Fourier Transform&lt;/h2&gt;

&lt;p&gt;푸리에 변환은 위에서 언급했듯이 여러 종류의 주파수를 갖는 함수로 분해하는 과정이라 하였습니다. 이 부분에 관한 의미를 2가지 측면으로 살펴보겠습니다. 푸리에 변환의 파동적인 측면에서의 개념(기본적 개념)과 선형대수적 개념입니다.&lt;/p&gt;

&lt;h3&gt;1. 푸리에 변환의 기본적 개념&lt;/h3&gt;

&lt;p&gt;푸리에 변환은 위에서 언급했듯이 여러 종류의 주파수를 갖는 함수로 분해하는 과정이라고 하였습니다. 어떤 방식으로 분해하는 걸까요 ? 이를 이해하기 위해선 오일러 공식을 알아야 합니다. 오일러 공식에 따르면, 복소지수함수 $e^{ix}$ 는 코사인과 사인의 합으로 구성됩니다. 오일러 공식을 좌표평면위에 나타나면 &amp;lt;그림 4.&amp;gt;와 같습니다. 이는 반지름이 1인 단위 원 위에 각 $x$ (그림에선 $\omega$) 성분을 가진 점으로 표현됩니다.&lt;/p&gt;

\[e^{ix} = cost + isinx \tag{3}\]

&lt;figcaption align=&quot;center&quot;&gt;수식 3. 오일러 공식&lt;/figcaption&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;300&quot; src=&quot;https://i.imgur.com/iVBkQVd.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 4. Euler's Formula&lt;/figcaption&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/dEe9227.gif&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 5. 푸리에 변환의 시각적 표현&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;&amp;lt;수식 1.&amp;gt;와 &amp;lt;수식 2.&amp;gt;를 보면 오일러 공식 부분을 대입해서 다시 쓰면 아래와 같습니다(이산 푸리에 변환에 대해서만 진행).&lt;/p&gt;

\[\mathnormal{X_k} = \sum_{n=0}^{N-1}x_n \cdot [cos(\frac{2\pi}{N}kn) - isin(\frac{2\pi}{N}kn)] \tag{4}\]

&lt;figcaption align=&quot;center&quot;&gt;수식 4. 푸리에 변환&lt;/figcaption&gt;

&lt;p&gt;&amp;lt;그림 4.&amp;gt;에서 단위 원 위에 있는 점이 일정한 속도로 움직이고, 이를 time domain 위에 그림을 그리면 &amp;lt;그림 5.&amp;gt;의 1번째 그림이 됩니다(1번째 그림이 단위 원이라고 가정한 것입니다). 여기서 속도를 결정하는 것이 바로 주파수에 해당됩니다. 즉 $\frac{2 \pi k}{N}$ 가 크면 클수록 원 위의 점이 빨리 움직이게 됩니다. &amp;lt;그림 5.&amp;gt;에서의 2번째그림에서 4번째 그림으로 갈수록 점의 움직임이 빨라지는 것을 볼 수 있는데, 이는 아래로 갈수록 큰 주파수를 가지는 것을 뜻합니다.&lt;/p&gt;

&lt;p&gt;마지막으로 &amp;lt;수식 4.&amp;gt;에서 $x_n$ 은 원의 반지름을 결정하는 요소입니다. 즉, $x_n$ 이 작을수록 작은 크기의 원 위의 점의 움직임에 해당되는 것입니다. &amp;lt;그림 5&amp;gt;에서 4번째 그림에 해당되는 것입니다.&lt;/p&gt;

&lt;p&gt;즉 푸리에 변환이란 &amp;lt;그림 5.&amp;gt;의 마지막 그림처럼 여러 크기와 주파수를 가진 복소수 함수의 분해를 뜻하는 것입니다. 마지막 그림에서 그려지는 신호는 결국 1~4번째 단일 신호들의 합으로 표현되는 것과 마찬가지입니다.&lt;/p&gt;

&lt;p&gt;푸리에 변환의 결과인 $\mathnormal{X_k}$ 가 뜻하는 건 이산화된 신호 $x_1, \cdots, x_n$ 인 각 지점에서 $\frac{2\pi k}{N}$ 주파수를 가진 주기함수를 얼마만큼 가지고 있느냐를 계산한 후 합한 것입니다. 즉, 전체적으로 해당 주파수를 가진 부분을 신호가 얼마만큼 가지고 있는지에 대한 정도를 하나의 계수로 표현한 것입니다. 따라서 &amp;lt;그림 3.&amp;gt; 에서 y축은 해당 주파수를 가진 주기함수가 이 신호에 얼마만큼 들어있는지에 대한 양을 나타내는 것입니다.&lt;/p&gt;

&lt;h2&gt;2. 푸리에 변환의 선형대수적 개념&lt;/h2&gt;

&lt;p&gt;다음으론 푸리에 변환의 선형대수적 개념에 대해 살펴보도록 하겠습니다. 이를 살펴보기 위해선 선형대수 지식이 필요합니다. 선형대수에서 N차원에서 N개의 직교기저가 있다면 이들 기저의 선형결합으로 N차원 위의 모든 점을 표현할 수 있습니다. 예를 들어 3차원 공간에서, 3개의 직교기저 (1,0,0), (0,1,0), (0,0,1)의 선형결합으로 3차원 위의 모든 점을 표현할 수 있습니다.&lt;/p&gt;

\[(x, y, z) = x(1, 0, 0) + y(0,1,0) + z(0,0,1) \tag{5}\]

&lt;p&gt;이산 푸리에 변환의 행렬 표현을 보면, 선형대수적인 개념을 확인할 수 있습니다.  &amp;lt;수식 2.&amp;gt;와 &amp;lt;수식 4.&amp;gt;에서 k=4까지의 이산 푸리에 변환 행렬은 아래와 같습니다.&lt;/p&gt;

&lt;p&gt;마찬가지로, 푸리에변환도 cosine과 sine로 구성된 직교 주기 함수의 선형결합으로, 신호가 N개로 이뤄진 벡터라면, cosine과 sine로 구성된 N차원의 선형결합으로 분석하고자 하는 신호를 표현한 것입니다. 이산 푸리에 변환을 행렬로 표현하는 과정을 보면 쉽게 이해하실 수 있습니다.&lt;/p&gt;

&lt;p&gt;전체 신호의 길이가 N인 이산 신호 $x_n$ 와 길이가 N인 주파수 성분 $\mathnormal X_k$ 에 대하여, &amp;lt;수식 2.&amp;gt;를 전개해보면 아래와 같습니다.&lt;/p&gt;

\[\mathnormal X_0 = x_0e^{-i\frac{2 \pi 0}{N}0} + x_1e^{-i\frac{2 \pi 0}{N}1} + x_2e^{-i\frac{2 \pi 0}{N}2} + \cdots + x_{N-1}e^{-i\frac{2 \pi 0}{N}(N-1)} \tag{6}\]

\[\mathnormal X_1 = x_0e^{-i\frac{2 \pi 1}{N}0} + x_1e^{-i\frac{2 \pi 1}{N}1} + x_2e^{-i\frac{2 \pi 1}{N}2} + \cdots + x_{N-1}e^{-i\frac{2 \pi 1}{N}(N-1)} \tag{7}\]

&lt;p&gt;$w = e^{-i\frac{2 \pi}{N}}$ 이라 한다면, 아래와 같이 선형 결합의 행렬 형태로 표현할 수 있습니다.&lt;/p&gt;

\[\begin{bmatrix}
   \ X_0 \\ X_1 \\ \vdots \\ X_{N-1}\end{bmatrix} =
   \begin{bmatrix}
    \ 1 &amp;amp; 1 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 1 \\
    \ 1 &amp;amp; w^1 &amp;amp; w^2 &amp;amp;\cdots &amp;amp; w^{N-1} \\
    \ \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
    \ 1 &amp;amp; w^{N-1} &amp;amp; w^{(N-1)2} &amp;amp; \cdots &amp;amp; w^{(N-1)(N-1)}\end{bmatrix} \begin{bmatrix} 
    \ x_0 \\ x_1 \\ \vdots \\ x_{N-1} \\\end{bmatrix} \tag{8}\]

&lt;p&gt;행렬의 선형 결합은 행렬 곱으로서 생각한다면, ‘내적’의 의미로도 해석할 수 있습니다. 내적의 의미는 곱해지는 벡터가 행렬의 열벡터와 얼마만큼 닮았는가를 의미하는데, 특정 주파수의 함량이 높다라는 건 해당 주파수와 이산 신호가 유사함을 높다라는 것을 뜻합니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2019-07-14-Freq_Sampling/pic1.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 6. 주파수 계수의 의미&lt;/figcaption&gt;
&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;이상으로 포스팅을 마치겠습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;ol&gt;
  &lt;li&gt;푸리에 변환 참고, &lt;a href=&quot;https://ralasun.github.io/deep%20learning/2021/02/15/gcn/&quot;&gt;https://ralasun.github.io/deep%20learning/2021/02/15/gcn/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;선형대수와 푸리에 변환 - 공돌이의 수학노트, &lt;a href=&quot;https://angeloyeo.github.io/2020/11/08/linear_algebra_and_Fourier_transform.html&quot;&gt;https://angeloyeo.github.io/2020/11/08/linear_algebra_and_Fourier_transform.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Fourier Transform, &lt;a href=&quot;https://ratsgo.github.io/speechbook/docs/fe/ft&quot;&gt;https://ratsgo.github.io/speechbook/docs/fe/ft&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Discrete Fourier Transform, &lt;a href=&quot;https://en.wikipedia.org/wiki/Discrete_Fourier_transform&quot;&gt;https://en.wikipedia.org/wiki/Discrete_Fourier_transform&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

</content>
 </entry>
 
 <entry>
   <title>Graph Convolutional Network에 대하여 - Spectral Graph Convolution(2)(작성 중)</title>
   <link href="http://localhost:4000/deep%20learning/2021/03/06/gcn(2)/"/>
   <updated>2021-03-06T00:00:00+09:00</updated>
   <id>http://localhost:4000/deep%20learning/2021/03/06/gcn(2)</id>
   <content type="html">&lt;blockquote&gt;
  &lt;p&gt;아직 작성 중에 있는 포스팅입니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;지난 포스팅 &lt;a href=&quot;https://ralasun.github.io/deep%20learning/2021/02/15/gcn/&quot;&gt;&amp;lt;Graph Convolutional Network에 대하여 - Spectral Graph Convolution&amp;gt;&lt;/a&gt; 에 이어서, Kipf et. al의 Graph convolutional Network에 대해서 살펴보도록 하겠습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1&gt;Spectral Graph Convolutional Network&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/hnw3IeK.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 1. convolution theorem&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;지난 포스팅에서 마지막에 언급한 spectral graph convolution 수식을 다시 살펴보도록 하겠습니다.&lt;/p&gt;

\[\mathbf {x} * G \mathbf {g} = \mathcal F^{-1}(\mathcal {F}(\mathbf {x}) \odot \mathcal {F}(\mathbf {g})) = \mathbf {U}(\mathbf {U^{\intercal}x} \odot \mathbf {U^{\intercal}g}) \tag1\]

\[\mathbf {x} * \mathbf {g}_\theta = \mathbf {U} \mathbf {g}_{\theta} \mathbf {U^{\intercal}x} \tag2\]

&lt;p&gt;수식 (1)에서 어떻게 수식(2)로 표현이 가능할까요 ? graph fourier transform은 Laplacian 행렬의 eigenvector의 선형결합이라고 하였습니다. 이 때, 학습해야 할 filte $\mathbf g$ 가 그림 1에서 time domain에서의 filter가 아니라, &lt;span style=&quot;text-decoration:underline&quot;&gt;(1)이미 frequency 영역에서의 filter $\mathbf g$ 라고 둔다면,&lt;/span&gt; 푸리에 변환된 signal과 단순 곱으로 계산할 수 있기 때문에 학습이 용이해집니다.&lt;/p&gt;

&lt;p&gt;지난 포스팅에서, convolution 연산의 특징 중 하나가 특정 signal이 시스템의 특성이 반영되어 필터링된 signal이 되는 것이라고 하였습니다. 그렇다면 GCN에서 이 filter를 어떻게 구축해야 signal의 특징을 잘 추출하고, filter의 특성도 잘 학습할 수 있을까요 ?&lt;/p&gt;

&lt;p&gt;푸리에 변환된 graph signal은 eigenvector들의 요소로 분해가 된 것입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1312.6203.pdf%20http://arxiv.org/abs/1312.6203&quot;&gt;Bruna, Joan, et al. “Spectral networks and locally connected networks on graphs.” arXiv preprint arXiv:1312.6203 (2013).&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

</content>
 </entry>
 
 <entry>
   <title>Graph Convolutional Network에 대하여 - Spectral Graph Convolution</title>
   <link href="http://localhost:4000/deep%20learning/2021/02/15/gcn/"/>
   <updated>2021-02-15T00:00:00+09:00</updated>
   <id>http://localhost:4000/deep%20learning/2021/02/15/gcn</id>
   <content type="html">&lt;p&gt;지난 GNN 포스팅&amp;lt;&lt;a href=&quot;https://ralasun.github.io/deep%20learning/2021/02/11/gcn/&quot;&gt;Introduction to Graph Neural Network - GNN 소개 및 개념&lt;/a&gt;&amp;gt;에서 graph neural network의 전반적인 개념에 대해 소개하였습니다. 이번 포스팅은 graph neural network가 더욱 유명해진 계기가 된 &lt;a href=&quot;https://arxiv.org/abs/1609.02907&quot;&gt;Kipf. et, al.의 Graph Convolutional Neural Network&lt;/a&gt;에 대해 다루도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;Kipf. et al.의 GCN을 이해하기 위해서는 먼저, spectral graph convolution에서부터 시작해야 합니다. 그러나 :spectral” 이라는 부분이 생소하신 분들이 많을 거라 생각됩니다. 반면에 일반적인 CNN 동작 방식은 많이 알려져 있습니다. 일반적인 CNN 동작은 spatial convolution입니다. 따라서 이를 유사하게 graph에 적용하는 방식을 spatial graph convolution입니다. 따라서, 이번 포스팅에서는 spectral 방식과 spatial 방식을 비교하고, spectral graph convolution에 대해 자세히 설명한 뒤에 Kipf. et al의 Graph Convolutional Network에 대해 다루도록 하겠습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1&gt;Spatial Graph Convolution vs. &lt;br /&gt;Spectral Graph Convolution&lt;/h1&gt;

&lt;p&gt;Graph convolution은 크게 2가지 방법이 있습니다. Spatial graph convolution과 Spectral graph convolution입니다. Spatial graph convolution은 convolution 연산을 graph위에서 직접 수행하는 방식으로, 각 노드와 가깝게 연결된 이웃 노드들에 한해서 convolution 연산을 수행합니다. 즉, 노드와 이웃노드들을 특정 grid form으로 재배열하여 convolution 연산을 수행하는 것입니다. 그러나, 우리가 일반적으로 아는 CNN의 filter는 고정된 사이즈를 가집니다(그림 1.).&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/S5B1k.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 1. CNN operation with fixed-size filter(3x3)&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;따라서, &lt;b&gt;&lt;i&gt;spatial graph convolution 방식의 관건은 고정된 크기의 이웃 노드를 선택하는 것입니다.&lt;/i&gt;&lt;/b&gt; 뿐만 아니라, CNN의 특징 중 하나는 “local invariance” 입니다. 입력의 위치가 바뀌어도 출력은 동일함을 의미합니다. 즉, 이미지 내의 강아지 위치가 달라도 CNN은 강아지라는 아웃풋을 출력함을 의미합니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://miro.medium.com/max/1400/1*HUJ3-xs3nUv-wY_GTBVUMg.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 2. Local invariance&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;따라서, &lt;b&gt;&lt;i&gt;Spatial graph convolution의 또다른 관건은 바로 “local invariance”를 유지를 해야한다는 것입니다.&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;앞에서 언급한 spatial graph convolution이 다뤄야 할 문제점과 별개로 또다른 문제점이 존재합니다. &lt;b&gt;Spatial graph convolution은 고정된 이웃 노드에서만 정보는 받아서 노드의 정보를 업데이트를 한다는 점입니다.&lt;/b&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img width=&quot;300&quot; src=&quot;https://imgur.com/KWmqbgk.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 3. Select neighborhood of red node&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;그러나, 그래프에서의 한 노드의 정보는 시간에 따라 여러 노드들의 정보의 혼합으로 표현될 수 있습니다. &amp;lt;그림 4.&amp;gt;를 살펴보도록 하겠습니다. 1번노드의 t=0일 때 정보는 [1,-1] 이지만 시간에 따라 여러 노드들의 정보(노드들의 signal)들이 밀려 들어오게 됩니다. 즉, 고정된 이웃노드 말고도 멀리 연결되어 있는 노드의 정보도 시간이 흐르면서 밀려 들어올 수 있는 것입니다. 이를 노드 간 message passing이라 합니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/Fv2FJbC.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 4. Message Passing in graph&lt;/figcaption&gt;
&lt;/p&gt;

&lt;p&gt;즉, 한 노드의 정보는 여러 노드의 signal이 혼재해 있는 것으로, 이를 time domain이 아닌 frequency 도메인으로 분석한다면, 한 노드 내에 혼재된 signal들을 여러 signal의 요소로 나눠서 node의 특징을 더 잘 추출할 수 있습니다. 이것에 관한 것이 바로 “Spectral Graph Convolution”입니다. Spectral graph convolution은 spectral 영역에서 convolution을 수행하는 것입니다. 이에 대해 자세히 살펴보도록 하겠습니다.&lt;/p&gt;

&lt;h1&gt;Dive into Spectral Graph Convolution&lt;/h1&gt;

&lt;p&gt;Signal Processing 분야에서 “spectral analysis”라는 것은 이미지/음성/그래프 신호(signal)을 time/spatial domain이 아니라 frequency domain으로 바꿔서 분석을 진행하는 것입니다. 즉, &lt;em&gt;어떤 특정 신호를 단순한 요소의 합으로 분해하는 것&lt;/em&gt;을 의미합니다. 대표적으로 이를 수행할 수 있는 방법이 푸리에 변환(Fourier Transform)입니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;spectral analysis에서 입력 신호가 전파/음성신호면 time domain을 frequency domain으로 변환하는 것이고, 컴퓨터 비전/그래프/영상처리 분야이면 spatial domain을 frequency domain으로 변환하는 것입니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;푸리에 변환이란, &lt;span style=&quot;text-decoration: underline&quot;&gt;&lt;b&gt;임의의 입력 신호를 다양한 주파수를 갖는 주기함수들의 합으로 분해하여 표현&lt;/b&gt;&lt;/span&gt;하는 것입니다. 아래 그림처럼 빨간색 신호를 파란색의 주기함수들의 성분으로 나누는 작업이 바로 푸리에 변환입니다. 즉, 파란색 주기함수들을 합하면 결국 빨간색 신호가 되는 것입니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/9967FA3359B63D8122&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 5. 푸리에 변환&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;그렇다면 graph signal에서의 푸리에 변환은 어떤 걸까요 ?&lt;/p&gt;

&lt;p&gt;결론부터 얘기하면, &lt;span style=&quot;text-decoration: underline&quot;&gt;&lt;b&gt;graph signal의 푸리에 변환은 graph의 Laplacian matrix를 eigen-decomposition하는 것&lt;/b&gt;&lt;/span&gt;입니다. 아래에서 수식과 함께 자세히 살펴보도록 하겠습니다.&lt;/p&gt;

&lt;h3&gt;Fourier transform&lt;/h3&gt;

&lt;p&gt;먼저, 푸리에 변환 식에 대해서 살펴봅시다. &lt;span style=&quot;color:gray&quot;&gt;&lt;del&gt;저도 푸리에 변환에 대한 이해가 아직 한없이 부족합니다. 최대한 공부하고 이해한 내용을 풀어볼려고 노력하였습니다.&lt;/del&gt;&lt;/span&gt;&lt;/p&gt;

\[\hat{f}(\xi) = \int_{\mathbf{R}^d} f(x)e^{2\pi ix\xi} \,dx \tag{1}\]

\[f(x) = \int_{\mathbf{R}^d} \hat{f}(\xi) e^{-2\pi ix\xi} \,d\xi \tag{2}\]

&lt;p&gt;(1)은 f의 푸리에 변환이고, (2)는 푸리에 역변환입니다. 푸리에 변환은 위에서 설명드린 것처럼, time domain을 frequency domain으로 변환한 것으로, 다양한 주파수를 갖는 주기함수의 합입니다. 그렇다면, 푸리에 역변환은 frequency domain의 함수를 다시 time domain으로 변환하는 것입니다. 푸리에 변환을 바라보는 관점은 여러가지가 존재하지만 그 중 하나는 ‘내적’입니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&quot;임의의 주파수 $f(x)$ 에 대하여, $\hat{f}(\xi)$ 는 $f(x)$ 와 $e^{-2\pi ix\xi}$ 의 내적&quot;&lt;/p&gt;

&lt;p&gt;‘내적’이 내포하고 있는 의미는 유사도입니다. 즉, “a와 b의 내적은 a와 b가 얼마나 닮았는가”를 뜻합니다. 결국 푸리에 변환은 다시 풀어쓰면 아래와 같은 의미를 가지고 있습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&quot;임의의 주파수 $f(x)$ 에 대하여, $\hat{f}(\xi)$ 는 $f(x)$ 와 $e^{-2\pi ix\xi}$ 가 얼마나 닮았는가&quot;&lt;/p&gt;

&lt;p&gt;그렇다면, $e^{-2\pi ix\xi}$ 의 의미는 무엇일까요 ? 이를 이해하기 위해선 ‘오일러 공식’이 필요합니다. 오일러 공식은 복소지수함수(complext exponential function)를 삼각함수(trigonometric function)로 표현하는 유명한 식입니다.&lt;/p&gt;

\[e^{ix} = cost + isinx \tag{3}\]

&lt;p&gt;따라서, 오일러 공식에 의해 (1)식의 $e^{2\pi ix\xi}$ 부분을 cos요소와 sin요소의 합으로 표현할 수 있습니다.&lt;/p&gt;

\[e^{2\pi ix\xi} = cos(2\pi x\xi) + i sin(2\pi x\xi) \tag{4}\]

&lt;p&gt;즉, 주어진 주파수 f(x)에 대해 cosine에서 유사한 정도와 sine과 유사한 정도의 합이 푸리에 변환이라고 생각할 수 있습니다.&lt;/p&gt;

&lt;p&gt;이번엔 푸리에 변환의 선형대수(linear algebra)적인 의미를 살펴보도록 하겠습니다. 선형 대수에서, 벡터 $a \in R^d$ 를 d차원의 orthonormal basis를 찾을 수 있다면, 벡터 $a$ 를 orhonormal basis의 선형결합으로 표현할 수 있습니다. 이 orthonormal basis를 찾는 방법 중 하나가 바로 Eigen-value decomposition 입니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;orthonormal이란 서로 직교하면서 길이가 1인 벡터들을 의미합니다. 또한, 모든 matrix에 대해서 eigen-value decomposition 결과로 찾은 basis가 orthonormal은 아닙니다. 하지만 real-symmetric matrix에 대하여 구한 eigenvector들은 orthgonal한 관계입니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;다시 돌아와서, 푸리에 변환에서 주기함수 요소인 sine과 cosine에 대해 살펴봅시다. 아래와 같이 sine과 sine, sine과 cosine, cosine과 cosine을 내적하면 모두 다 0이 나옵니다. 이는 즉 삼각함수는 직교함을 알 수 있습니다(삼각함수의 직교성).&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/Bdo17jG.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 6. 삼각함수의 직교성&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;그렇다면, 선형대수 관점에서, &lt;span style=&quot;text-decoration: underline&quot;&gt;sine과 cosine 기저들의 선형결합이 즉 푸리에 변환이 되는 것&lt;/span&gt;입니다. 즉, &lt;span style=&quot;text-decoration:underline; color:red&quot;&gt;어떤 특정 graph signal에 관한 행렬이 존재하고 이 행렬이 real-symmetric matrix이며 이들의 eigenvectors를 구할 수 있다면, eigenvector의 선형결합이 graph signal의 푸리에 변환&lt;/span&gt;임을 의미하는 것입니다.&lt;/p&gt;

&lt;h3&gt;Laplacian(Laplace Operator)&lt;/h3&gt;

&lt;p&gt;Graph laplacian을 보기 전에 Laplace Operator에 대해 살펴보도록 하겠습니다. Laplace operator는 differential operator로, 벡터 기울기의 발산(Divergence)을 의미합니다.&lt;/p&gt;

\[\triangle f= \triangledown \cdot \triangledown f = \triangledown^2 f \tag{5}\]

&lt;p&gt;$\triangledown f$ 는 $f$ 의 기울기를 의미하는 것으로 1차함수의 기울기처럼, 한 점에서의 변화하는 정도를 의미합니다. 이를 그림으로 나타나면 아래와 같습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2019-08-28_laplacian/noname01.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 7. Scalar 함수&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;&amp;lt;그림 7.&amp;gt;의 scalar 함수의 gradient는 아래와 같습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2019-08-28_laplacian/noname03.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 8. Scalar 함수의 gradient&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;&amp;lt;그림 7.&amp;gt;과 &amp;lt;그림 8.&amp;gt;을 보시면, (x,y)=(0,2) 부근에는 수렴하는 형태의 gradient가 형성되어 있고, (x,y)=(0,-2) 부근에는 발산하는 형태의 gradient가 형성되어 있습니다. Laplace 연산자를 이용해서 이 기울기의 발산 $\triangle f$ 을 구해주면, 아래와 같습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2019-08-28_laplacian/noname04.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 9. Divergence&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;divergence가 나타내는 의미, 즉, Laplace operator가 나타내는 의미는 무엇일까요 ? 이 함수의 높고 낮음을 표시하는 것입니다. 고등학교 때, 배운 2차 편미분과 비슷합니다. 이차 편미분 값이 양수면 아래로 볼록이고, 이차 편미분 값이 음수면 위로 볼록인 것과 유사합니다. 즉, 노란 부분일수록 양수 이기때문에 위로볼록인 모양이고, 파란부분일수록 음수값이기 때문에 아래로 볼록입니다.&lt;/p&gt;

&lt;p&gt;그렇다면, graph signal 영역에서 Laplace operator가 갖는 의미가 무엇일까요 ? graph signal 영역에서 Laplace operator를 적용한다는 건, 한 노드에서의 signal의 흩어짐 정도, 즉, 흐르는 정도를 알 수 있습니다. &lt;span styple=&quot;text-decoration:underline; color:red&quot;&gt;특정 노드에서 signal이 들어왔을 때 그 signal이 특정 노드와 연결된 노드들로 각각 얼마만큼 빠르게 흩어지는지를 알 수 있고 이는 즉 그 노드의 특징이 될 수 있는 것입니다.&lt;/span&gt; 위의 그림을 예를 들어서 설명한다면, 만약에 한 signal이 그림 7.의 가장 높은 부분(노란색 부분)에서 시작된다면 가장 낮은 부분(파란색 부분)까지 빠른 속도로 흘러갈 것입니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;빨간색으로 강조한 부분이 왜 laplacian matrix의 eigen-value decomposition이 fourier transform과 연결되는지에 관한 부분입니다. 포스팅을 쭉 끝까지 읽어보시면 아하! 하면서 이해가 되실겁니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;아래는 이와 관련된 gif이미지입니다. Grid 격자를 어떤 graph라고 생각한다면, 어떤 노드에서 signal이 들어왔을 때 흩어지는 양상을 보실 수 있습니다. 이 흩어지는 양상을 자세히 알기 위해서는 laplcian operator를 이용하여 계산하면 됩니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://miro.medium.com/max/1120/1*gz2hyrcSSJG9MtDzmQLe3w.gif&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 9. Diffusion of some signal in a regular grid graph based on the graph Laplacian&lt;/figcaption&gt;&lt;/p&gt;

&lt;h3&gt;Graph Laplacian&lt;/h3&gt;

&lt;p&gt;그러나, 이제까지 설명한 laplacian operator는 지난 포스팅에서 언급한 Laplacian matrix랑 무슨 관련이 있는 걸까요? 이름이 비슷한 걸 보니, laplacian matrix도 어떤 differential operator, 즉 ‘변화’에 관한 행렬임을 짐작할 수 있습니다.&lt;/p&gt;

\[\triangle f = \triangledown^2 f =\sum_{i=1}^{n}\frac{\partial^2 f}{\partial {x_i}^2} \tag{6}\]

&lt;p&gt;1차원에서의 laplacian operator는 &lt;a href=&quot;https://en.wikipedia.org/wiki/Second_derivative&quot;&gt;이차도함수 극한 정의&lt;/a&gt;와 동일합니다.&lt;/p&gt;

\[\triangledown^2 f = \lim_{h \rightarrow 0} \frac{f(x+h) - 2f(x) - f(x-h)}{h^2} \tag{7}\]

&lt;p&gt;그렇다면 위 laplacian operator가 graph 상에서 적용되면 어떻게 될까요 ? 위 식 (7)은 아래 그림과 같이 표현될 수 있습니다. 즉, x노드의 signal f(x)와 x노드와 h사이 만큼 떨어진 이웃노드의 signal f(x+h), f(x-h) 와의 변화량을 통해 x노드의 signal 특징을 구한 것입니다.이것이 바로 x노드 signal에 laplacian operator를 적용한 거라고 생각될 수 있습니다. 따라서, 한 노드 의 특징은 해당 노드와 연결된 이웃노드와의 관계라는 관점에서 표현될 수 있고, 즉 이 표현을 위해 이웃 노드와의 차이(즉, 변화)를 이용한 것이 laplacian operator가 됩니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img width=&quot;400&quot; src=&quot;https://imgur.com/BYvjBKt.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 10. discretized laplacian operator&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;또한 graph laplacian은 연속적인 성질(continuous)이 아닌 이산적인 성질(discrete)을 띕니다. 이제까지 언급한 특징을 정리되면 아래의 graph laplacian 식이 이해가 되실 것입니다. 아래는 $v_i$ 노드에서 graph laplacian operator를 적용한 것입니다.&lt;/p&gt;

\[\triangle f(v_i) = Lf|_{v_i} = \sum_{v_j ~ v_i}[f(v_i) - f(v_j)] \tag{8}\]

&lt;p&gt;weighted undirected graph인 경우, weighted undirected graph는 노드간 엣지에 가중치가 있는 경우입니다. graph laplacian operator는 아래와 같습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img width=&quot;400&quot; src=&quot;https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSRUOW9mJ2mNgmHDw7_q7uiK2m1slCZgiah1Q&amp;amp;usqp=CAU.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 11. weighted undirected graph&lt;/figcaption&gt;&lt;/p&gt;

\[\triangle f(v_i) = \sum_{j \in N_i}W_{ij}[f(v_i) - f(v_j)] \tag{9}\]

&lt;p&gt;여기서 $v_i$ 는 특정노드를 가르키는 인덱스이고, $f(v_i)$ 는 각 노드의 signal 입니다. 즉, 함수 $f \in R^N$ 는 각 노드의 특성을 signal로 맵핑해주는 함수라고 생각하시면 됩니다.&lt;/p&gt;

&lt;p&gt;Graph laplacian 예를 들어봅시다. 아래와 같은 간단한 graph가 있을 때, 각 노드에 대한 laplacian operator는 아래와 같습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img width=&quot;400&quot; src=&quot;https://imgur.com/DYfJGS9.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 12. Graph laplacian example&lt;/figcaption&gt;&lt;/p&gt;

\[\triangle f(v_1) = 2f(v_1) - f(v_2) - f(v_3)\]

\[\triangle f(v_2) = 3f(v_2) - f(v_1) - f(v_3) - f(v_4)\]

\[\triangle f(v_3) = 2f(v_3) - f(v_1) - f(v_2)\]

\[\triangle f(v_4) = f(v_4) - f(v_2)\]

&lt;p&gt;이를 행렬로 표현하면 아래와 같습니다.&lt;/p&gt;

\[M = \begin{bmatrix}
       2 &amp;amp; -1 &amp;amp; -1 &amp;amp; 0 \\
       -1 &amp;amp; 3 &amp;amp; -1 &amp;amp; -1 \\
       0 &amp;amp; -1 &amp;amp; 0 &amp;amp; 1\end{bmatrix} 
       \begin{bmatrix} f(v_1) \\ f(v_2) \\ f(v_3) \\ f(v_4)\end{bmatrix} \tag{10}\]

&lt;p&gt;위 식 (10) 의 앞부분 행렬은 지난 GNN 포스팅&amp;lt;&lt;a href=&quot;https://ralasun.github.io/deep%20learning/2021/02/11/gcn/&quot;&gt;Introduction to Graph Neural Network - GNN 소개 및 개념&lt;/a&gt;&amp;gt; 에서 소개한 Laplacian matrix 입니다. 즉, &lt;span style=&quot;text-decoration:underline&quot;&gt;laplacian matrix란 graph representation 중에서 이웃 노드와의 변화 흐름을 통해 노드의 특징을 나타내는 그래프 표현이라고 생각할 수 있습니다.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;이웃 노드와의 차이(변화, variation)는 결국 노드 간의 매끄러움 정도(smoothness)를 의미합니다. 한 노드가 이웃 노드와의 차이가 작다는 건 그 노드는 이웃 노드와 특성이 비슷한 경우이고, 이를 ‘매끄럽다’라고 생각할 수 있습니다. 반면에, 이웃 노드와의 차이가 크다는 건 그 노드는 이웃 노드와 특성이 상이하다는 것이고 이는 ‘매끄럽지 않다’라고 생각할 수 있습니다. 따라서 Laplacian Matrix는 graph의 smoothness와 관련이 있습니다.&lt;/p&gt;

&lt;p&gt;이를 ‘신호’라는 관점에서 다시 생각해봅시다. 어떤 한 노드 내에서 흐르는 신호는 크게 2가지로 나눈다면, 나와 비슷한 특성을 가진 이웃노드에서 들어오는 신호와 나와 상이한 특성을 가진 이웃노드에서 들어오는 신호로 나눌 수 있습니다. 즉, &lt;span style=&quot;text-decoration:underline; color:red&quot;&gt;한 노드 내에 혼잡해 있는 신호는 나와 유사한 특성을 가진 노드에서 오는 신호와 나와 상이한 특성을 가진 노드에서 오는 신호의 결합으로 생각할 수 있습니다.&lt;/span&gt; 이는 결국 푸리에 변환과 관련된 개념입니다. 그리고 나와 유사한 속성의 노드와 상이한 속성의 노드를 나눌 수 있는 것에 관한 정보가 바로 Laplacian matrix에 담겨져 있는 것입니다. 따라서, &lt;span style=&quot;text-decoration:underline; color:red&quot;&gt;lapalcian matrix를 이용한 푸리에 변환이 바로 “Graph Fourier Transform” 이며, 이는 위에서 언급한 “eigen-value decomposition”과 관련이 있는 것입니다.&lt;/span&gt;&lt;/p&gt;

&lt;h3&gt;Graph Fourier Transform&lt;/h3&gt;

&lt;p&gt;그렇다면, graph fourier transform이 왜 Laplacian matrix를 eigen-decomposition을 하는지에 대한 궁금증이 여기서 생깁니다. 이 부분을 이제 짚고 넘어가겠습니다. 이 부분까지 이해가 되신다면, 이 이후에 진행할 spectral graph convolution을 제대로 이해하실 수 있으실 것입니다.&lt;/p&gt;

&lt;p&gt;위에서 특정 노드와 유사한 노드 집단과 상이한 노드집단을 나눌 수 있다면 특정 노드에 혼잡해 있는 신호를 여러 특성의 신호로 분해할 수 있다고 하였습니다. 그렇다면 먼저 이 집단을 구별할까요 ? 아래 식을 살펴봅시다. 아래 식은 결국, 노드 간의 차이의 합입니다.&lt;/p&gt;

\[S = \sum_{(i,j) \in \epsilon}W_{ij}[f(i) - f(j)]^2 = \mathbf {f^{\intercal}Lf} \tag{11}\]

&lt;p&gt;이는 결국 graph의 smoothness와 관련된 것이며, Laplacian quadratic form으로 표현가능합니다. 위의 식을 최소화하게 하는 $\mathbf f$ 를 찾는다면, 특정 노드와 특성이 유사한 노드 집단과 상이한 노드 집단을 구분할 수 있습니다.&lt;/p&gt;

\[min_{f \in R^N, \,||f||_2 = 1} \mathbf {f^{\intercal}Lf} \tag{12}\]

&lt;p&gt;Lagrange 방정식에 의해 최적화 방정식으로 바꿀 수 있습니다.&lt;/p&gt;

\[L = \mathbf {f^{\intercal}Lf} - \lambda(\mathbf {f^{\intercal}f} - 1) \tag{13}\]

&lt;p&gt;위 식을 최소가 되게 하기 위한 $\mathbf f$ 를 찾기 위해 $\mathbf f$ 로 미분하면 아래와 같습니다.&lt;/p&gt;

\[\frac{\partial L}{\partial f} = 2\mathbf {Lf} - 2\lambda \mathbf f = 0 \tag{14}\]

&lt;p&gt;따라서, 최소가 되는 $\mathbf f$ 는 아래 식을 만족해야 합니다.&lt;/p&gt;

\[\mathbf {Lf} = \lambda \mathbf f \tag{15}\]

&lt;p&gt;위의 식은 laplacian 행렬의 eigen-value decomposition입니다. laplacian matrix의 eigenvector eigen vector들이 특정 노드와 유사한 노드집단과 상이한 노드집단을 구분하는 기준이 되고, 특히 작은 값의 eigenvalue의 대응하는 eigenvector일수록 graph를 더욱 smooth하게 나누는 기준이 됩니다.&lt;/p&gt;

&lt;p&gt;laplacian matrix의 eigenvector의 예를 들어봅시다. 아래 그림은 사람의 behavior를 graph으로 표현했을 때, 해당 graph의 laplacian 행렬의 eigenvector $u_2, u_3, u_4, u_8, \,\,, (0 &amp;lt; \lambda_2 \leq \lambda_3 \leq \lambda_4 \leq \lambda_8)$ 에 graph 노드를 임베딩한 것입니다. 즉, 그래프 노드 $f(v_i)$ 라면, $u^{\intercal}f(v_i)$ 를 계산한 겁니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img width=&quot;400&quot; src=&quot;https://imgur.com/JWVlsGQ.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 12. Projection on eigenvector of Laplacian Matrix with each graph node&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;사람의 행동을 graph로 표현했을 때, 위의 그림처럼 가까이에 있는 부분끼리 이웃노드에 해당될 가능성이 높습니다. 머리 쪽과 팔쪽은 가까우니깐 이웃노드일 가능성이 높고, 머리와 다리 쪽은 상이한 노드일 가능성이 높습니다. 따라서, eigenvalue가 가장 작은 eigenvector $u_2$ 에 임베딩했을 때, 해당 graph의 노드들을 이웃노드 집단과 상이한 노드 집단으로 잘 분리되는 것을 보실 수 있습니다.&lt;/p&gt;

&lt;p&gt;이를 다시 돌아와서 fourier transform 관점에서 해석해봅시다. &lt;span style=&quot;text-decoration:underline; color:red&quot;&gt;graph의 특정 노드 signal을 $f(v_i)$ 를 작은 값의 eigenvalue에 대응되는 eigenvector에 사영시키면, 혼재되어 있는 signal 중 가까운 이웃노드에서 들어오는 signal의 성분을 추출한 것으로 볼 수 있고, 큰 값의 eigenvalue에 대응되는 eigenvector에 사영시키면, 혼재되어 있는 signal 중 멀리 있는 상이한 노드에서 들어오는 signal의 성분을 추출한 것입니다.&lt;/span&gt; 따라서, 혼재되어 있는 graph signal을 eigenvector의 선형결합으로 표현하여, 여러 집단(가장 유사한 집단 &amp;lt; … &amp;lt; 매우 상이한 집단)에서 들어오는 signal의 합으로 표현할 수 있습니다.&lt;/p&gt;

&lt;p&gt;또한 위에서, fourier transform은  혼합 signal을 sine과 cosine의 주파수의 성분으로 쪼개어 이 성분들의 선형결합이라고 하였습니다. 그리고, 이 때 주파수의 성분은 삼각함수의 직교성으로 인해, 직교기저를 이룬다고 하였습니다. 마찬가지로, laplacian matrix은 real-symmetric 행렬이어서 eigenvector들이 직교기저를 이룹니다. 즉, graph signal을 laplacian eigenvector 행렬에 사영시키면, graph signal을 laplacian의 직교 기저들의 성분으로 분해하여 이를 합한 선형결합에 해당됩니다.&lt;/p&gt;

&lt;p&gt;이제까지 설명한 것을 토대로 Graph Fourier Transform은 바로 Laplacian 행렬의 Eigen-value decomposition과 관련이 있게 되는 것입니다. 수식으로도 왜 그렇게 되는지 알 수 있는데, 이는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;Fourier transform의 식을 다시 쓰면 아래와 같습니다.&lt;/p&gt;

\[\hat{f}(\xi) = &amp;lt;f, e^{2\pi ix\xi}&amp;gt;\int_{\mathbf{R}^d} f(x)e^{2\pi ix\xi} \,dx \tag{16}\]

&lt;p&gt;주파수 성분은 $e^{2\pi ix\xi}$ 이며, 여기에 laplace operator를 적용하면 아래와 같습니다.&lt;/p&gt;

\[Lf = -\triangle(e^{2\pi ix\xi}) = -\frac{\partial^2}{\partial x^2}e^{2\pi ix\xi} = (2\pi \xi)^2 e^{2\pi ix\xi} = \lambda f \tag{17}\]

&lt;p&gt;즉, 주파수 성분 $e^{2\pi ix\xi}$ 도 laplace operator의 eigen-function라고 볼 수 있습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;1차원 같은 경우, eigen-function이라 한 이유는 eigen-vector는 2차원 이상일 때의 eigen-function이라고 볼 수 있기 때문입니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이제까지는 왜 Graph Fourier Transform이 Laplacian 행렬이 eigen-value decomposition과 관련 있는지 개념적인 이유와 수식적인 이유에 대해서 살펴봤습니다. 그러면 마지막으로, Graph Fourier Transform의 수식을 정리해보도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;Laplacian 행렬의 eigen-value decomposition은 아래와 같습니다.&lt;/p&gt;

\[\mathbf {L} = \mathbf {U^{\intercal}\Lambda U} \tag{18}\]

&lt;p&gt;이를 이용한 graph fourier transform과 inverse graph fourier transform은 아래와 같습니다.&lt;/p&gt;

\[\mathcal {F}(\mathbf x) = \mathbf {U^{\intercal}x} \tag{19}\]

\[\mathcal {F^{-1}}(\hat {\mathbf {x}}) = \mathbf {U} \hat {\mathbf {x}} \tag{20}\]

&lt;blockquote&gt;
  &lt;p&gt;inverse fourier transform은 frequency domain으로 변환된 signal을 다시 time domain으로 변환하는 것입니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3&gt;Spectral Graph Convolution&lt;/h3&gt;

&lt;p&gt;드디어 Spectral Graph Convolution 입니다. 꽤 많이 돌아왔으나, Spectral Graph Convolution을 이해하기 위해서 필요한 Fourier Transform, Graph Laplacian, Graph Fourier Transform을 살펴봤습니다. 그렇다면 이제까지 배운 개념을 가지고 Spectral Graph Convolution이 어떻게 작동하는지 알아보겠습니다.&lt;/p&gt;

&lt;h5&gt;convolution theorem&lt;/h5&gt;

&lt;p&gt;입력과 출력이 있는 시스템에서, 출력 값은 현재의 입력에만 영향을 받는 것이 아니라 이전의 입력값에도 영향을 받습니다. &lt;span style=&quot;text-decoration:underline&quot;&gt;따라서 이전의 값까지의 영향을 고려하여 시스템의 출력을 계산하기 위한 연산이 convolution입니다.&lt;/span&gt; 어떤 시스템에 입력신호가 들어가서 출력신호가 있다고 했을 때, 출력신호는 입력신호와 시스템함수의 convolution 연산을 통해서 나오는 것입니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img width=&quot;400&quot; src=&quot;https://imgur.com/dk7otIh.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 13. convolution&lt;/figcaption&gt;&lt;/p&gt;

\[(f*g)(t) = \int_{-\infty}^{\infty}f(\tau)g(t-\tau)d\tau \tag{21}\]

&lt;p&gt;convolution의 특징은 &lt;span style=&quot;text-decoration:underline&quot;&gt;시스템의 출력으로 ‘시스템의 특성’을 알 수 있다는 점입니다.&lt;/span&gt; &amp;lt;그림 13.&amp;gt;에서 시스템의 특성이 담긴 시스템 함수가 위와 같은 모양이라면, 출력도 시스템 함수와 유사한 모양으로 나옵니다.&lt;/p&gt;

&lt;p&gt;&amp;lt;그림 14&amp;gt;와 같은 모양을 가진 시스템 함수라면, 출력도 시스템 함수와 유사한 모양이 나옵니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img width=&quot;400&quot; src=&quot;https://t1.daumcdn.net/cfile/tistory/11297E0F4CFB6C3721&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 14. convolution(2)&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;이 개념을 적용한 것이 CNN에서의 filter에 해당되는 것입니다. 이미지 영역내에서의 convolution은 filter라는 시스템에 이미지라는 신호를 입력하여 filter에 해당되는 특징을 출력해 내는 것입니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img width=&quot;400&quot; src=&quot;https://imgur.com/wvuTh7M.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 15. 이미지와 특정 filter의 convolution 연산 수행 후&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;convolution의 기본 개념에 대해서 대략적으로 살펴봤습니다. graph convolution은 그렇다면 우리는 어떤 graph의 특징을 추출해 낼 수 있는 filter를 학습으로 얻고 싶은 것입니다.&lt;/p&gt;

&lt;p&gt;convolution theorem은 그럼 어떤 것일까요 ? ‘time 영역에서의 signal과 시스템 함수와의 convolution 연산은 각각 frequency 영역으로 변환한 뒤의 곱과 같다’가 바로 convolution theorem 입니다. ‘time 영역에서의 signal과 시스템 함수와의 convolution 연산은 각각 frequency 영역으로 변환한 뒤의 곱과 같다’가 바로 convolution theorem 입니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;convolution in spatial/time domain is equivalent to multiplication in Fourier domain&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/hnw3IeK.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 16. convolution theorem&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;frequency 영역에서의 convolution은 단순 곱으로 계산될 수 있기 때문에 훨씬 편리합니다. 마찬가지로 graph 영역에서도 graph signal을 fourier transform으로 frequency 도메인으로 바꿔서 계산하면 마찬가지로 편리해집니다. 또한 노드와 가까이 있는 이웃노드에서부터 멀리 떨어져 있는 노드에서 오는 신호까지 모두 고려하여 graph signal의 특징을 추출할 수 있게 되는 것입니다. 이것이 바로 ‘spectral graph convolution’ 입니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;멀리 떨어져 있는 노드에서 오는 신호까지 고려한다는 것이 사실 ‘convolution’ 입니다. convolution은 현재 출력 값은 현재 입력값 뿐만 아니라 이전 입력값에도 영향을 받는다는 것을 고려한 연산입니다. 즉, 멀리 떨어져 있는 노드에서 오는 신호는 비교적 최근에 온 신호가 될 것이고, 이웃노드에서 온 신호가 비교적 이전 시간에서 이미 영향을 준 신호입니다. 그러나 이를 모든 시간에 대해 분해에서 연산하기가 어렵기 때문에(시간 영역에서의 convolution 연산이 어렵기 때문에) frequency 영역으로 바꿔서 간편하게 계산을 하는 것입니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;따라서, spectral graph convolution 식은 아래와 같습니다. 이 식에서 학습할 filter는 $\mathbf g$ 입니다.&lt;/p&gt;

\[\mathbf {x} * G \mathbf {g} = \mathcal F^{-1}(\mathcal {F}(\mathbf {x}) \odot \mathcal {F}(\mathbf {g})) = \mathbf {U}(\mathbf {U^{\intercal}x} \odot \mathbf {U^{\intercal}g})\]

&lt;p&gt;$\mathbf {g}_\theta = diag(\mathbf {U^{\intercal}g})$ 라 가정한다면(학습할 filter가 대각요소만 있다고 가정한다면), 위의 식은 아래와 같이 됩니다.&lt;/p&gt;

\[\mathbf {x} * \mathbf {g}_\theta = \mathbf {U} \mathbf {g}_{\theta} \mathbf {U^{\intercal}x}\]

&lt;hr /&gt;

&lt;p&gt;이번 포스팅은 spectral graph convolution 연산을 이해하기 위해서 fourier transform, laplacian operator 와 graph fourier transform을 살펴보고, 마지막으로 spectral graph convolution을 설명하였습니다. &lt;a href=&quot;https://ralasun.github.io/deep%20learning/2021/03/06/gcn(2)/&quot;&gt;다음 포스팅&lt;/a&gt;에서 이어서 spectral-based CNN에 대해서 살펴보도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;spectral-based CNN을 깊게 이해하기 위해 리서치를 하고 이해한 내용을 최대한 정리하였으나 전공 분야가 아니라 부족한 부분이 많을 거라고 예상됩니다. 혹시나 내용이 틀렸거나 문의가 있으시면 메일이나 댓글 달아주세요!&lt;/p&gt;

&lt;hr /&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1609.02907&quot;&gt;Kipf, Thomas N., and Max Welling. “Semi-supervised classification with graph convolutional networks.” arXiv preprint arXiv:1609.02907 (2016).&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1812.08434&quot;&gt;Zhou, Jie, et al. “Graph neural networks: A review of methods and applications.” arXiv preprint arXiv:1812.08434 (2018).&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://dsba.korea.ac.kr/seminar/?mod=document&amp;amp;pageid=1&amp;amp;keyword=spectral&amp;amp;uid=1330&quot;&gt;DSBA 연구실 세미나 자료, [Paper Review] MultiSAGE - Spatial GCN with Contextual Embedding&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;푸리에 변환 참고 페이지
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://darkpgmr.tistory.com/171&quot;&gt;https://darkpgmr.tistory.com/171&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.math.ucla.edu/~tao/preprints/fourier.pdf&quot;&gt;https://www.math.ucla.edu/~tao/preprints/fourier.pdf&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://angeloyeo.github.io/2019/10/11/Fourier_Phase.html&quot;&gt;https://angeloyeo.github.io/2019/10/11/Fourier_Phase.html&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Laplacian Operator
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://angeloyeo.github.io/2019/08/25/laplacian.html&quot;&gt;https://angeloyeo.github.io/2019/08/28/laplacian.html&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/spectral-graph-convolution-explained-and-implemented-step-by-step-2e495b57f801&quot;&gt;https://towardsdatascience.com/spectral-graph-convolution-explained-and-implemented-step-by-step-2e495b57f801&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Second_derivative&quot;&gt;https://en.wikipedia.org/wiki/Second_derivative&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Graph Fourier Transform
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1211.0053&quot;&gt;Shuman, David I., et al. “The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains.” IEEE signal processing magazine 30.3 (2013): 83-98.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;spectral graph convolution
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://trip2ee.tistory.com/101&quot;&gt;https://trip2ee.tistory.com/101&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://m.blog.naver.com/PostView.nhn?blogId=namunny&amp;amp;logNo=110183516999&amp;amp;proxyReferer=https:%2F%2Fwww.google.com%2F&quot;&gt;https://m.blog.naver.com/PostView.nhn?blogId=namunny&amp;amp;logNo=110183516999&amp;amp;proxyReferer=https:%2F%2Fwww.google.com%2F&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1312.6203.pdf%20http://arxiv.org/abs/1312.6203&quot;&gt;Bruna, Joan, et al. “Spectral networks and locally connected networks on graphs.” arXiv preprint arXiv:1312.6203 (2013).&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

</content>
 </entry>
 
 <entry>
   <title>Introduction to Graph Neural Network - GNN 소개 및 개념</title>
   <link href="http://localhost:4000/deep%20learning/2021/02/11/gcn/"/>
   <updated>2021-02-11T00:00:00+09:00</updated>
   <id>http://localhost:4000/deep%20learning/2021/02/11/gcn</id>
   <content type="html">&lt;p&gt;이번 포스팅을 시작으로, Graph Neural Network(GNN)에 대해 본격적으로 다루도록 하겠습니다. 이번 포스팅은 Graph Neural Network가 나온 배경 및 기본적인 구조와 개념에 대해 다루도록 하겠습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;우리가 흔히 많이 보는 데이터의 종류로는 이미지, 정형 데이터, 텍스트가 있습니다. 이미지는 2-D grid 형식인 격자 형식을 가지며, 정형 테이터는 테이블 형태를 띕니다. 또한 텍스트는 1-D sequence로 생각할 수 있습니다. 즉, 이들 데이터는 ‘격자’의 모양으로 표현할 수 있으며 이는 Euclidean space 상에 있는 것을 뜻합니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/0bBI5DP.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 1. Euclidean space vs. Non-Euclidean space&lt;/figcaption&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img width=&quot;300&quot; src=&quot;https://imgur.com/wsEg0pl.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 2. 3D mesh 이미지&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;그러나, social network 데이터, molecular 데이터, 3D mesh 이미지 데이터(그림 2.)는 ‘비’ Eculidean space 데이터입니다. 그렇다면 기존 CNN과 RNN계열의 모델과 다르게 이런 형태의 데이터를 처리할 수 있는 새로운 모델이 필요합니다. 그것이 바로 Graph Neural Network 입니다.&lt;/p&gt;

&lt;h1&gt;What is graph?&lt;/h1&gt;

&lt;p&gt;GNN을 본격적으로 시작하기 전에 그래프에 대해서 알아보도록 하겠습니다. 그래프란 $G = (N, E)$ 로 구성된 일종의 자료 구조입니다. V는 노드들의 집합이고, E는 노드 사이를 연결하는 엣지들의 집합입니다. 노드에는 일반적으로 데이터의 정보가 담겨있고, 엣지는 데이터 간의 관계 정보가 포함되어 있습니다. 또한, 아래와 같은 그래프 형태를 ‘undirected graph’ 라고도 합니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/rRWSycm.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 3. graph&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;directed graph&lt;/b&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img width=&quot;300&quot; src=&quot;https://imgur.com/HO2ho4k.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 4. directed graph&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;방향 그래프란 엣지가 방향성을 가지는 그래프입니다. 아래 그림에서 $V_2$ 에서 $V_1$ 으로 향하는 엣지 $e_1$ 이 있다면, $V_2$ 를 predecessor, $V_1$ 을 sucessor 라고 부릅니다. 그리고 $e_1$ 을 $V_2$ 의 outgoing edge, $V_1$ 의 incoming edge 라고 합니다.&lt;/p&gt;

&lt;p&gt;그렇다면, 이러한 그래프를 네트워크의 인풋으로 넣기 위해선 행렬 형태로 표현해야 합니다. 따라서 그래프를 표현하기 위한 방법으로는 adjacency matrix, degree matrix, laplacian matrix가 있습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/bYiaa4S.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 5. degree vs. adjacency vs. laplacian&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Adjacency matrix&lt;/b&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;adjacency 행렬은 그래프 노드의 개수가 N개라면, NxN 정사각 행렬입니다. i노드와 j노드가 연결되어 있으면 $A_{ij} = 1$ 아니면 $A_{ij} = 0$ 의 성분을 가집니다.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Degree matrix&lt;/b&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Degree 행렬은 그래프 노드의 개수가 N개라면 NxN 크기를 가지는 대각행렬입니다. 각 꼭짓점의 차수에 대한 정보를 포함하고 있는 행렬로, 꼭짓점의 차수란 꼭짓점와 연결된 엣지의 갯수를 말합니다.&lt;/p&gt;

\[D_{i,j} = \begin{cases} deg(v_i) \quad if \,\, i=j \\
		                      0 \quad otherwise \end{cases}\]

&lt;p&gt;&lt;b&gt;Laplacian matrix&lt;/b&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;adjacency 행렬은 노드 자신에 대한 정보가 없습니다. 그에 반해 laplacian 행렬은 노드와 연결된 이웃노드와 자기 자신에 대한 정보가 모두 포함된 행렬입니다. laplacian 행렬은 degree 행렬에서 adjacency 행렬을 빼준 것입니다.&lt;/p&gt;

\[L = D - A\]

\[L_{i,j} = \begin{cases}
		deg(v_i) \quad if \,\, i=j \\
		 -1 \quad if \,\, i \neq j \\
		  0 \quad otherwise \end{cases}\]

&lt;h1&gt;Motivation : GNN $\approx$ CNN&lt;/h1&gt;

&lt;p&gt;다시 GNN으로 돌아오겠습니다. GNN의 아이디어는 Convolutional Neural Network(CNN)에서 시작되었습니다. CNN은 아래와 같은 특징을 가지고 있습니다.&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;local connectivity&lt;/li&gt;
&lt;li&gt;shared weights&lt;/li&gt;
&lt;li&gt;use of Multi-layer&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;위와 같은 특징 때문에, CNN은 spatial feature를 계속해서 layer마다 계속해서 추출해 나가면서 고차원적인 특징을 표현할 수 있습니다. 위와 같은 특징은 마찬가지로 graph 영역에도 적용할 수 있습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/qa04Jf2.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 6. GNN $\approx$ CNN&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Local Connectivity&lt;/b&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&amp;lt;그림 3.&amp;gt; 을 보면, CNN과 GNN의 유사한 점을 확인할 수 있습니다. 먼저, graph도 한 노드와 이웃노드 간의 관계를 local connectivity라 볼 수 있기 때문에, 한 노드의 특징을 뽑기 위해서 local connection에 있는 이웃노드들의 정보만 받아서 특징을 추출할 수 있습니다. 즉, CNN의 filter의 역할과 유사합니다.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Shared Weights&lt;/b&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;또한 이렇게 graph 노드의 특징을 추출하는 weight은 다른 노드의 특징을 추출하는데도 동일한 가중치를 사용할 수 있어(shared weight), computational cost를 줄일 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Use of Multi-layer&lt;/b&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;CNN에서 multi layer 구조로 여러 레이어를 쌓게 되면 초반에는 low-level feature위주로 뽑고, 네트워크가 깊어질수록 high level feature를 뽑습니다. &lt;span style=&quot;color:red&quot;&gt;graph같은 경우에 multi-layer구조로 쌓게되면 초반 layer는 단순히 이웃노드 간의 관계에 대해서만 특징을 추출하지만, 네트워크가 깊어질수록 나와 간접적으로 연결된 노드의 영향력까지 고려된 특징을 추출할 수 있게 됩니다.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;그렇다면, 위와 같은 특성을 가지려면 GNN은 어떻게 인풋 그래프에 대하여 연산을 해야하는지 알아보도록 하겠습니다.&lt;/p&gt;

&lt;h1&gt;Original Graph Neural Network&lt;/h1&gt;

&lt;p&gt;graph neural network는 &lt;a href=&quot;https://www.infona.pl/resource/bwmeta1.element.ieee-art-000004700287&quot;&gt;Scarselli et al.의 The Graph Neural Network Model&lt;/a&gt;에서 처음 등장했습니다. GNN의 목적은 결국 이웃노드들 간의 정보를 이용해서 해당 노드를 잘 표현할 수 있는 특징 벡터를 잘 찾아내는 것입니다. 이렇게 찾아낸 특징 벡터를 통해 task를 수행할 수 있습니다(graph classification, node classification 등).&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img width=&quot;400&quot; src=&quot;https://imgur.com/eDqPQFW.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 7. GNN&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;GNN의 동작은 따라서 크게 두가지로 생각할 수 있습니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;propagation step - 이웃노드들의 정보를 받아서 현재 자신 노드의 상태를 업데이트 함&lt;/li&gt;
  &lt;li&gt;output step - task 수행을 위해 노드 벡터에서 task output를 출력함&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;이를 수식으로 표현하면 아래와 같습니다.&lt;/p&gt;

\[x_n = f_w(l_n, l_{co[n]}, x_{ne[n]}, l_{ne[n]})\]

\[o_n = g_w(x_n, l_n)\]

&lt;p&gt;이때, $l_n, l_{co[n]}, x_{ne[n]}, l_{ne[n]}$ 은 각각 n 노드의 라벨, n노드와 연결된 엣지들의 라벨, 이웃노드들의 states, 이웃노드들의 라벨입니다. 또한 $f_w$ 와 $o_w$ 는 각각 propagation function(논문에선 transition function 이라 표현함)와 output function입니다.&lt;/p&gt;

&lt;p&gt;propagation function(transition function)은 이웃 노드들의 정보와 노드와 연결된 엣지정보들을 토대로 현재 자신의 노드를 표현합니다. 즉, d-차원의 공간에 이러한 인풋들을 받아서 맵핑하는 과정이라 생각할 수 있습니다. output function은 task 수행을 위해 학습을 통해 얻은 node feature을 입력으로 하여 output을 얻습니다. 예를 들어, node label classification 이라면 node label이 아웃풋이 될 것입니다.&lt;/p&gt;

&lt;h3&gt;Learning algorithm : Banach fixed point theorem&lt;/h3&gt;

&lt;p&gt;그렇다면 어떻게 학습이 이뤄질까요 ? 위에서 Motivation : GNN $\approx CNN$ 에서 Multi-layer를 GNN에 사용하면 얻는 이점은 layer가 깊어질수록 직접적으로 연결된 이웃 노드 이외에 멀리 있는 노드들의 영향력을 고려하여 현재 노드의 feature를 구성할 수 있다고 하였습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;이렇게 멀리있는 노드에서부터 현재 노드까지 정보가 전달되는 과정을 message passing이라고 합니다. message passing이란 개념은 GNN이 등장하고 난 이후에, Gilmer et al.의 “Neural message passing for quantumchemistry” 에서 등장하였습니다. 해당 논문은 여러 종류의 GNN 구조를 일반화하는 프레임워크를 message passing 이라는 것으로 제안한 논문입니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;하지만, 초기 GNN은 multi-layer 구조가 아니기 때문에 불가능합니다. 따라서, Banach fixed point theorem에 따라, iterative method로 고정된 node feature를 찾습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img width=&quot;400&quot; src=&quot;https://imgur.com/UIfPnoL.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 8. Network obtained by unfolding the encoding network&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;$x_n$ 와 $o_n$ 이 어떤 수렴된 값을 가지려면, Banach fixed point theorem에 의하면 propagation function이 contraction map이어야 합니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;$F_w$ is a &lt;em&gt;contraction map&lt;/em&gt; with respect to the state, i.e., there exists $\mu$ , $0 \leq \mu \le 1$ , such that $|F_w(x, l) - F_w(y, l)| \leq \mu |x-y|$ holds for any x, y where $| \cdot |$ denotes a vectorial norm.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;b&gt;Contraction Map에 대한 개인적인 생각&lt;/b&gt;&lt;br /&gt;
사실 contraction map의 수학적인 이해가 완벽하게 되진 않았습니다. 그러나, 제가 생각하는 contraction map은 다음과 같습니다. 선형대수학에서 선형변환을 진행하면, m차원의 벡터가 n차원의 공간으로 맵핑이 됩니다. 이 때, 서로 다른 두 m차원의 벡터가 n차원의 공간으로 맵핑이 되었을 때, 두 벡터 사이의 거리가 줄어드는 방향이라면 이 맵핑 function은 contraction map입니다.&lt;/p&gt;

&lt;p&gt;그렇다면 fixed point가 되려면, 즉 수렴된 node feature들은 contraction map에 의해 정의된 공간 안에서 존재하는 것이고, 어떻게 보면 node feature를 서치하는 범위가 작다라고 생각할 수 있습니다.&lt;/p&gt;

&lt;p&gt;이러한 문제 때문에 추후 다양한 버전의 GNN은 이러한 제한된 가정을 두지 않고 우리가 딥러닝 네트워크 학습시 사용하는 방식으로 node feature 값을 찾습니다. 즉, node feature의 search space가 훨씬 넓어지는 것입니다.&lt;/p&gt;

&lt;p&gt;다시 돌아와서, 그렇다면 iterative method식으로 수식을 전개하면 아래와 같이 전개할 수 있습니다.&lt;/p&gt;

\[x_n(t+1) = f_w(l_n, l_{co[n]}, x_{ne[n]}(t), l_{ne[n]})\]

\[o_n(t) = g_w(x_n(t), l_n), \quad n \in N\]

&lt;p&gt;fixed 된 $x_n, o_n$ 을 얻으면 아래와 같은 loss를 계산할 수 있고, gradient 계산을 통해 weight을 업데이트합니다. 여기서 weight은 $F_w$ 의 파라미터 입니다. neural network 라면 network의 가중치가 됩니다.&lt;/p&gt;

&lt;h1&gt;Variants of GNNs&lt;/h1&gt;

&lt;p&gt;Scarselli의 GNN 이후로 여러 변형된 GNN이 많이 등장하였습니다. 초기 GNN은 학습 방식의 단점에 의해 수렴이 잘 되지 않는다는 문제가 있습니다. 이러한 문제를 해결하기 위해 초기 GNN 이후에 다양한 GNN이 등장하였습니다. 대표적으로 Graph Convolutional Network와 Gated Graph Neural Network 등이 있습니다.&lt;/p&gt;

&lt;p&gt;다음 포스팅부터는 GNN이 더욱 더 유명해진 계기가 된 Graph Convolutional Network에 대해 다루도록 하겠습니다. 읽어주셔서 감사합니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1812.08434&quot;&gt;Zhou, Jie, et al. “Graph neural networks: A review of methods and applications.” arXiv preprint arXiv:1812.08434 (2018).&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;What is graph?, &lt;a href=&quot;https://ratsgo.github.io/data%20structure&amp;amp;algorithm/2017/11/18/graph/&quot;&gt;https://ratsgo.github.io/data%20structure&amp;amp;algorithm/2017/11/18/graph/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.infona.pl/resource/bwmeta1.element.ieee-art-000004700287&quot;&gt;Scarselli, F., et al. “The Graph Neural Network Model.” IEEE Transactions on Neural Networks 1.20 (2009): 61-80.&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</content>
 </entry>
 
 <entry>
   <title>TADA, Trend Alignment with Dual-Attention Multi-Task Recurrent Neural Networks for Sales Prediction 논문 리뷰</title>
   <link href="http://localhost:4000/deep%20learning/2021/01/18/tada/"/>
   <updated>2021-01-18T00:00:00+09:00</updated>
   <id>http://localhost:4000/deep%20learning/2021/01/18/tada</id>
   <content type="html">&lt;hr /&gt;

&lt;p&gt;다변량 시계열 예측 모델에 관한 논문으로, 다변량 시계열 데이터를 가지고 encoder-decoder RNN 모델 기반으로 dual-attention과 multi-task RNN으로 구성된 모델입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2&gt;Problem&lt;/h2&gt;
&lt;p&gt;다변량 시계열 예측을 위한 여러 통계 기반 모델링이 있으나, 판매량에 영향을 주는 변수들 간의 관계를 파악하기 어렵고, 이 변수들로 부터 의미있는 정보(contextual information)을 추출하는 건 더욱 어렵습니다. 예를 들어, 겨울의복은 날씨에 영향에 두드러지게 받지만, 일반적인 셔츠는 사계절내내 잘 입는 옷이기 때문에 겨울의복보단 계절의 영향을 덜 받습니다. 또한 소비자의 주관적인 선호도(브랜드 선호도, 상품 선호도 등)에 따라 상품 판매는 크게 또한 달라지게 됩니다. 따라서, 본 논문에서 주목하는 다변량 시계열 예측에서의 문제는 아래와 같이 크게 세가지입니다.&lt;/p&gt;

&lt;ol&gt;&lt;li&gt;how to fully capture the dynamic dependencies among multiple influential factors?&lt;br /&gt;
판매에 영향을 주는 여러 변수들 간의 관계는 시간에 따라 변할 가능성이 높습니다. 그렇다면 매 스텝마다 변수들 간의 관계를 어떻게 포착할 수 있을까요 ?&lt;/li&gt;&lt;li&gt;how can we possibly glean wisdoms form the past to compensate for the unpredictability of influential factors?&lt;br /&gt;이 변수들이 미래에 어떻게 변할지는 아무도 모릅니다. 그렇다면 과거 이 변수들의 정보만을 가지고 어떻게 미래를 눈여겨 볼 수 있는 정보를 추출할지는 생각해 봐야 합니다.&lt;/li&gt;
&lt;li&gt;how to align the upcoming trend with historical sales trends?&lt;br /&gt;현실 시계에서의 판매 트랜드는 전혀 규칙적이지 않습니다. 그렇다면 과거 판매 트렌드를 어떻게 하면 현실 트렌드와 연관지을 수 있을까요 ?&lt;/li&gt;&lt;/ol&gt;

&lt;h2&gt;TADA : Trend Alignment with Dual-Attention Multi-Task RNN&lt;/h2&gt;

&lt;h3&gt;Problem Formulation&lt;/h3&gt;

&lt;p&gt;본 논문에서 풀고자 하는 다변량 시계열 예측 문제는 아래와 같이 수학적으로 정의됩니다.&lt;/p&gt;

\[\{\hat{y_t\}}^{T+\triangle}_{t=T+1} = F({\{\mathbf x_t\}}^{T}_{t=1}, {\{y_t\}}^{T}_{t=1})\]

&lt;p&gt;$\mathbf x_t$ 는 influential factors로 판매량 이외의 변수(ex. 날씨, 브랜드, 상품인덱스 등)이고, $y_t$ 는 판매량 입니다.&lt;/p&gt;

&lt;h3&gt;TADA 모델 개요&lt;/h3&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/w09ZSHF.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 1. 모델 개요&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;위의 그림은 본 논문의 모델 개요입니다. 크게 아래와 같이 구성되어 있습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Multi-task based Encoder Structures&lt;/li&gt;
  &lt;li&gt;Dual-Attention based Decoder Structures
    &lt;ul&gt;
      &lt;li&gt;Attention got weighted decoder input mapping&lt;/li&gt;
      &lt;li&gt;attention for trend alignment&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;Multi-task based Encoder Structures&lt;/h4&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/leH0yfV.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 2. Multi-task based Encoder&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;influential factor의 semantic한 특징을 잘 추출한다면 분명 예측에 도움이 될 것입니다. 그러나 매 타임 스텝마다 어떻게 하면 판매량 예측에 도움될 semantic한 특징을 추출할 수 있을까요 ? 본 논문에서는 influential factor를 크게 intrinsic한 속성과 objective한 속성으로 나누어 LSTM을 이용한 인코딩을 각각 따로하였습니다. 이를 통해 각각 두 개의 LSTM(intrinsic LSTM, external LSTM)을 통해 각기 다른 semantic한 특징을 추출할 수 있습니다. 따라서, 위의 문제 정의는 아래와 같이 다시 정의될 수 있습니다.&lt;/p&gt;

\[\{\hat{y_t\}}^{T+\triangle}_{t=T+1} = F({\{\mathbf x_t^{int}\}}^{T}_{t=1}, {\{\mathbf x_t^{ext}\}}^{T}_{t=1}, {\{y_t\}}^{T}_{t=1})\]

&lt;p&gt;intrinsic한 속성이란 브랜드, 카테고리, 가격등 상품과 관련된 것이고, objective한 속성은 날씨, 휴일유무, 할인등과 관련된 속성입니다. 아래 표는 논문에서 실험한 데이터의 intrinsic/objective 속성입니다.&lt;/p&gt;

&lt;p&gt;하지만 우리가 구하고 싶은 건 두 가지의 다른 semantic한 feature를 적절하게 결합하여 의미있는 &lt;strong&gt;contextual vector&lt;/strong&gt;를 만드는 것입니다. 따라서 또다른 LSTM 네트워크인 Synergic LSTM을 구축하여 joint representation을 학습합니다. 이때, Synergic LSTM에 입력으로 들어가는 건 각 타임스텝에 해당되는 $h_t^{int}$ 와 $h_t^{ext}$ 뿐만 아니라 판매량 $y_t$ 도 같이 joint space가 구축되도록 학습됩니다.&lt;/p&gt;

&lt;p&gt;먼저, 두 타입스텝 t에서의 두 개의 hidden state을 $h_t^{int}$ 와 $h_t^{ext}$ 이용하여 Synergic LSTM의 인풋인 $\mathbf X_t^{syn}$ 을 아래와 같이 계산합니다.&lt;/p&gt;

\[\mathbf X_t^{syn} = \mathbf W_{syn}[\mathbf h_t^{int};\mathbf h_t^{ext};y_t]\]

&lt;p&gt;그런 다음, intrinsic LSTM/external LSTM과 동일하게 각 타임스텝마다 두 정보가 결합되어 인코딩된 hidden stated인 $\mathbf h^{con}_t$ 를 계산합니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/ijwajF4.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 3. Multi-task based Encoder(2)&lt;/figcaption&gt;&lt;/p&gt;

&lt;h4&gt;Dual-Attention based Decoder Structures&lt;/h4&gt;
&lt;p&gt;Multi-task Encoder를 통해 과거 판매량 시계열 데이터를 인코딩하면 contextual vectors인 ${\mathbf h_t^{con}}^T_{t=1}$ 이 계산되어 나옵니다. $h_t^{con}$ 은 타임스텝 t까지의 시계열 데이터에 대한 contextual 정보를 품고 있습니다.&lt;/p&gt;

&lt;p&gt;LSTM decoder도 encoder와 유사하게 예측에 필요한 contextual vector $\mathbf d_t^{con}$ 을 생성합니다. 따라서, $T &amp;lt; t \leq T + \Delta$ 에 대해 decoder 수학식은 아래와 같습니다.&lt;/p&gt;

\[\mathbf d_t^{con} = LSTM^{dec}(\mathbf x_t^{dec}, \mathbf d^{con}_{t-1})\]

&lt;p&gt;위 식에서 $\mathbf x_t^{dec}$ 는 attention weighted input입니다. 그러면 contextual vector가 어떻게 만들어지는지 보기 전에 attention weighted input 계산 과정을 살펴봅시다.&lt;/p&gt;

&lt;h5&gt;Attention for Weighted Decoder Input Mapping&lt;/h5&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/NvCkYMs.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 4. Attention for Weighted Decoder Input&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;Decoder에 입력될 Input은 encoder contextual vector들에서 각 디코터 타임 스텝에 필요한 정보를 적절하게 취하도록 하기 위해 attention 메카니즘을 통해 생성합니다.&lt;/p&gt;

\[\mathbf x_t^{dec} = \mathbf W_{dec}\left[\sum_{t'=1}^T \alpha_{tt'}^{int}\mathbf h_{t'}^{int};\sum_{t'=1}^T \alpha_{tt'}^{ext}\mathbf h_{t'}^{ext}\right] + \mathbf b_{dec}\]

&lt;p&gt;$\alpha_{tt’}^{int}$ 와 $\alpha_{tt’}^{ext}$ 는 어텐션 가중치를 의미합니다. 어텐션 가중치는 아래 과정을 통해 계산됩니다.&lt;/p&gt;

\[e^{int}_{tt'} = \mathbf v^{\mathrm T}_{int}tanh(\mathbf M_{int}\mathbf d_{t-1}^{con} + \mathbf H_{int}\mathbf h_{t'}^{int})\]

\[e^{ext}_{tt'} = \mathbf v^{\mathrm T}_{ext}tanh(\mathbf M_{int}\mathbf d_{t-1}^{con} + \mathbf H_{ext}\mathbf h_{t'}^{ext})\]

\[\alpha_{tt'}^{int} = \frac{exp(e_{tt'}^{int})}{\sum_{s=1}^{T}exp(e_{ts}^{int})}\]

\[\alpha_{tt'}^{ext} = \frac{exp(e_{tt'}^{ext})}{\sum_{s=1}^{T}exp(e_{ts}^{ext})}\]

&lt;p&gt;이때, $\sum_{t’=1}^{T}\alpha_{tt’}^{int} = \sum_{t’=1}^{T}\alpha_{tt’}^{ext} = 1$ 이 여야 합니다.&lt;/p&gt;

&lt;h5&gt;Attention for Trend Alignment&lt;/h5&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/riUczJ9.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 5. Attention for Trend Alignment&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;미래를 예측하기 위해선 과거의 trend 패턴을 안다면 좀 더 수월할 수 있습니다. 따라서, 미래에 예상되는 패턴과 유사한 패턴을 과거에서 찾는 작업을 attention을 통해 진행하는 과정을 본 논문에서 제안하였습니다. 그러나, 
일반적으로 attention 메카니즘은 현 타임스텝에서 아웃풋을 출력하기 위해 이전 hidden state들중에서 가장 align되는 정보를 선택합니다. 과거 정보들 중에서 &lt;strong&gt;미래의 트렌드와 유사한 트렌드 정보&lt;/strong&gt;를 선택적으로 이용하고 싶다면 전통적인 attention 메카니즘을 그대로 사용하기는 어렵습니다. 왜냐하면, 일반적인 데이터에선 trend외에 노이즈도 많이 포함하고 있기 때문입니다. 즉, 전체 데이터에 trend + noise라서 이전 모든 과거들에서 유사한 trend 패턴만을 집중하는 건 힘듭니다. 따라서 논문 저자는 아래와 같은 방법을 고안하였습니다.&lt;/p&gt;

&lt;p&gt;먼저, ${\mathbf h_t^{con}}_{t=1}^T$ 를 $\triangle$ 타임 스텝 크기에 해당되는  contextual vector를 이어붙여서 $\triangle$ -step trend vector를 생성합니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/yUmHRP0.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$\mathbf p_i$ 는 과거 시계열 데이터에서 $\triangle$ 간격에 해당되는 구간의 트렌드를 나타냅니다. $i$ 가 1씩 증가하므로, 마치 슬라이딩 윈도우 1씩 움직이면서 트렌드를 포착하는 것과 유사합니다.&lt;/p&gt;

&lt;p&gt;마찬가지 방식으로 decoder hidden state들을 이어 붙여 미래에 예상될 트렌드 정보를 생성합니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/I9C1wnQ.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;따라서, 그림5 처럼 과거에 생성된 트렌드 벡터과 미래 트렌드 벡터를 각각 내적하여 가장 큰 값에 해당되는 인덱스 i를 반환합니다. 내적값이 가장 크다는 것은 가장 유사함을 의미합니다.&lt;/p&gt;

\[e_i^{trd} = \mathbf p_i^{\mathrm T} \tilde{\mathbf p}\]

\[i' = argmax(e_i^{trd} , e_{i+1}^{trd},\dots, e_{T+\triangle -1}^{trd})\]

&lt;p&gt;그 다음 $\mathbf p_{i’}$ 내의 각 ${\mathbf d_t^{con}}$ 와 $\mathbf h_t^{con}$ 을 아래와 같은 계산과정을 거쳐서 $\tilde {\mathbf d}^{con}$ 을 생성합니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/haL8Udd.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$\tilde {\mathbf d}^{con}$ 은 타임스텝 t에서의 과거 유사한 트렌드 정보에 집중하여 생성된 aligned contextual vector 입니다.&lt;/p&gt;

&lt;h4&gt;Sales Prediction and Model Learning&lt;/h4&gt;

&lt;p&gt;위에서 생성된 aligned contextual vector ${\widetilde {\mathbf d_t}^{con}}$ 를 가지고 판매량을 예측합니다.&lt;/p&gt;

\[\hat y_t = \mathbf v_y^{\mathrm T} \mathbf {\widetilde d}^{con} + b_y\]

&lt;p&gt;$\hat y_t , \,\,(T+1 \leq t \leq T+\Delta)$ 는 타임스텝 T에서의 예측된 판매량입니다.&lt;/p&gt;

&lt;p&gt;본 논문에서 학습은 L2 regularization과 함께 Mean Squared Error를 minimize하였습니다.&lt;/p&gt;

&lt;h2&gt;Experiment and result.&lt;/h2&gt;

&lt;p&gt;전체적인 결과에 관한 건 논문을 참고 바랍니다. trend alignment 부분에 대한 결과를 살펴보면 과거 유사하다고 찾은 trend와 예측된 trend는 아래 그래프와 같이 나왔습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/F5w1W2o.png&quot; /&gt;&lt;/p&gt;
&lt;p&gt;보면 어느정도 과거 매치된 트렌드와 유사한 트렌드를 따르는 것을 확인할 수 있었습니다.&lt;/p&gt;

&lt;h2&gt;Lessons Learned&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;결과를 보면 어느정도 lag가 발생하는 것으로 보입니다.&lt;/li&gt;
  &lt;li&gt;trend에 대한 파악을 먼저하고 판매량 데이터 입력을 나중에 하면 어떨까?&lt;/li&gt;
  &lt;li&gt;dual-stage attention에서의 input attention 모듈과 multi-tasked encoder를 결합하는 건 어떨까 ??&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;이상으로 본 논문 리뷰를 마치겠습니다.&lt;/p&gt;

&lt;hr /&gt;
&lt;ol&gt;
  &lt;li&gt;Chen, Tong, et al. “Tada: trend alignment with dual-attention multi-task recurrent neural networks for sales prediction.” 2018 IEEE International Conference on Data Mining (ICDM). IEEE, 2018.&lt;/li&gt;
&lt;/ol&gt;
</content>
 </entry>
 
 <entry>
   <title>A Dual-Stage Attention-Based Recurrent Neural Network for Time-Series Prediction 논문 리뷰</title>
   <link href="http://localhost:4000/deep%20learning/2021/01/18/dual-stage-attention/"/>
   <updated>2021-01-18T00:00:00+09:00</updated>
   <id>http://localhost:4000/deep%20learning/2021/01/18/dual-stage-attention</id>
   <content type="html">&lt;p&gt;A Dual-Stage Attention-Based Recurrent Neural Network는 다변량 시계열 예측 모델입니다(Multi-Variate Time Series Prediction). Bahdanau et al.의 &lt;a href=&quot;https://arxiv.org/abs/1409.0473&quot;&gt;Attention network 기반 시퀀스 모델&lt;/a&gt; 을 베이스로, 인코더 뿐만 아니라 디코더에도 Attention netowork를 이용해 예측을 위한 다변량 시계열 변수 간 상대적인 중요도와 타임 스텝 간 상대적인 중요도를 모두 고려한 모델입니다.&lt;/p&gt;

&lt;h2&gt;Problem&lt;/h2&gt;

</content>
 </entry>
 
 <entry>
   <title>RDD, Resilient Distributed Dataset에 대하여[3] - RDD액션, RDD 데이터 불러오기와 저장하기, 공유변수</title>
   <link href="http://localhost:4000/spark%20programming/2021/01/07/rdd(3)/"/>
   <updated>2021-01-07T00:00:00+09:00</updated>
   <id>http://localhost:4000/spark%20programming/2021/01/07/rdd(3)</id>
   <content type="html">&lt;p&gt;이번 포스팅은 지난 포스팅 &amp;lt;RDD, Resilient Distributed DataSet에 대하여[2] - RDD기본액션, RDD트랜스포메이션&amp;gt; 에 이어서 진행하도록 하겠습니다. 교재는 빅데이터 분석을 위한 스파크2 프로그래밍을 참고하였습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2&gt;2.1.6 RDD 액션&lt;/h2&gt;
&lt;p&gt;RDD트랜스포메이션 연산은 느긋한 평가(lazy evaluation) 또는 지연 계산 방식을 따릅니다. 이는 계산에 필요한 정보를 누적하다가 계산이 필요한 시점이 돼서야 계산을 수행하는 방식을 뜻합니다. 여기서 계산이 필요한 시점은 RDD 액션 메서드가 호출된 시점입니다. RDD 액션 메서드가 호출이 되어야 비로소 RDD 트랜스포메이션 연산이 수행되게 됩니다.&lt;/p&gt;

&lt;h3&gt;1. 출력 연산&lt;/h3&gt;
&lt;h4&gt;1.1. first&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;RDD 요소 중 ,첫번째 요소를 돌려줌&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize(range(50))
&amp;gt;&amp;gt;&amp;gt; result = rdd.first()
&amp;gt;&amp;gt;&amp;gt; print(result)
0
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;1.2. take&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;RDD 요소중, n번째까지 요소를 돌려줌&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize(range(50))
&amp;gt;&amp;gt;&amp;gt; result = rdd.take(5)
&amp;gt;&amp;gt;&amp;gt; print(result)
[0, 1, 2, 3, 4]
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;1.3. takeSample&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;지정된 크기의 sample을 추출해서 리스트, 배열 타입등으로 반환함&lt;/li&gt;
  &lt;li&gt;sample 메서드와의 차이점
    &lt;ul&gt;
      &lt;li&gt;sample 메서드는 RDD 트랜스포메이션 메서드이고, 크기를 지정할 수 없음.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;takeSample(withReplacement, num, seed=None)&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize(range(100))
&amp;gt;&amp;gt;&amp;gt; result = rdd.takeSample(False, 3)
&amp;gt;&amp;gt;&amp;gt; result
[55, 23, 45]
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;1.5. countByValue&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;RDD의 요소들이 나타낸 횟수를 맵 형태로 돌려주는 메서드&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize([1,1,3,2,1,2,2,1,1,4,5,3,2,3])
&amp;gt;&amp;gt;&amp;gt; result = rdd.countByValue()
&amp;gt;&amp;gt;&amp;gt; print(result)
defaultdict(&amp;lt;class 'int'&amp;gt;, {1: 5, 3: 3, 2: 4, 4: 1, 5: 1})
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;1.6. reduce&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;reduce 메서드 인자는 함수가 들어감.&lt;/li&gt;
  &lt;li&gt;그 함수는 교환법칙과 결합법칙이 성립하는 함수여야 함.&lt;/li&gt;
  &lt;li&gt;따라서, 메서드 인자로 받은 함수를 이용해서 하나의 요소로 합치는 메서드임.&lt;/li&gt;
  &lt;li&gt;def reduce(f: (T,T)=&amp;gt;T):T
    &lt;ul&gt;
      &lt;li&gt;동일한 타입 2개를 입력으로 받아, 같은 타입으로 반환해주는 메서드임&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;실제 구현은 파티션단위로 나눠져서 처리됨. 분산 프로그램이기 때문임.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; from operator import add
&amp;gt;&amp;gt;&amp;gt; add(1,2)
3
&amp;gt;&amp;gt;&amp;gt; sc.parallelize([1,2,3,4,5]).reduce(add)
15
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;1.7. fold&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;reduce와 동일하나, 초기값을 설정할 수 있음&lt;/li&gt;
  &lt;li&gt;def fold(zeroValue: T)(op: (T,T)=&amp;gt;T):T&lt;/li&gt;
  &lt;li&gt;그런데 유의할 점은 파티션단위로 나뉘어서 처리하기 때문에, 파티션단위로 처리할 때마다 초깃값을 이용하여 연산이 수행됨. 따라서, 더하기 연산을 할 땐 항등원인 0을, 곱셈 연산을 할 땐 항등원인 1을 초깃값으로 설정하는 것이 좋음&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize(range(1,11), 3)
&amp;gt;&amp;gt;&amp;gt; rdd.fold(1, add)
59 #값이 55가 아니라 59가 나오는 것을 확인할 수 있음. 
&lt;/code&gt;&lt;/pre&gt;

&lt;h5&gt;reduce와 fold차이&lt;/h5&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;#product.py
class Product:
    def __init__(self, price):
        self.price = price
        self.count = 1
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;def addPriceandCount(p1, p2):
    p1.price += p2.price
    p1.count += 1
    return p1 #return을 p1인 이유 --&amp;gt; 입력값과 출력값의 타입이 동일해야 함.

if __name__ =='__main__':
    conf = SparkConf()
    conf.set(&quot;spark.driver.host&quot;, &quot;127.0.0.1&quot;)
    sc = SparkContext(master=&quot;local[*]&quot;, appName=&quot;RDDOpSample&quot;, conf=conf)

    rdd = sc.parallelize([Product(300), Product(200), Product(100)], 10)

    #reduce
    result = rdd.reduce(addPriceandCount)
    print(result.price, result.count)

    #fold
    result = rdd.fold(Product(0), addPriceandCount)
    print(result.price, result.count)
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;fold의 count합을 보면 11인 것을 알 수 있음. 그 이유는 위에서 파티션 개수를 10으로 지정하였고, 파티션 단위로 연산을 초기값을 이용하여 연산을 수행하기 때문임&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;1.8. aggregate&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;입력와 출력의 타입이 다른 경우 사용 가능&lt;/li&gt;
  &lt;li&gt;def aggregate&lt;a href=&quot;zeroValue: U&quot;&gt;U&lt;/a&gt;(seqOp:(U,T)=&amp;gt;U, combOp:(U,U)=&amp;gt;U):U
    &lt;ul&gt;
      &lt;li&gt;크게 세가지 인자를 받음. 첫번째는 초깃값으로 fold와 동일&lt;/li&gt;
      &lt;li&gt;aggregate은 병합을 크게 2단계로 구성되는데, 1단계는 seqOp에 의해, 2단계는 combOp에 의해 진행됨&lt;/li&gt;
      &lt;li&gt;seqOp는 초깃값과 동일한 타입(U)과 RDD요소 타입(T)가 입력되어 병합 결과 초깃값과 동일한 타입인 U가 반환됨&lt;/li&gt;
      &lt;li&gt;combOp는 최종병합에서 사용됨&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;#rdd에 속한 요소들의 평균을 aggregate을 이용하여 구하는 예제
#record.py
class Record:

    def __init__(self, amount, number=1):
        self.amount = amount
        self.number = number

    def addAmt(self, amount):
        return Record(self.amount + amount, self.number + 1)

    def __add__(self, other):
        amount = self.amount + other.amount
        number = self.number + other.number
        return Record(amount, number)

    def __str__(self):
        return &quot;avg:&quot; + str(self.amount / self.number)

    def __repr__(self):
        return 'Record(%r, %r)' % (self.amount, self.number)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;def seqop(r,v):
    return r.addAmt(v)

if __name__ =='__main__':
    conf = SparkConf()
    conf.set(&quot;spark.driver.host&quot;, &quot;127.0.0.1&quot;)
    sc = SparkContext(master=&quot;local[*]&quot;, appName=&quot;RDDOpSample&quot;, conf=conf)

    rdd = sc.parallelize([100,80,75,90,95], 3)

    #aggregate
    result = rdd.aggregate(Record(0,0), seqop, lambda r1, r2:r1+r2)
    print(result) # avg:88.0
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;1.9. sum&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;모든 요소의 합을 구해주며, Double, Long등 숫자타입인 경우에만 사용가능&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize(range(1,11))
&amp;gt;&amp;gt;&amp;gt; rdd.sum()
55
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;1.10. foreach, foreachPartition&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;foreach는 RDD의 개별요소에 전달받은 함수를 적용하는 메서드이고, foreachPartition은 파티션 단위로 적용됨&lt;/li&gt;
  &lt;li&gt;이때 인자로 받는 함수는 한개의 입력값을 가지는 함수임&lt;/li&gt;
  &lt;li&gt;이 메서드를 사용할 때 유의할 점은 &lt;strong&gt;드라이버 프로그램(메인 함수를 포함하고 있는 프로그램)이 작동하고 있는 서버위가 아니라 클러스터의 각 개별 서버에서 실행된다는 것&lt;/strong&gt;임&lt;/li&gt;
  &lt;li&gt;따라서 foreach() 인자로 print함수를 전달한다는 것은 각 서버의 콘솔에 출력하라는 의미가 됨.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;def sideEffect(values):
    print(&quot;partition side effect&quot;)
    for v in values:
        print(&quot;value side effect : %s&quot; %v)

if __name__ =='__main__':
    conf = SparkConf()
    conf.set(&quot;spark.driver.host&quot;, &quot;127.0.0.1&quot;)
    sc = SparkContext(master=&quot;local[*]&quot;, appName=&quot;RDDOpSample&quot;, conf=conf)

    rdd = sc.parallelize(range(1,11),3)
    result = rdd.foreach(lambda v:print(&quot;value side effect: %s&quot; %v))
    result2 = rdd.foreachPartition(sideEffect)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;###
value side effect: 2
value side effect: 3
value side effect: 4
value side effect: 5
value side effect: 6
value side effect: 7
value side effect: 8
value side effect: 9
value side effect: 10
partition side effect
value side effect : 7
value side effect : 8
value side effect : 9
value side effect : 10
partition side effect
value side effect : 4
value side effect : 5
value side effect : 6
partition side effect
value side effect : 1
value side effect : 2
value side effect : 3
###
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;1.11. toDebugString&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;디버깅을 위한 메서드. RDD파티션 개수나 의존성 정보 등 세부 정보 알고 싶을 때 사용&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize(range(1,100), 10).persist().map(lambda v:(v,1)).coalesce(2)
&amp;gt;&amp;gt;&amp;gt; rdd.toDebugString()
b'(2) CoalescedRDD[65] at coalesce at NativeMethodAccessorImpl.java:0 []\n |  PythonRDD[64] at RDD at PythonRDD.scala:53 []\n |  PythonRDD[63] at RDD at PythonRDD.scala:53 []\n |  ParallelCollectionRDD[62] at parallelize at PythonRDD.scala:195 []'
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;1.12. cache, persist, unpersist&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;rdd액션 연산이 수행될때마다 RDD 생성 히스토리를 이용해 복구하는 단계를 수행하지만 너무나 번거로움&lt;/li&gt;
  &lt;li&gt;따라서 반복적으로 사용되는 RDD인 경우 메모리에 저장해서 사용함&lt;/li&gt;
  &lt;li&gt;cache와 persist는 rdd정보를 메모리 또는 디스크에 저장해서 다음 액션을 수행 시 다시 rdd를 생성하는 단계를 거치지 않음&lt;/li&gt;
  &lt;li&gt;unpersist는 저장된 메모리가 더이상 필요없을 시 취소할 때 사용&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;RDD 데이터 불러오기와 저장하기&lt;/h2&gt;
&lt;p&gt;스파크는 하둡 API기반이라서 다양한 데이터 포맷과 파일을 지원합니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;파일 포맷 : 텍스트파일, JSON, 하둡의 시퀀스파일, csv&lt;/li&gt;
  &lt;li&gt;파일 시스템 : 로컬 파일 시스템, 하둡파일시스템(HDFS), AWS의 S3, 오픈스택의 Swift등
    &lt;ul&gt;
      &lt;li&gt;파일시스템이란 ? 컴퓨터에서 파일이나 자료를 쉽게 발견할 수 있도록 유지 관리하는 방법임. 즉, 저장매체에는 많은 파일이 있으므로, 이러한 파일을 관리하는 방법을 말함. 파일을 빠르게 읽기, 쓰기, 삭제 등 기본적인 기능을 원활히 수행하기 위한 목적임&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;1. 텍스트 파일&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;rdd = sc.textFile(&quot;file:////Users/ralasun/Desktop/ralasun.github.io/_posts/2020-07-11-introRL(1).md&quot;)
&amp;gt;&amp;gt;&amp;gt; rdd.collect()
['---', 'layout : post', 'title: Reinforcement Learning 소개[1]', 'category: Reinforcement Learning', 'tags: cs234 reinforcement-learning david-silver sutton', '---', '', '이번 포스팅은 강화학습이 기존에 알려진 여러 방법론들과의 비교를 통한 강화학습 특성과 구성요소를 다룹니다. ...```
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;“file:///”처럼 ///를 세개 작성해야 함. HDFS와 구별하기 위해서임&lt;/li&gt;
  &lt;li&gt;또한 클러스터내 각 서버에서 동일한 경로를 통해 지정한 파일에 접근이 가능해야 함&lt;/li&gt;
  &lt;li&gt;sc.textFile(path, n)에서, n을 통해 파티션 개수 정할 수 있음&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;#save
rdd.saveAsTextFile(&quot;&amp;lt;path_to_save&amp;gt;/sub1&quot;)

#save(gzip)
rdd.saveAsTextFile(&quot;&amp;lt;path_to_save&amp;gt;/sub1&quot;, codec)
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;위와 같이 rdd를 text파일로도 저장이 가능함. 두번째는 압축을 사용하는 방법임&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;2. 오브젝트 파일&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;오브젝트 직렬화 방법으로 RDD를 저장함. python의 경우, pickle형태로 저장함&lt;/li&gt;
  &lt;li&gt;텍스트파일도 가능함&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize(range(1,1000),3)
&amp;gt;&amp;gt;&amp;gt; rdd.saveAsPickleFile(&quot;/Users/ralasun/Desktop/pythonpickle.pkl&quot;)
&amp;gt;&amp;gt;&amp;gt; rdd2 = sc.pickleFile(&quot;/Users/ralasun/Desktop/pythonpickle.pkl&quot;)       
&amp;gt;&amp;gt;&amp;gt; rdd2.take(2)
[667, 668]
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;3. 시퀀스 파일&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;시퀀스파일이란, 키와 값으로 구성된 데이터를 저장하는 이진 파일 포맷으로, 하둡에서 자주 사용됨&lt;/li&gt;
  &lt;li&gt;오브젝트 파일과의 차이점은 오브젝트 파일은 RDD에 포함된 각 데이터가 serializable 인터페이스를 구현하고 있어야 하는 것처럼 시퀀스 파일로 만들고 싶은 RDD가 하둡의 writable 인터페이스를 구현하고 있어야 함.&lt;/li&gt;
  &lt;li&gt;saveAsNewAPIHadoopFile(path, outputFormatClass, keyClass=None, valueClass=None, keyConverter=None, valueConverter=None, conf=None)
    &lt;ul&gt;
      &lt;li&gt;sequence파일로 저장하기 위해선 outputFormatClass에 문자열의 형태로 하둡내 시퀀스포맷의 풀네임을 작성해야 함. keyclass와 valueclass도 마찬가지임. 이렇게 하는 이유는 하둡의 writable 인터페이스를 구현해야 할 객체가 필요하기 때문임.&lt;/li&gt;
      &lt;li&gt;따라서 내부에서는 keyclass와 valueclass 인자에 전달한 포맷으로 rdd를 변환한 뒤 sequencefile포맷으로 저장하는 작업을 거치는 것임&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt; path = &quot;/Users/ralasun/Desktop/ppkl&quot;
&amp;gt;&amp;gt;&amp;gt; outputFormatClass = &quot;org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat&quot;
&amp;gt;&amp;gt;&amp;gt; inputformatClass = &quot;org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat&quot;
&amp;gt;&amp;gt;&amp;gt; keyClass = &quot;org.apache.hadoop.io.Text&quot;
&amp;gt;&amp;gt;&amp;gt; valueClass = &quot;org.apache.hadoop.io.IntWritable&quot;
&amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize([&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;b&quot;,&quot;c&quot;])
&amp;gt;&amp;gt;&amp;gt; rdd2 = rdd.map(lambda x:(x,1))
&amp;gt;&amp;gt;&amp;gt; rdd2.collect()
[('a', 1), ('b', 1), ('c', 1), ('b', 1), ('c', 1)]
&amp;gt;&amp;gt;&amp;gt; rdd2.saveAsNewAPIHadoopFile(path, outputFormatClass, keyClass, valueClass)
rdd3 = sc.newAPIHadoopFile(path, inputformatClass, keyClass, valueClass)
&amp;gt;&amp;gt;&amp;gt; for k, v in rdd3.collect():
...     print(k,v)
... 
a 1
b 1
b 1
c 1
c 1
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;클러스터 환경에서의 공유 변수&lt;/h2&gt;

&lt;p&gt;클러스터 환경에서 하나의 잡을 수행하기 위해 다수의 서버가 여러 개의 프로세스를 실행합니다. 따라서, 여러 프로세스가 공유할 수 있는 자원을 관리(읽기/쓰기 자원)할 수 있도록 스파크는 지원하는데, 브로드캐스트 변수와 어큐뮬레이터라 합니다.&lt;/p&gt;

&lt;h5&gt;브로드캐스트 변수(Broadcast Variables)&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;스파크 잡이 실행되는 동안 클러스터 내의 모든 서버에서 공유할 수 있는 읽기전용 자원을 설정할 수 있는 변수임&lt;/li&gt;
  &lt;li&gt;예를 들어, 온라인 쇼핑몰에서 사용자 ID와 구매 정보가 담긴 10TB짜리 로그를 분석할 때, 우리가 찾고자 하는 사용자 ID목록이 담긴 세트 컬렉션 타입의 데이터를 공유 변수로 설정해 각 서버에서 로그를 처리하면서 현재 처리하려는 로그가 우리가 찾고자 하는 로그가 맞는지 확인하는 용도로 사용 가능함&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; bu = sc.broadcast([&quot;u1&quot;,&quot;u2&quot;])
#1. sparkcontext의 broadcast인자를 이용해서 broadcast변수 생성

&amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize([&quot;u1&quot;,&quot;u2&quot;,&quot;u3&quot;,&quot;u4&quot;,&quot;u5&quot;,&quot;u6&quot;],3)
&amp;gt;&amp;gt;&amp;gt; result = rdd.filter(lambda v: v in bu.value)
#2. broadcast변수 요소 접근시 value매서드를 이용

&amp;gt;&amp;gt;&amp;gt; result.collect()
['u1', 'u2']
&lt;/code&gt;&lt;/pre&gt;

&lt;h5&gt;어큐뮬레이터(Accumulators)&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;어큐뮬레이터는 쓰기 동작을 위한 것임&lt;/li&gt;
  &lt;li&gt;예를 들어, 온라인 쇼핑몰에서 사용자 접속 로그파일을 각 서버에서 취합해서 분석하는 경우임&lt;/li&gt;
  &lt;li&gt;또한 다수의 서버로 구성된 클러스터 환경에서 오류가 발생 했을 시, 어느 프로세스에서 오류가 난건지 확인이 필요함. 그러기 위해선 에러 정보를 한곳에 모아서 볼 수 있는 방법이 필요함.&lt;/li&gt;
  &lt;li&gt;어큐뮬레이터는 이렇게 클러스터내의 모든 서버가 공유하는 쓰기 공간을 제공해서, 각 서버에서 발생하는 이벤트나 정보를 모아두는 용도로 사용함.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;#accumulator 기본 예제
def accumulate(v, acc):
    if(len(v.split(&quot;:&quot;)) !=2):
        acc.add(1)

if __name__ =='__main__':
    conf = SparkConf()
    conf.set(&quot;spark.driver.host&quot;, &quot;127.0.0.1&quot;)
    sc = SparkContext(master=&quot;local[*]&quot;, appName=&quot;RDDOpSample&quot;, conf=conf)

    acc1 = sc.accumulator(0)
    data = [&quot;U1:Addr1&quot;, &quot;U2:Addr2&quot;, &quot;U3&quot;, &quot;U4:Addr4&quot;, &quot;U5:Addr5&quot;,&quot;U6:Addr6&quot;, &quot;U7&quot;]
    rdd = sc.parallelize(data)
    rdd.foreach(lambda v : accumulate(v, acc1))
    print(acc1.value)
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;파이썬의 경우 어큐뮬레이터의 이름 지정 불가능함&lt;/li&gt;
  &lt;li&gt;기본 제공하는 어큐뮬레이터는 sparkcontext의 accumulator 메서드를 이용하는데, 초깃값으로 정수, 실수, 복소수 타입중 하나여야 함. 따라서, 사용자 정의 데이터 타입에 대한 어큐뮬레이터는 아래와 같이 사용해야 함.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;from pyspark import AccumulatorParam
from record import Record
from builtins import isinstance

class RecordAccumulatorParam(AccumulatorParam):

    def zero(self, initialValue):
        return Record(0)

    def addInPlace(self, v1, v2):
        if(isinstance(v2, Record)):
            return v1+v2
        else:
            return v1.addAmt(v2)

def accumulate(v, acc):
    if(len(v.split(&quot;:&quot;))!=2):
        acc.add(1)

if __name__ =='__main__':
    conf = SparkConf()
    conf.set(&quot;spark.driver.host&quot;, &quot;127.0.0.1&quot;)
    sc = SparkContext(master=&quot;local[*]&quot;, appName=&quot;RDDOpSample&quot;, conf=conf)

    acc = sc.accumulator(Record(0), RecordAccumulatorParam())
    data = [&quot;U1:Addr1&quot;, &quot;U2:Addr2&quot;, &quot;U3&quot;, &quot;U4:Addr4&quot;, &quot;U5:Addr5&quot;,&quot;U6:Addr6&quot;, &quot;U7&quot;]
    rdd = sc.parallelize(data)
    rdd.foreach(lambda v: accumulate(v, acc))
    print(acc.value.amount) #&amp;gt;&amp;gt; 2
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;#AccumulatorParam에 대한 pyspark Document

class pyspark.AccumulatorParam
# Helper object that defines how to accumulate values of a given type.

	addInPlace(value1, value2)
	# Add two values of the accumulator’s data type, returning a new value; for efficiency, can also update value1 in place and return it.

	zero(value)
	# Provide a “zero value” for the type, compatible in dimensions with the provided value (e.g., a zero vector)
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Recordclass타입에 대한 accumulator를 작성한 것임.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;어큐뮬레이터 사용시 주의할 점 두 가지&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;
        &lt;ol&gt;
          &lt;li&gt;어큐뮬레이터를 증가시키는 동작은 클러스터 내 모든 서버에서 가능하나, 어큐뮬레이터 내 데이터를 읽는 동작은 드라이버 프로그램 내에서만 가능
            &lt;ul&gt;
              &lt;li&gt;transformation 또는 action 연산 내부에서는 어큐뮬레이터를 증가시킬 수 있으나, 그 값을 참조해서 사용은 불가능하다는 것을 뜻함.&lt;/li&gt;
              &lt;li&gt;어큐뮬레이터는 액션 메서드 내에서만 수행하는 것이 좋음. 트렌스포메이션은 여러번 수행될 수 있기 때문에 집계가 잘못될 수 있음&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;이상으로 본 포스팅을 마치겠습니다.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>RDD, Resilient Distributed Dataset에 대하여[2] - RDD기본액션, RDD트랜스포메이션</title>
   <link href="http://localhost:4000/spark%20programming/2020/12/07/rdd(2)/"/>
   <updated>2020-12-07T00:00:00+09:00</updated>
   <id>http://localhost:4000/spark%20programming/2020/12/07/rdd(2)</id>
   <content type="html">&lt;p&gt;이번 포스팅은 지난 포스팅 &amp;lt;RDD, Resilient Distributed DataSet에 대하여[1]&amp;gt; 에 이어서 진행하도록 하겠습니다. 교재는 빅데이터 분석을 위한 스파크2 프로그래밍을 참고하였습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1&gt;2. RDD&lt;/h1&gt;

&lt;h2&gt;2.1.1 들어가기에 앞서&lt;/h2&gt;
&lt;p&gt;지난 포스팅 &lt;a href=&quot;https://ralasun.github.io/spark%20programming/2020/11/20/rdd/&quot;&gt;2-1. RDD Resilient Distributed Dataset에 대하여&lt;/a&gt; 에서 다뤘습니다.&lt;/p&gt;

&lt;h2&gt;2.1.2. 스파크컨텍스트 생성&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;스파크 컨텍스트는 스파크 애플리케이션과 클러스터의 연결을 관리하는 객체임&lt;/li&gt;
  &lt;li&gt;따라서, 스파크 애플리케이션을 사용하려면 무조건 스파크 컨텍스트를 생성하고 이용해야 함&lt;/li&gt;
  &lt;li&gt;RDD 생성도 스파크컨텍스트를 이용해 생성 가능함&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Scala&quot;&gt;val conf = new SparkConf().setMaster(“local[*]”).setAppName(“RDDCreateSample”)
val sc = new SparkContext(conf)
&lt;/code&gt;&lt;/pre&gt;
&lt;figcaption align=&quot;center&quot;&gt;[예] scala - sparkcontext 생성&lt;/figcaption&gt;
&lt;p&gt; &lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt; sc = SparkContext(master=“local[*], appName=“RDDCreateTest”, conf=conf)
&lt;/code&gt;&lt;/pre&gt;
&lt;figcaption align=&quot;center&quot;&gt;[예] python - spark context 생성&lt;/figcaption&gt;
&lt;p&gt; &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;스파크 동작에 필요한 여러 설정 정보 지정 가능함&lt;/li&gt;
  &lt;li&gt;SparkConf(), conf=conf 부분에서 config을 통과시켜서 지정 가능함&lt;/li&gt;
  &lt;li&gt;지정해야 하는 정보 중에, master 정보와 appName 정보는 필수 지정 정보임
    &lt;ul&gt;
      &lt;li&gt;master 정보란 ? 스파크가 동작할 클러스터의 마스터 서버를 의미하는 것. 로컬모드에서 local, local[3], local[*]와 같이 사용. [*]는 쓰레드 개수를 의미하며, *는 사용 가능한 모든 쓰레드를 이용하겠다는 이야기임&lt;/li&gt;
      &lt;li&gt;appName은 애플리케이션 이름으로, 구분하기 위한 목적임. 스파크 UI화면에 사용됨&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;2.1.3. RDD생성&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;RDD를 생성하는 방법은 크게 2가지임&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;&lt;li&gt; 드라이버 프로그램의 컬렉션 객체 이용&lt;/li&gt;
&lt;ul&gt;&lt;li&gt;
자바 or 파이썬 ? 리스트 이용, 스칼라 ? 시퀀스타입 이용&lt;/li&gt;
&lt;li&gt;드라이버 프로그램?&lt;br /&gt;
	- 최초로 메인 함수를 실행해 RDD등을 생성하고 각종 연산을 호출하는 프로그램&lt;br /&gt;
	- 드라이버 내의 메인 함수는 스파크 애플리케이션과 스파크 컨텍스트 객체를 생성함&lt;br /&gt;
	- 스파크 컨텍스트를 통해 RDD의 연산 정보를 DAG스케쥴러에 전달하면 스케쥴러는 이 정보를 가지고 실행 계획을 수립한 후 이를 클러스터 매니저에게 전달함&lt;br /&gt;&lt;/li&gt;

&lt;pre lang=&quot;Scala&quot;&gt;&lt;code&gt;
val rdd1 = sc.parallelize(List(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;,&quot;e&quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;figcaption align=&quot;center&quot;&gt;[예] scala - rdd 생성&lt;/figcaption&gt; 

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/MxtJV7I.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;[예] python - 드라이버의 컬렉션 객체를 이용한 RDD 생성&lt;/figcaption&gt;&lt;/p&gt;

&lt;li&gt;
문자열을 포함한 컬렉션 객체 생성 example) python : ['a','b','c','d']&lt;/li&gt;
&lt;li&gt;parallelize() 메서드를 이용해 RDD 생성&lt;br /&gt; - RDD의 파티션 수를 지정하고 싶을 때,  parallelize() 메서드의 두 번째 매개변수로 파티션 개수 지정 가능&lt;/li&gt;

&lt;pre lang=&quot;scala&quot;&gt;&lt;code&gt;
val rdd1 = sc.parallelize(1 to 1000, 10)
&lt;/code&gt;&lt;/pre&gt;&lt;/ul&gt;

&lt;li&gt;외부 데이터를 읽어서 새로운 RDD를 생성&lt;/li&gt; 
&lt;ul&gt;
&lt;li&gt;기본적으로 하둡의 다루는 모든 입출력 유형 가능&lt;/li&gt;
&lt;li&gt;내부적으로 하둡의 입출력을 사용하기 때문임&lt;/li&gt;&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/eByLARc.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;[예] python - 외부데이터를 이용한 RDD 생성&lt;/figcaption&gt;&lt;/p&gt;&lt;/ol&gt;

&lt;h2&gt;2.1.4 RDD 기본 액션&lt;/h2&gt;
&lt;p&gt;기본 액션 연산의 종류에 대해 알아보도록 하겠습니다.&lt;/p&gt;

&lt;h3&gt;1. collect&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;collect은 RDD의 모든 원소를 모아서 배열로 리턴&lt;/li&gt;
  &lt;li&gt;&lt;b&gt;반환 타입이 RDD가 아닌 배열&lt;/b&gt;이므로 액션 연산&lt;/li&gt;
  &lt;li&gt;RDD에 있는 모든 요소들이 서버의 메모리에 수집됨. 즉, 대용량 데이터를 다룰 땐 조심하고, 주로 작은 용량의 데이터 디버깅용으로 사용함&lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/a1uu1V0.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;[예] python - collect 연산&lt;/figcaption&gt;&lt;/p&gt;

&lt;h3&gt;2. count&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;RDD 구성하는 전체 요소 개수 반환&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;2.1.5 RDD 트랜스포메이션&lt;/h2&gt;
&lt;p&gt;기존 RDD를 이용해 새로운 RDD를 생성하는 연산입니다.&lt;/p&gt;

&lt;h3&gt;1. Map 연산&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;RDD에 속하는 &lt;b&gt;모든 요소에 적용&lt;/b&gt;하여 새로운 RDD 생성하는 연산&lt;/li&gt;
  &lt;li&gt;RDD의 몇몇 연산은 특정 데이터 타입에만 적용 가능함&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;1.1. map&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;b&gt;하나의 인자를 받는 함수 자체&lt;/b&gt;가 map의 인자로 들어감&lt;/li&gt;
  &lt;li&gt;이 함수를 이용해 rdd의 모든 요소에 적용한 뒤 새로운 RDD 리턴&lt;/li&gt;
  &lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/1nFR0IH.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;[예] python - map 연산&lt;/figcaption&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;map()에 전달되는 함수의 입력 데이터 타입과 출력 데이터 타입이 일치할 필요 없음. 문자열을 입력받아 정수로 반환하는 함수 사용 가능&lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/lHGsvGD.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;[예] python - map 연산, 입력/출력 일치하지 않는 경우&lt;/figcaption&gt;&lt;/p&gt;

&lt;h4&gt;1.2. flatMap&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;map()과 마찬가지로, 하나의 인자를 받는 함수가 flatMap의 인자로 들어감&lt;/li&gt;
  &lt;li&gt;map()과 차이점은 각 함수의 인자가 반환하는 값의 타입이 다름&lt;/li&gt;
  &lt;li&gt;flatMap()에 사용하는 함수 f는 반환값으로 리스트나 시퀀스 같은 여러 개의 값을 담은 (이터레이션이 가능한) 일종의 컬렉션과 유사한 타입의 값을 반환해야 함
    &lt;ul&gt;
      &lt;li&gt;map[U](f:(T) -&amp;gt; U):RDD[U]&lt;/li&gt;
      &lt;li&gt;flatMap&lt;a href=&quot;f:(T) -&amp;gt; TraversableOnce\[U\]\&quot;&gt;U&lt;/a&gt;:RDD[U])
        &lt;ul&gt;
          &lt;li&gt;TraversableOnce는 이터레이터 타입을 의미&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h5&gt;map()과 flatMap() 차이점 예시&lt;/h5&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/hQ7sfCG.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;[예] python - map 연산 vs. flatMap 연산&lt;/figcaption&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;map연산은 문자열의 배열로 구성된 RDD를 생성함&lt;/li&gt;
  &lt;li&gt;각 요소의 문자열(T)이 단어가 포함된 배열(U)이기 때문임&lt;/li&gt;
  &lt;li&gt;반면, flatMap 연산은 문자열로 구성된 RDD를 생성함&lt;/li&gt;
  &lt;li&gt;TraversableOnce(U)이기 때문에 문자열의 배열 내의 요소가 모두 끄집어져 나오는 작업을 하게 됨&lt;/li&gt;
  &lt;li&gt;flatMap()은 하나의 입력값(“apple, orange”)에 대해 출력 값이 여러개인 경우([“apple”, “orange”]) 유용하게 사용할 수 있음&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;1.3. mapPartitions&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;map과 flatMap은 하나의 인자만을 받는 함수가 인자로 들어가지만, mapPartitions은 여러 인자를 받는 함수가 인자로 들어갈 수 있음 ex) 이터레이터를 인자로 받는 함수&lt;/li&gt;
  &lt;li&gt;mapartitions은 인자로 받은 함수가 파티션 단위로 적용하여 새로운 RDD를 생성함. 반면에, map과 flatMap은 인자로 받은 함수가 요소 한개 단위로 적용됨&lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/bAIoEqG.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;[예] python - mapPartitions&lt;/figcaption&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;sc.parallelize(range(1,11),3)으로 파티션 3개로 나뉨&lt;/li&gt;
  &lt;li&gt;DB 연결!!! 가 세번 출력된 걸 보니 파티션 단위로 처리한 것을 확인할 수 있음&lt;/li&gt;
  &lt;li&gt;increase함수는 각 파티션 내의 요소에 대한 이터레이터를 전달받아 함수 내부에서 파티션의 개별 요소에 대한 작업을 처리하고 그 결과를 다시 이터레이터 타입으로 반환&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;1.4. mapPartitionsWithIndex&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;mapPartions와 동일하고 다른 점은 인자로 전달되는 함수를 호출할 때 파티션에 속한 요소의 정보뿐만 아니라 해당 파티션의 인덱스 정보도 함께 전달해 준다는 것임&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;def IncreaseWithIndex(idx, numbers):
	for i in numbers:
		if(idx == 1):
			yield i+1
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;mapPartitionswithIndex에 인자로 들어갈 함수는 위와 같이 인덱스 정보도 같이 들어감&lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/wO8pOFo.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;[예] python - mapPartitionsWithIndex&lt;/figcaption&gt;&lt;/p&gt;

&lt;h4&gt;1.5. mapValues&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;RDD의 모든 요소들이 키와 값의 쌍을 이루고 있는 경우에만 사용 가능한 메서드이며, 인자로 전달받은 함수를 “값”에 해당하는 요소에만 적용하고 그 결과로 구성된 새로운 RDD를 생성&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;rdd1 = sc.parallelize([&quot;a&quot;,&quot;b&quot;,&quot;c&quot;]).map(lambda v:(v,1)) //(키,값)으로 구성된 rdd생성
rdd2 = rdd1.mapValues(lambda i:i+1)
print(rdd2.collect())
&lt;/code&gt;&lt;/pre&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/aFNZfuf.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;[예] python - mapValues&lt;/figcaption&gt;&lt;/p&gt;

&lt;h4&gt;1.6. flatMapValues&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;MapValues 처럼 키에 해당되는 값에 함수를 적용하나 flatMap() 연산을 적용할 수 있음&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;rdd1 = sc.parallelize([(1, &quot;a,b&quot;),(2, &quot;a,c&quot;),(3, &quot;d,e&quot;)])
rdd2 = rdd1.flatMapValues(lambda v:v.split(','))
rdd2.collect()
&lt;/code&gt;&lt;/pre&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/5J8kSE1.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;[예] python - flatMapValues&lt;/figcaption&gt;&lt;/p&gt;

&lt;h3&gt;2. 그룹화 연산&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;특정 조건에 따라 요소를 그룹화하거나 특정 함수 적용&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;2.1. zip&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;두 개의 서로 다른 RDD를 각 요소의 인덱스에 따라 첫번째 RDD의 ‘인덱스’번째를 키로, 두번째 RDD의 ‘인덱스’번째를 값으로 하는 순서쌍을 생성&lt;/li&gt;
  &lt;li&gt;두 개 RDD는 같은 개수의 파티션과 각 파티션 당 요소개수가 동일해야 함&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;rdd1 = sc.parallelize([&quot;a&quot;,&quot;b&quot;,&quot;c&quot;])
rdd2 = sc.parallelize([1,2,3])
rdd3 = rdd1.zip(rdd2)
rdd3.collect()
&amp;gt;&amp;gt;&amp;gt; [('a', 1), ('b', 2), ('c', 3)]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;rdd1 = sc.parallelize(range(1,10),3)
rdd2 = sc.parallelize(range(11,20),3)
rdd3 = rdd1.zip(rdd2)
rdd3.collect()
&amp;gt;&amp;gt;&amp;gt; [(1, 11), (2, 12), (3, 13), (4, 14), (5, 15), (6, 16), (7, 17), (8, 18), (9, 19)]
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;2.2. zipPartitions&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;zip()과 다르게 파티션의 개수만 동일하면 됨&lt;/li&gt;
  &lt;li&gt;zipPartitions()은 최대 4개 RDD까지 인자로 넣을 수 있음&lt;/li&gt;
  &lt;li&gt;파이썬 사용불가!!&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;2.3. groupBy&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;RDD의 요소를 &lt;b&gt;일정한 기준&lt;/b&gt;에 따라 그룹을 나누고, 각 그룹으로 구성된 새로운 RDD를 생성함&lt;/li&gt;
  &lt;li&gt;각 그룹은 키와 각 키에 속한 요소의 시퀀스(iterator)로 구성됨&lt;/li&gt;
  &lt;li&gt;인자로 전달하는 함수가 각 그룹의 키를 결정하는 역할을 담당함&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd1 = sc.parallelize(range(1,11))
&amp;gt;&amp;gt;&amp;gt; rdd2 = rdd1.groupBy(lambda v: &quot;even&quot; if v%2==0 else &quot;odd&quot;) 
/// groupBy에 인자로 전달된 함수에 의해 키(even/odd) 결정

print(rdd2.collect())
&amp;gt;&amp;gt;&amp;gt; [('even', &amp;lt;pyspark.resultiterable.ResultIterable object at 0x7fbc3f090e80&amp;gt;), ('odd', &amp;lt;pyspark.resultiterable.ResultIterable object at 0x7fbc3f090470&amp;gt;)]
/// 각 키에 해당하는 값은 iterator임을 확인할 수 있음

&amp;gt;&amp;gt;&amp;gt; for x in rdd2.collect():
...     print(x[0], list(x[1]))
... 
even [2, 4, 6, 8, 10]
odd [1, 3, 5, 7, 9]
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;2.3. groupByKey&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;이미 키와 값의 쌍으로 구성된 RDD에만 적용 가능함&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd1 = sc.parallelize([&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;b&quot;,&quot;c&quot;]).map(lambda v:(v,1))
&amp;gt;&amp;gt;&amp;gt; rdd1.collect()
[('a', 1), ('b', 1), ('c', 1), ('b', 1), ('c', 1)]
/// (키,값) 쌍으로 구성된 RDD 생성

&amp;gt;&amp;gt;&amp;gt; rdd2 = rdd1.groupByKey()
&amp;gt;&amp;gt;&amp;gt; rdd2.collect()
[('b', &amp;lt;pyspark.resultiterable.ResultIterable object at 0x7fbc3f0ab6a0&amp;gt;), ('c', &amp;lt;pyspark.resultiterable.ResultIterable object at 0x7fbc3f0ab6d8&amp;gt;), ('a', &amp;lt;pyspark.resultiterable.ResultIterable object at 0x7fbc3f0ab0b8&amp;gt;)]
/// 키에 따라 그룹화함. 그 결과 키에 해당하는 시퀀스 생성

&amp;gt;&amp;gt;&amp;gt; for x in rdd2.collect():
...     print(x[0], list(x[1]))
... 
b [1, 1]
c [1, 1]
a [1]
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;2.4. cogroup&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;이미 키와 값의 쌍으로 구성된 RDD에만 적용 가능함&lt;/li&gt;
  &lt;li&gt;여러 개의 RDD를 인자로 받음(최대 3개)&lt;/li&gt;
  &lt;li&gt;여러 RDD에서 동일한 키에 해당하는 요소들로 구성된 시퀀스를 만든 후, (키, 시퀀스)의 튜플을 구성. 그 튜플들로 구성된 새로운 RDD를 생성함&lt;/li&gt;
  &lt;li&gt;Tuple(키, Tuple(rdd1요소들의 집합, rdd2요소들의 집합, …))&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd1 = sc.parallelize([(&quot;k1&quot;,&quot;v1&quot;),(&quot;k2&quot;,&quot;v2&quot;),(&quot;k1&quot;,&quot;v3&quot;)])
&amp;gt;&amp;gt;&amp;gt; rdd2 = sc.parallelize([(&quot;k1&quot;,&quot;v4&quot;)])
&amp;gt;&amp;gt;&amp;gt; rdd1.collect()
[('k1', 'v1'), ('k2', 'v2'), ('k1', 'v3')]
&amp;gt;&amp;gt;&amp;gt; rdd2.collect()
[('k1', 'v4')]
/// (키, 값)쌍으로 구성된 RDD 2개 생성

&amp;gt;&amp;gt;&amp;gt; rdd3 = rdd1.cogroup(rdd2)
&amp;gt;&amp;gt;&amp;gt; rdd3.collect()
[('k1', (&amp;lt;pyspark.resultiterable.ResultIterable object at 0x7fbc3f0b2fd0&amp;gt;, &amp;lt;pyspark.resultiterable.ResultIterable object at 0x7fbc3f00a828&amp;gt;)), ('k2', (&amp;lt;pyspark.resultiterable.ResultIterable object at 0x7fbc3f00ac18&amp;gt;, &amp;lt;pyspark.resultiterable.ResultIterable object at 0x7fbc3f00a5f8&amp;gt;))]
///k1 키에 대해 rdd1의 v1과 v3요소를 묶고, rdd2의 v4요소를 묶어서 튜플로 구성

&amp;gt;&amp;gt;&amp;gt; for x in rdd3.collect():
...     print(x[0], list(x[1][0]), list(x[1][1]))
... 
k1 ['v1', 'v3'] ['v4']
k2 ['v2'] []
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;3. 집합 연산&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;RDD에 포함된 요소를 하나의 집합으로 간주하여 집합 연산을 수행(합/교집합)&lt;/li&gt;
  &lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;3.1. distinct&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;RDD의 원소에서 중복을 제외한 요소로만 새로운 RDD 구성&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize([1,2,3,1,2,3,1,2,3])
&amp;gt;&amp;gt;&amp;gt; rdd2 = rdd.distinct()
&amp;gt;&amp;gt;&amp;gt; rdd2.collect()
[1, 2, 3]
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;3.1. cartesian&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;두 RDD요소의 카테시안곱을 구하고 그 결과를 요소로 하는 새로운 RDD구성&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd1 = sc.parallelize([1,2,3])
&amp;gt;&amp;gt;&amp;gt; rdd2 = sc.parallelize(['a','b','c'])
&amp;gt;&amp;gt;&amp;gt; rdd3 = rdd1.cartesian(rdd2)
&amp;gt;&amp;gt;&amp;gt; rdd3.collect()
[(1, 'a'), (1, 'b'), (1, 'c'), (2, 'a'), (2, 'b'), (2, 'c'), (3, 'a'), (3, 'b'), (3, 'c')]
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;3.2. subtract&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;rdd1.subtract(rdd2) : (rdd1의 요소집합 - rdd2의 요소집합)의 차집합&lt;/li&gt;
  &lt;li&gt;rdd2.subtract(rdd1) : (rdd2의 요소집합 - rdd1의 요소집합)의 차집합&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd1 = sc.parallelize([&quot;a&quot;, &quot;b&quot;,&quot;c&quot;,&quot;d&quot;,&quot;e&quot;])
&amp;gt;&amp;gt;&amp;gt; rdd2 = sc.parallelize([&quot;d&quot;,&quot;e&quot;])
&amp;gt;&amp;gt;&amp;gt; rdd3 = rdd1.subtract(rdd2)
&amp;gt;&amp;gt;&amp;gt; rdd3.collect()
['a', 'b', 'c']  
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;3.2. union&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;두 RDD요소의 합집합&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd1 = sc.parallelize([&quot;a&quot;, &quot;b&quot;,&quot;c&quot;,&quot;d&quot;,&quot;e&quot;])
&amp;gt;&amp;gt;&amp;gt; rdd2 = sc.parallelize([&quot;d&quot;,&quot;e&quot;])
&amp;gt;&amp;gt;&amp;gt; rdd3 = rdd1.union(rdd2)
&amp;gt;&amp;gt;&amp;gt; rdd3.collect()
['a', 'b', 'c', 'd', 'e', 'd', 'e']
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;3.3. intersection&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;두 RDD요소의 교집합으로 중복되지 않은 요소로 구성&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd1 = sc.parallelize([&quot;a&quot;,&quot;a&quot;,&quot;b&quot;,&quot;c&quot;])
&amp;gt;&amp;gt;&amp;gt; rdd2 = sc.parallelize([&quot;a&quot;,&quot;a&quot;,&quot;c&quot;,&quot;c&quot;])
&amp;gt;&amp;gt;&amp;gt; rdd3 = rdd1.intersection(rdd2)
&amp;gt;&amp;gt;&amp;gt; rdd3.collect()
['a', 'c']
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;3.4. join&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;RDD의 구성요소가 키와 값의 쌍으로 구성된 경우에 사용할 수 있는 메서드&lt;/li&gt;
  &lt;li&gt;공통된 키에 대해서만 join수행&lt;/li&gt;
  &lt;li&gt;join 수행 결과 Tuple(키, Tuple(첫번째 RDD요소, 두번쨰 RDD요소))&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd1 = sc.parallelize([&quot;a&quot;, &quot;b&quot;,&quot;c&quot;,&quot;d&quot;,&quot;e&quot;]).map(lambda v : (v,1))
&amp;gt;&amp;gt;&amp;gt; rdd1.collect()
[('a', 1), ('b', 1), ('c', 1), ('d', 1), ('e', 1)]
&amp;gt;&amp;gt;&amp;gt; rdd2 = sc.parallelize([&quot;b&quot;,&quot;c&quot;]).map(lambda v:(v,2))
&amp;gt;&amp;gt;&amp;gt; rdd2.collect()
[('b', 2), ('c', 2)]
&amp;gt;&amp;gt;&amp;gt; rdd3 = rdd1.join(rdd2)
&amp;gt;&amp;gt;&amp;gt; rdd3.collect()
[('b', (1, 2)), ('c', (1, 2))]
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;3.5. leftOuterJoin, rightOuterJoin&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;키와 값의 쌍으로 구성된 RDD에 사용가능&lt;/li&gt;
  &lt;li&gt;leftjoin, rightjoin을 수행&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd1 = sc.parallelize([&quot;a&quot;, &quot;b&quot;,&quot;c&quot;,&quot;d&quot;,&quot;e&quot;]).map(lambda v : (v,1))
&amp;gt;&amp;gt;&amp;gt; rdd2 = sc.parallelize([&quot;b&quot;,&quot;c&quot;]).map(lambda v:(v,2))
&amp;gt;&amp;gt;&amp;gt; rdd3 = rdd1.leftOuterJoin(rdd2)
&amp;gt;&amp;gt;&amp;gt; rdd3.collect()
[('a', (1, None)), ('e', (1, None)), ('b', (1, 2)), ('c', (1, 2)), ('d', (1, None))]
///rdd2에는 a,d,e 키가 없기 때문에 해당 키에 대한 튜플 요소는 (rdd1의 요소, None)으로 구성됨

&amp;gt;&amp;gt;&amp;gt; rdd4 = rdd1.rightOuterJoin(rdd2)
&amp;gt;&amp;gt;&amp;gt; rdd4.collect()
[('b', (1, 2)), ('c', (1, 2))]
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;3.6. subtractByKey&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;키와 값의 쌍으로 구성된 RDD에 사용가능&lt;/li&gt;
  &lt;li&gt;rdd1의 요소 중에서 rdd2와 겹치지 않는 키로 구성된 새로운 RDD 생성&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd1 = sc.parallelize([&quot;a&quot;,&quot;b&quot;]).map(lambda v:(v,1))
&amp;gt;&amp;gt;&amp;gt; rdd2 = sc.parallelize([&quot;b&quot;]).map(lambda v:(v,1))
&amp;gt;&amp;gt;&amp;gt; rdd3 = rdd1.subtractByKey(rdd2)
&amp;gt;&amp;gt;&amp;gt; rdd3.collect()
[('a', 1)]
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;4. 집계와 관련된 연산들&lt;/h3&gt;

&lt;h4&gt;4.1 reduceByKey&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;키와 값의 쌍으로 구성된 RDD에서 사용 가능&lt;/li&gt;
  &lt;li&gt;RDD 내의 동일한 키를 하나로 병합해 (키,값) 쌍으로 구성된 새로운 RDD 생성&lt;/li&gt;
  &lt;li&gt;함수를 인자로 받음.&lt;/li&gt;
  &lt;li&gt;왜냐하면, 파티션 별로 연산을 수행했을 때, 항상 같은 순서로 연산이 수행되는 것을 보장 못하므로, 함수가 수행하는 연산은 교환법칙과 결합법칙이 성립해야 함&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize(['a','b','b']).map(lambda v:(v,1))
&amp;gt;&amp;gt;&amp;gt; rdd.collect()
[('a', 1), ('b', 1), ('b', 1)]
&amp;gt;&amp;gt;&amp;gt; rdd2 = rdd.reduceByKey(lambda v1, v2:(v1+v2))
&amp;gt;&amp;gt;&amp;gt; rdd2.collect()
[('b', 2), ('a', 1)]
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;&lt;b&gt;(키,값)쌍으로 하는 RDD를 인자로 받는 트랜스포메이션 메서드&lt;/b&gt;&lt;br /&gt;
- 데이터 처리 과정에서 사용할 파티셔너와 파티션 개수를 지정할 수 있는 옵션이 있음
- 자체적으로 작성한 파티셔너나 파티션 개수를 통해 병렬 처리 수준 변경 가능
&lt;/blockquote&gt;

&lt;h4&gt;4.2 foldByKey&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;키와 값으로 구성된 RDD에 사용 가능&lt;/li&gt;
  &lt;li&gt;reduceByKey()와 유사하지만, 병합 연산의 초기값을 인자로 전달할 수 있음&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;rdd = sc.parallelize([&quot;a&quot;,&quot;b&quot;,&quot;b&quot;]).map(lambda v:(v,1))
&amp;gt;&amp;gt;&amp;gt; rdd2 = rdd.foldByKey(0, lambda v1,v2:v1+v2)
&amp;gt;&amp;gt;&amp;gt; rdd2.collect()
[('b', 2), ('a', 1)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;My view) 개인적으로, foldByKey와 reduceByKey의 차이가 잘 이해가 되지 않아, 초기값과 문자열 병합으로 pyspark를 실행해 보았습니다.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize([&quot;a&quot;,&quot;b&quot;,&quot;b&quot;]).map(lambda v:(v,1))

///초기값을 1로 준 경우
&amp;gt;&amp;gt;&amp;gt; rdd2 = rdd.foldByKey(1, lambda v1,v2:v1+v2)

///초기값을 0으로 준 경우와 다른 결과임
&amp;gt;&amp;gt;&amp;gt; rdd2.collect()
[('b', 4), ('a', 2)]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize([&quot;a&quot;,&quot;b&quot;,&quot;b&quot;]).map(lambda v:(v,'c'))

///초기값을 t로 준 경우
&amp;gt;&amp;gt;&amp;gt; rdd2 = rdd.foldByKey('t', lambda v1,v2:v1+v2)
&amp;gt;&amp;gt;&amp;gt; rdd2.collect()
[('b', 'tctc'), ('a', 'tc')]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;위의 두 연산를 보니, foldByKey는 초기값 처리를 아래와 같이 진행하는 것 같습니다. 예를 들어, 초기 rdd가 [(‘a’, 1), (‘b’, 1), (‘b’, 1)] 라 한다면, foldByKey는 키 ‘a’와 ‘b’에 대해 각각 초기값을 가지고 병합연산을 수행합니다. 이 때 먼저, 초기값을 가지고 병합 연산을 수행합니다. 키 ‘a’인 경우, 초기값 1라면, v1=1, v2=1(키 ‘a’에 대응되는 값)이 병합연산을 수행해 (‘a’,2)가 됩니다. 그 다음 reducebykey와 같은 연산을 수행하나, 키 ‘a’는 초기 rdd에 하나밖에 없기 때문에, v1=2, v2=None이 되어 최종 foldByKey연산 결과는 키 ‘a’에 대해서 2값을 가지게 됩니다.&lt;/p&gt;

&lt;p&gt;‘b’키 같은 경우도 마찬가지입니다. 먼저 초기값을 가지고 연산을 수행합니다. v1=1(초기값), v2=1(b에 대응되는 값)가 병합연산을 수행해 v1=2가 됩니다. ‘b’키는 두개가 있으므로, 나머지 ‘b’키에 대해 v1=1, v2=1가 병합연산을 거쳐 v2=2가 됩니다. 그다음 두개의 ‘b’키에 대해 다시 병합연산이 수행되어 v1=2, v2=2가 되어 최종적으로 ‘b’키에 대해 4의 값이 생성됩니다.&lt;/p&gt;

&lt;h4&gt;4.3 combineByKey&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;키와 값인 RDD에 사용 가능&lt;/li&gt;
  &lt;li&gt;foldByKey와 reduceByKey와 유사함. 차이점은 병합연산 수행 결과 값의 타입이 바뀔 수 있음&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Scala&quot;&gt;def reduceByKey(func:(V,V)=&amp;gt;V):RDD[K,V]
def foldByKey(zeroValue: V)(func:(V,V)=&amp;gt;V):RDD[K,V]
def combineByKey[C](createCombiner:(V)=&amp;gt;C, mergeValue:(C,V)=&amp;gt;V, mergeCombiners:(C,C)=&amp;gt;C):RDD[K,C]
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;combineByKey는 reduceByKey와 foldByKey와 다르게 타입이 C로 바뀌어 있음&lt;/li&gt;
  &lt;li&gt;위의 combineByKey를 보면 메서드가 총 createCombiner, mergeValue, mergeCombiners 세 개임을 알 수 있음&lt;/li&gt;
  &lt;li&gt;아래는 combineByKey에서 자주 등장하는 평균구하기 예시임&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;#record.py 에 따로 저장해야 함. 
#아니면, _pickle.PicklingError: Can't pickle &amp;lt;class '__main__.Record'&amp;gt;: attribute lookup Record on __main__ failed 에러 발생.

class Record:

    def __init__(self, amount, number=1):
        self.amount = amount
        self.number = number
        
    def addAmt(self, amount):
        return Record(self.amount + amount, self.number + 1)
    
    def __add__(self, other):
        amount = self.amount + other.amount
        number = self.number + other.number 
        return Record(amount, number)
        
    def __str__(self):
        return &quot;avg:&quot; + str(self.amount / self.number)

    def __repr__(self):
        return 'Record(%r, %r)' % (self.amount, self.number)
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#createCombiner, mergeValue, mergeCombiners 정의
def createCombiner(v):
    return Record(v)

def mergeValue(c, v):
    return c.addAmt(v)

def mergeCombiners(c1, c2):
    return c1 + c2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;#combineByKey 실행
rdd = sc.parallelize([(&quot;Math&quot;, 100), (&quot;Eng&quot;, 80), (&quot;Math&quot;, 50), (&quot;Eng&quot;, 70), (&quot;Eng&quot;, 90)])
rdd2 = rdd.combineByKey(lambda v:createCombiner(v), lambda c,v:mergeValue(c,v), lambda c1,c2:mergeCombiners(c1,c2))
print('Math', rdd2.collectAsMap()['Math'], 'Eng', rdd2.collectAsMap()['Eng'])
&amp;gt;&amp;gt;&amp;gt; Math avg:75.0 Eng avg:80.0
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;createCombiner()
    &lt;ul&gt;
      &lt;li&gt;값을 병합하기 위한 컴바이너. 컴바이너는 병합을 수행하고 컴바이너 타입으로 내부에 저장함. 위의 예로는 createCombiner()는 Record클래스 타입으로 저장됨. 즉, rdd의 각 키에 해당되는 값이 record클래스 타입으로 변환되어 저장됨.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;mergeValue()
    &lt;ul&gt;
      &lt;li&gt;키에 대한 컴바이너가 존재한다면, 컴바이너를 이용해 값을 병합함.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;My View) 위의 예 같은 경우, rdd요소 순서대로 combineByKey가 작동한다면 먼저 (“Math”, 100)에 대해 Combiner가 생성되어, math 키에 대한 Record(100)이 생성됩니다.&lt;/p&gt;

&lt;p&gt;그 다음 (“Eng”, 70)에 대해 작동합니다. “Eng”키는 기존에 없던 키이기 때문에 새로 Combiner인 Record(70)이 생성됩니다.&lt;/p&gt;

&lt;p&gt;그 다음 (“Math”, 50)인데, 기존 Math키에 대한 컴바이너가 존재하기 때문에, 기존 Math 키 컴바이너를 이용하여 병합합니다. 즉, Record(100).addAmt(50)이 발생합니다. 이렇게 Math키와 Eng키에 대해 모든 요소에 대해 병합을 실시하게 됩니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;mergeCombiners()
    &lt;ul&gt;
      &lt;li&gt;createCombiner()와 mergeValue()는 파티션별로 수행됨. 그다음 모든 파티션에 생성된 combiner를 병합하는 과정을 mergeCombiners()를 통해 수행하는 것임. 이를 통해 최종결과가 발생함&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;4.4 aggregateByKey&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;키와 값의 RDD에서 사용 가능&lt;/li&gt;
  &lt;li&gt;초깃값을 설정할 수 있는 점을 제외하면 comebineByKey와 동일&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Scala&quot;&gt;def combineByKey[C](createCombiner:(V)=&amp;gt;C, mergeValue:(C,V)=&amp;gt;V, mergeCombiners:(C,C)=&amp;gt;C):RDD[(K,C)]
def aggregateByKey[U](zeroValue: U)(seqOp:(U,V)=&amp;gt;U, combOp:(U,U)=&amp;gt;U):RDD[(K,U)]
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;aggregateByKey에서 seqOp는 mergeValue역할을, comOp는 mergeCombiner역할을 함.&lt;/li&gt;
  &lt;li&gt;combineByKey에서 createCombiner로 병합을 위한 초깃값을 구하지만 aggregateByKey는 함수를 이용해 초깃값을 설정하는 대신 바로 ‘값’으로 초기값을 설정함&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;rdd = sc.parallelize([(&quot;Math&quot;, 100), (&quot;Eng&quot;, 80), (&quot;Math&quot;, 50), (&quot;Eng&quot;, 70), (&quot;Eng&quot;, 90)])
rdd2 = rdd.aggregateByKey(Record(0,0), lambda c,v:mergeValue(c,v), lambda c1,c2:mergeCombiners(c1,c2))
print('Math :', rdd2.collectAsMap()['Math'], 'Eng :', rdd2.collectAsMap()['Eng'])
&amp;gt;&amp;gt;&amp;gt;Math : avg:75.0 Eng : avg:80.0
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;5. pipe 및 파티션과 관련된 연산&lt;/h3&gt;

&lt;h4&gt;5.1 pipe&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;pipe를 이용하면 데이터를 처리하는 과정에서 외부 프로세스를 활용할 수 있음&lt;/li&gt;
  &lt;li&gt;세 개의 숫자로 구성된 문자열을 리눅스의 cut 유틸리티를 이용해 분리한 뒤첫번째와 세번재 숫자를 뽑아내는 예제&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize([&quot;1,2,3&quot;, &quot;4,5,6&quot;, &quot;7,8,9&quot;])
&amp;gt;&amp;gt;&amp;gt; rdd2 = rdd.pipe(&quot;cut -f 1,3 -d ,&quot;)
&amp;gt;&amp;gt;&amp;gt; rdd2.collect()
['1,3', '4,6', '7,9']
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;5.2. coalesce와 repartition&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;현재 RDD에 사용된 파티션 개수를 조정함&lt;/li&gt;
  &lt;li&gt;coalesce는 파티션 개수를 줄이기만 되고, repartition은 늘리는 것과 줄이는 것 둘 다 가능&lt;/li&gt;
  &lt;li&gt;coalesce가 따로 있는 이유는 처리 방식에 따른 성능 차이 때문임
    &lt;ul&gt;
      &lt;li&gt;repartition은 셔플을 기반으로 동작을 수행하는 데 반해, coalesce는 강제로 셔플을 수행하라는 옵션을 지정하지 않는 한 셔플을 사용하지 않음.&lt;/li&gt;
      &lt;li&gt;따라서 필터링 등으로 인해 데이터 개수가 줄어든 경우 coalesce을 사용하는 것이 좋음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize(list(range(1,11)),10)
&amp;gt;&amp;gt;&amp;gt; rdd2 = rdd.coalesce(5)
&amp;gt;&amp;gt;&amp;gt; rdd3 = rdd2.repartition(10)
&amp;gt;&amp;gt;&amp;gt; print(&quot;partition size : %d&quot; 
&amp;gt;&amp;gt;&amp;gt; print(&quot;partition size : %d&quot; %rdd2.getNumPartitions())
partition size : 5
&amp;gt;&amp;gt;&amp;gt; print(&quot;partition size : %d&quot; %rdd3.getNumPartitions())
partition size : 10
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;5.3. repartitionAndSortWithinPartitions&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;키와 값으로 구성된 RDD에서 사용 가능&lt;/li&gt;
  &lt;li&gt;RDD를 구성하는 모든 데이터를 특정 기준에 따라 여러 개의 파티션으로 분리하고 각 파티션 단위로 정렬을 수행한 뒤 새로운 RDD를 생성해 주는 메서드임&lt;/li&gt;
  &lt;li&gt;각 데이터가 어떤 파티션에 속할지 결정하기 위한 파티셔너(org.apache.spark.Partitioner)설정
    &lt;ul&gt;
      &lt;li&gt;키 값을 이용하여 어떤 파티션에 속할지 결정할 뿐만 아니라 키 값을 이용한 정렬도 수행함&lt;/li&gt;
      &lt;li&gt;파티션 재할당을 위해 셔플을 수행하는 단계에서 정렬도 함께 다루게 되어 파티션과 정렬을 각각 따로하는 것에 비해 더 높은 성능을 발휘할 수 있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;10개의 무작위 숫자를 위 메서드를 이용해 3개의 파티션으로 분리해 보는 예제&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; data = [random.randrange(1,100) for i in range(0,10)]
&amp;gt;&amp;gt;&amp;gt; rdd1 = sc.parallelize(data).map(lambda v:(v,&quot;-&quot;))
&amp;gt;&amp;gt;&amp;gt; rdd2 = rdd1.repartitionAndSortWithinPartitions(3, lambda x:x)
&amp;gt;&amp;gt;&amp;gt; rdd2.foreachPartition(lambda values:print(list(values)))
[(50, '-')]
[(16, '-'), (52, '-'), (61, '-'), (67, '-')]
[(6, '-'), (12, '-'), (48, '-'), (51, '-'), (87, '-')]
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;pyspark에서 repartitionAndSortWithinPartitions에서 default 파티셔너는 hash 파티셔너로 되어있음&lt;/li&gt;
  &lt;li&gt;foreachPartition은 partition단위로 특정함수를 실행해주는 메서드임. 위의 예제에서는 파티션단위로 파티션에 속해있는 값을 프린트해주는 함수를 실행했음&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;5.4. partitionBy&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;키와 값으로 구성된 RDD에서 사용가능&lt;/li&gt;
  &lt;li&gt;파티션을 변경하고 싶을 때 사용가능&lt;/li&gt;
  &lt;li&gt;기본적으로, hashpartitioner와 rangepartitioner가 있음&lt;/li&gt;
  &lt;li&gt;org.apache.spark.partitioner 클래스를 상속해서 파티셔너를 커스터마이징도 가능&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd1 = sc.parallelize([(&quot;apple&quot;,1),(&quot;mouse&quot;,1),(&quot;monitor&quot;,1)],5)
&amp;gt;&amp;gt;&amp;gt; rdd1.collect()
[('apple', 1), ('mouse', 1), ('monitor', 1)]
&amp;gt;&amp;gt;&amp;gt; rdd2 = rdd1.partitionBy(3)
&amp;gt;&amp;gt;&amp;gt; print(&quot;rdd1: %d, rdd2: %d&quot; %(rdd1.getNumPartitions(), rdd2.getNumPartitions()))
rdd1: 5, rdd2: 3
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;위의 예제에서 partitionby에 의해 파티션 갯수가 변경된 것을 확인할 수 있음&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;6. 필터와 정렬 연산&lt;/h3&gt;
&lt;p&gt;특정 조건을 만족하는 요소만 선택하거나, 각 요소를 정해진 기준에 따라 정렬함&lt;/p&gt;

&lt;h4&gt;6.1. filter&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;RDD의 각각 요소에 조건에 따라 True/False로 가려내는 함수를 적용하여 True에 해당하는 요소만 걸러냄&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd1 = sc.parallelize(range(1,6))
&amp;gt;&amp;gt;&amp;gt; rdd2 = rdd1.filter(lambda i:i&amp;gt;2)
&amp;gt;&amp;gt;&amp;gt; print(rdd2.collect())
[3, 4, 5]
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;6.2. sortByKey&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;키 값을 기준으로 요소를 정렬하는 연산임&lt;/li&gt;
  &lt;li&gt;따라서, 키와 값으로 구성된 RDD에 적용 가능함&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd1 = sc.parallelize([(&quot;q&quot;,1),(&quot;z&quot;,1),(&quot;a&quot;,1)])
&amp;gt;&amp;gt;&amp;gt; result = rdd1.sortByKey()
&amp;gt;&amp;gt;&amp;gt; print(result.collect())
[('a', 1), ('q', 1), ('z', 1)]
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;6.3. keys, values&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;키와 값으로 구성된 RDD에 적용 가능함&lt;/li&gt;
  &lt;li&gt;keys는 RDD의 키 요소로 구성된 RDD를 생성하고, values는 RDD의 value요소로 구성된 RDD를 생성함&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd1 = sc.parallelize([(&quot;q&quot;,1),(&quot;z&quot;,1),(&quot;a&quot;,1)])
&amp;gt;&amp;gt;&amp;gt; rdd2 = rdd1.keys()
&amp;gt;&amp;gt;&amp;gt; rdd3 = rdd1.values()
&amp;gt;&amp;gt;&amp;gt; print(rdd2.collect(), rdd3.collect())
['q', 'z', 'a'] [1, 1, 1]
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;6.4. sample&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;샘플을 추출하는 RDD메서드&lt;/li&gt;
  &lt;li&gt;sample(withReplacement, fraction, seed=None)
    &lt;ul&gt;
      &lt;li&gt;withReplacement : True/False복원추출 결정&lt;/li&gt;
      &lt;li&gt;fraction
        &lt;ul&gt;
          &lt;li&gt;복원추출인 경우, RDD각 요소당 평균 추출횟수를 의미함&lt;/li&gt;
          &lt;li&gt;비복원추출인 경우, RDD각 요소당 샘플될 확률을 의미함&lt;/li&gt;
          &lt;li&gt;fraction이 sample사이즈를 결정하는 것은 아님. 아래 예제를 보면, sample사이즈는 random한 것을 알 수 있음.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd1 = sc.parallelize(range(100))
&amp;gt;&amp;gt;&amp;gt; rdd2 = rdd1.sample(True, 1.5, seed=1234)
&amp;gt;&amp;gt;&amp;gt; rdd3 = rdd1.sample(False, 0.2, seed=1234)
&amp;gt;&amp;gt;&amp;gt; 
&amp;gt;&amp;gt;&amp;gt; rdd2.collect()
[1, 1, 2, 6, 6, 7, 7, 9, 10, 10, 11, 12, 12, 12, 13, 15, 17, 18, 19, 19, 19, 20, 21, 21, 23, 24, 25, 25, 26, 26, 26, 26, 26, 27, 28, 28, 28, 29, 30, 31, 32, 33, 33, 34, 35, 36, 36, 36, 37, 37, 38, 39, 39, 42, 42, 44, 44, 45, 45, 46, 48, 48, 48, 49, 49, 49, 50, 50, 50, 51, 53, 54, 57, 58, 59, 60, 63, 64, 65, 65, 66, 69, 71, 71, 71, 72, 72, 72, 73, 73, 75, 76, 77, 79, 80, 80, 81, 84, 84, 85, 85, 85, 86, 88, 88, 88, 89, 89, 89, 90, 90, 91, 91, 92, 92, 92, 94, 94, 95, 95, 95, 95, 96, 97, 97, 99]
&amp;gt;&amp;gt;&amp;gt; rdd3.collect()
[0, 5, 6, 7, 8, 11, 15, 35, 39, 41, 55, 56, 58, 61, 62, 71, 72, 78, 81, 89, 90, 93, 97, 99]
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;이상으로 본 포스팅을 마치겠습니다. 다음 포스팅은 &amp;lt;RDD, Resilient Distributed Dataset에 대하여[3] - RDD액션, RDD데이터 불러오기와 저장하기&amp;gt; 에 대해 진행하도록 하겠습니다.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>dataframe, numpy 등 array에서 double-colon(::) slicing</title>
   <link href="http://localhost:4000/code%20snippet/2020/12/03/double-colon-slicing/"/>
   <updated>2020-12-03T00:00:00+09:00</updated>
   <id>http://localhost:4000/code%20snippet/2020/12/03/double-colon-slicing</id>
   <content type="html">&lt;hr /&gt;

&lt;p&gt;pandas, numpy 등 자주 헷갈리는 코드 사용을 모아두었습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;dfc&quot;&gt;df[::c]&lt;/h1&gt;

&lt;p&gt;시작부터 c 간격마다 있는 row를 슬라이싱해줍니다. 자세히 설명하면, 1번째, (1+c)번째, (1+2c)번째, …, (1+nc)번째 row가 선택됩니다. 아래는 예제입니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sampledf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'A'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'B'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;sampledf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[::&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div&gt;
&lt;style scoped=&quot;&quot;&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;A&lt;/th&gt;
      &lt;th&gt;B&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0.312234&lt;/td&gt;
      &lt;td&gt;0.788584&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;-0.123720&lt;/td&gt;
      &lt;td&gt;0.445176&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0.411344&lt;/td&gt;
      &lt;td&gt;0.617469&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;-0.434367&lt;/td&gt;
      &lt;td&gt;0.674210&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;-0.563221&lt;/td&gt;
      &lt;td&gt;0.009331&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;190&lt;/th&gt;
      &lt;td&gt;1.797756&lt;/td&gt;
      &lt;td&gt;0.963394&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;192&lt;/th&gt;
      &lt;td&gt;-0.679177&lt;/td&gt;
      &lt;td&gt;0.033222&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;194&lt;/th&gt;
      &lt;td&gt;0.975527&lt;/td&gt;
      &lt;td&gt;0.041236&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;196&lt;/th&gt;
      &lt;td&gt;-1.354463&lt;/td&gt;
      &lt;td&gt;0.450887&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;198&lt;/th&gt;
      &lt;td&gt;-2.341788&lt;/td&gt;
      &lt;td&gt;0.009804&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;100 rows × 2 columns&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;위에 sampledf[::2]를 보시면 첫번째(index=0), 세번째(index=2), …., 199번째(index=198)이 선택되는 것을 확인하실 수 있습니다. 2의 간격 크기로 행이 선택되는 것입니다.&lt;/p&gt;

&lt;h1&gt;df[::-1]&lt;/h1&gt;

&lt;p&gt;df[::-1] 인 경우는 열의 배치를 뒤집어줍니다. 아래는 예시입니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;sampledf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[::&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div&gt;
&lt;style scoped=&quot;&quot;&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;A&lt;/th&gt;
      &lt;th&gt;B&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;199&lt;/th&gt;
      &lt;td&gt;2.600890&lt;/td&gt;
      &lt;td&gt;0.775489&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;198&lt;/th&gt;
      &lt;td&gt;-2.341788&lt;/td&gt;
      &lt;td&gt;0.009804&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;197&lt;/th&gt;
      &lt;td&gt;-0.365103&lt;/td&gt;
      &lt;td&gt;0.413758&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;196&lt;/th&gt;
      &lt;td&gt;-1.354463&lt;/td&gt;
      &lt;td&gt;0.450887&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;195&lt;/th&gt;
      &lt;td&gt;0.685687&lt;/td&gt;
      &lt;td&gt;0.933069&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0.411344&lt;/td&gt;
      &lt;td&gt;0.617469&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1.703587&lt;/td&gt;
      &lt;td&gt;0.718288&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;-0.123720&lt;/td&gt;
      &lt;td&gt;0.445176&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;0.208545&lt;/td&gt;
      &lt;td&gt;0.459722&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0.312234&lt;/td&gt;
      &lt;td&gt;0.788584&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;200 rows × 2 columns&lt;/p&gt;
&lt;/div&gt;

&lt;h1&gt;df[::-c]&lt;/h1&gt;

&lt;p&gt;마찬가지로, df[::-c] 이면 뒤에 row부터 2간격마다 row가 선택됩니다. 아래는 예시입니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;sampledf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[::&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div&gt;
&lt;style scoped=&quot;&quot;&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;A&lt;/th&gt;
      &lt;th&gt;B&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;199&lt;/th&gt;
      &lt;td&gt;2.600890&lt;/td&gt;
      &lt;td&gt;0.775489&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;197&lt;/th&gt;
      &lt;td&gt;-0.365103&lt;/td&gt;
      &lt;td&gt;0.413758&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;195&lt;/th&gt;
      &lt;td&gt;0.685687&lt;/td&gt;
      &lt;td&gt;0.933069&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;193&lt;/th&gt;
      &lt;td&gt;0.267967&lt;/td&gt;
      &lt;td&gt;0.020342&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;191&lt;/th&gt;
      &lt;td&gt;-0.918194&lt;/td&gt;
      &lt;td&gt;0.917082&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;0.924938&lt;/td&gt;
      &lt;td&gt;0.837344&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;0.890616&lt;/td&gt;
      &lt;td&gt;0.096270&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;-0.603043&lt;/td&gt;
      &lt;td&gt;0.697143&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1.703587&lt;/td&gt;
      &lt;td&gt;0.718288&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;0.208545&lt;/td&gt;
      &lt;td&gt;0.459722&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;100 rows × 2 columns&lt;/p&gt;
&lt;/div&gt;

</content>
 </entry>
 
 <entry>
   <title>pandas.DataFrame.any(), numpy.any()</title>
   <link href="http://localhost:4000/code%20snippet/2020/12/01/any-all-usage/"/>
   <updated>2020-12-01T00:00:00+09:00</updated>
   <id>http://localhost:4000/code%20snippet/2020/12/01/any-all-usage</id>
   <content type="html">&lt;hr /&gt;

&lt;p&gt;평소에 헷갈리는 any(), all()에 대해 정리하였습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1&gt;df.isna()&lt;/h1&gt;

&lt;div class=&quot;prompt input_prompt&quot;&gt;
In&amp;nbsp;[1]:
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;prompt input_prompt&quot;&gt;
In&amp;nbsp;[2]:
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'./data/top1_1880109251922.csv'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index_col&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'date'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'date'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Date'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'date'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Date'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;asfreq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'D'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;df.isna()는 데이터프레임에서 NaN 요소에 해당되는 부분을 True로 리턴해준다.&lt;/p&gt;

&lt;div class=&quot;prompt input_prompt&quot;&gt;
In&amp;nbsp;[3]:
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;isna&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div&gt;
&lt;style scoped=&quot;&quot;&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;date&lt;/th&gt;
      &lt;th&gt;store&lt;/th&gt;
      &lt;th&gt;product_c&lt;/th&gt;
      &lt;th&gt;sales&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Date&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;2018-02-01&lt;/th&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2018-02-02&lt;/th&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2018-02-03&lt;/th&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2018-02-04&lt;/th&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2018-02-05&lt;/th&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2019-07-27&lt;/th&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2019-07-28&lt;/th&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2019-07-29&lt;/th&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2019-07-30&lt;/th&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2019-07-31&lt;/th&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;546 rows × 4 columns&lt;/p&gt;
&lt;/div&gt;

&lt;h1&gt;df.any()&lt;/h1&gt;

&lt;p&gt;여기서, dataframe.any(axis=0)인 경우엔 각 column의 row를 다 훑어서, row요소들 중 적어도 하나의 row애 True가 있으면, True를 반환합니다.&lt;/p&gt;

&lt;div class=&quot;prompt input_prompt&quot;&gt;
In&amp;nbsp;[4]:
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;isna&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;any&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext output_data_text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;date         True
store        True
product_c    True
sales        True
dtype: bool
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;반면에, dataframe.any(axis=1)인 경우엔 각 index별로 column요소를 다 훑어서 적어도 하나의 column에 True가 있으면 True를 반환합니다.
아래 코드를 보면, 해당 index에 data, store, product_c, sales가 모두 True이면 해당 index row는 True를 반환합니다.&lt;/p&gt;

&lt;div class=&quot;prompt input_prompt&quot;&gt;
In&amp;nbsp;[5]:
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;isna&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;any&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext output_data_text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Date
2018-02-01    False
2018-02-02    False
2018-02-03    False
2018-02-04    False
2018-02-05    False
              ...  
2019-07-27     True
2019-07-28     True
2019-07-29    False
2019-07-30    False
2019-07-31    False
Freq: D, Length: 546, dtype: bool
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1&gt;np.any()&lt;/h1&gt;

&lt;p&gt;np.any() 는 dataframe.any()와 유사합니다. 주어진 축(axis) 정보에 따라 해당 요소에서 True가 하나 이상이라도 있으면 True를 반환합니다. 아래는 np.any()의 예제입니다.&lt;/p&gt;

&lt;div class=&quot;prompt input_prompt&quot;&gt;
In&amp;nbsp;[6]:
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;먼저, 예제 array를 생성합니다. True, False로 구성된 random한 array를 만들었습니다.&lt;/p&gt;

&lt;div class=&quot;prompt input_prompt&quot;&gt;
In&amp;nbsp;[7]:
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;samplearr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;choice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;samplearr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;47&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;choice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;samplearr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;47&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;choice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;samplearr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;47&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext output_stream highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[ True  True  True  True  True  True False  True  True  True False False
 False  True False  True False False False False  True False  True  True
 False  True False False False  True False  True False  True  True False
 False  True False False  True False False  True  True  True False]
[False False  True False  True False  True  True False  True  True False
  True False  True False False False False False  True  True False False
 False False  True False False  True False False False  True False  True
  True  True False False  True  True  True  True False  True  True]
[False False  True False False False False  True False False  True  True
  True False False  True  True  True  True False False False False  True
 False  True  True  True  True  True  True  True False False False  True
 False False False False False False  True  True  True  True  True]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;np.any(a,b,c)는 에러를 발생합니다. 반드시, 하나의 array나 아니면 array와 유사한 list형식으로 묶어서 넣어줘야합니다.&lt;/p&gt;

&lt;div class=&quot;prompt input_prompt&quot;&gt;
In&amp;nbsp;[8]:
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;any&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext output_traceback_line highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;---------------------------------------------------------------------------
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext output_traceback_line highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;TypeError                                 Traceback (most recent call last)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext output_traceback_line highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;ipython-input-8-7a7facd3228c&amp;gt; in &amp;lt;module&amp;gt;
----&amp;gt; 1 np.any(a,b,c)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext output_traceback_line highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;__array_function__ internals&amp;gt; in any(*args, **kwargs)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext output_traceback_line highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py in any(a, axis, out, keepdims)
   2328 
   2329     &quot;&quot;&quot;
-&amp;gt; 2330     return _wrapreduction(a, np.logical_or, 'any', axis, None, out, keepdims=keepdims)
   2331 
   2332 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext output_traceback_line highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py in _wrapreduction(obj, ufunc, method, axis, dtype, out, **kwargs)
     85                 return reduction(axis=axis, out=out, **passkwargs)
     86 
---&amp;gt; 87     return ufunc.reduce(obj, axis, dtype, out, **passkwargs)
     88 
     89 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext output_traceback_line highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;TypeError: only integer scalar arrays can be converted to a scalar index
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2&gt;axis=0 vs. axis=1&lt;/h2&gt;

&lt;p&gt;그전에 axis=0과 1에 따라 차이를 살펴봅시다. axis=0인 경우엔 각 column의 모든 row를 훑고, axis=1인 경우엔 각 row의 모든 column을 훑습니다. 아래는 관련 그림입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2020-12-01-any-all-usage_files/axis.jpg&quot; alt=&quot;jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;axis=0인 경우, 각각의 column요소에서 모든 row를 훑어서 하나 이상이 True요소라면 True를 반환합니다. 결과는 [a,b,c]의 column의 갯수만큼 출력됩니다.&lt;/p&gt;

&lt;div class=&quot;prompt input_prompt&quot;&gt;
In&amp;nbsp;[9]:
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;any&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext output_data_text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True, False,  True,  True,  True,  True, False,  True,  True,
        True,  True,  True,  True,  True, False,  True,  True,  True,
        True,  True, False, False,  True,  True,  True,  True,  True,
        True,  True])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;axis=1인 경우, 각각의 row요소에서 모든 column을 훑어서 하나 이상이 True요소라면 True를 반환합니다. 결과는 [a,b,c]의 row 갯수만큼 출력됩니다.&lt;/p&gt;

&lt;div class=&quot;prompt input_prompt&quot;&gt;
In&amp;nbsp;[10]:
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;any&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext output_data_text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;array([ True,  True,  True])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1&gt;np.all()&lt;/h1&gt;

&lt;p&gt;np.all()은 np.any()와 반대로, 검사할 축에 모든 요소가 True여야지만 True를 반환합니다. 아래는 예제입니다.&lt;/p&gt;

&lt;div class=&quot;prompt input_prompt&quot;&gt;
In&amp;nbsp;[11]:
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext output_data_text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;array([False, False,  True, False, False, False, False,  True, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False,  True, False, False, False, False, False, False,
       False, False, False, False, False, False, False,  True, False,
        True, False])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;prompt input_prompt&quot;&gt;
In&amp;nbsp;[12]:
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext output_data_text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;array([False, False, False])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

</content>
 </entry>
 
 <entry>
   <title>General Approach to Time Series Analysis - Time Series Data, Stationarity 등에 대하여</title>
   <link href="http://localhost:4000/time%20series%20analysis/2020/11/23/time-series-intro(1)/"/>
   <updated>2020-11-23T00:00:00+09:00</updated>
   <id>http://localhost:4000/time%20series%20analysis/2020/11/23/time-series-intro(1)</id>
   <content type="html">&lt;p&gt;이번 포스팅을 시작으로, 시계열 분석에 대해서 다루도록 하겠습니다. 메인 교재는 Brockwell와 Richard A. Davis의 &amp;lt; Introduction to Time Series and Forecasting &amp;gt; 와 패스트캠퍼스의 &amp;lt;파이썬을 활용한 시계열 분석 A-Z&amp;gt; 를 듣고 정리하였습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2&gt;1.1. What is Time Series?&lt;/h2&gt;

&lt;p&gt;시계열이란, 일정 시간 간격으로 배치된 데이터들의 수열입니다. 그 중에서 discrete한 시계열이란 관측이 발생한 시각 t의 집합 $T_0$ 가 discrete한 경우이며, 관측치가 시간 구간 안에서 연속적으로 발생한다면 continuous한 시계열입니다.&lt;/p&gt;

&lt;p&gt;시계열 시퀀스는 일반적으로 자기 상관성이 있는 수열입니다. 즉, 과거의 데이터가 현재를 넘어서 미래까지 영향을 미치는 것을 뜻합니다.&lt;/p&gt;

\[Cov(X_i, X_j) \neq 0\]

&lt;p&gt;따라서, 시계열 데이터로 모델링을 하기 위해선 먼저 데이터를 최대한 분해해서 살펴봐야 합니다. 확률 모델링을 하기 위해선 i.i.d 여야 하기 때문입니다. 일반적으로 시계열 데이터는 trend, seasonality, noise 항으로 구성되어 있습니다. 여기서 시계열 데이터가 자기 상관성을 가지게 되는 요인은 trend와 seasonality 요소 때문이고, noise는 i.i.d한 독립변수로 구성된 에러항입니다.&lt;/p&gt;

&lt;h2&gt;1.2. Objectives of Time Series Analysis&lt;/h2&gt;
&lt;p&gt;시계열 분석의 목적은 주로 시계열 데이터를 보고 앞으로 일어날 일들을 예측하는 것입니다. 그러기 위에선 기존에 있는 시계열 데이터를 가지고 추론을 해야합니다. 따라서, 이러한 추론을 하기 위해선 가정에 맞는 적절한 확률 모델을 선택하여 모델링을 진행해야 합니다.&lt;/p&gt;

&lt;p&gt;그러나, 시계열 데이터는 자기 상관성이 존재하는 데이터입니다. 따라서 확률적 모델링을 통해 이 시계열 데이터를 서로 독립인 데이터로 변환해야 하는데 이 과정이 seasonal adjustment 또는 trend and seasonal decomposition입니다. 그 밖에 log transformation, differencing 같은 과정도 존재합니다.&lt;/p&gt;

&lt;p&gt;어쨌든, 시계열 분석의 궁극적인 목표는 독립적인 변수로 최대한 변환한 뒤, 이를 기반으로 확률적 통계 모델링을 해서, inference를 하는 것입니다. Inference 결과는 다시 우리가 얻고자 하는 예측값으로 바꾸기 위한 reverting 과정을 거쳐야 합니다. 왜냐하면, seasonal adjustment나 Decomposition을 통해 상관성을 제거했기 때문에 원하는 예측값을 얻기 위해선 다시 원래대로 이 과정을 뒤집어서 돌아가야 하는 것입니다.&lt;/p&gt;

&lt;h2&gt;1.3. Some Simple Time Series Models&lt;/h2&gt;
&lt;p&gt;위에서 말씀 드린 것과 같이 시계열 데이터를 보고 적절한 확률 모델을 선택하는 것은 매우 중요합니다. 따라서, 몇가지 간단한 time series model을 소개하겠습니다.&lt;/p&gt;

&lt;h3&gt;1.3.1. Definition of a time series model&lt;/h3&gt;
&lt;p&gt;관측된 ${x_t}$ 에 대한 time-series 모델이란, 랜덤 변수 ${X_t}$ 시퀀스들의 joint distribution 을 의미합니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;A time series model for the observed data ${x_t}$ is a specification of the joint distributions(or possibly only the means and covariances) of a sequence of random variables ${X_t}$ of which ${x_t}$ is postulated to be a realization.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;즉, 랜덤 변수들의 시퀀스 ${X_1, X_2, \dots }$ 로 구성된 time-series 확률 모델은 랜덤 벡터 $(X_1, \dots, x_n)’ ,\,\, n=1,2,\dots,$ 의 결합 분포입니다. 아래 그림은 랜던 변수들의 시퀀스 ${S_t, t=1, \dots, 200}$ 로 나올 수 있는 가능성 중 한가지가 ‘실현’ 된 것입니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/izJrvZl.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 1. Time-series 예시&lt;/figcaption&gt;&lt;/p&gt;

&lt;h3&gt;1.3.2. Some Simple Time Series Model&lt;/h3&gt;

&lt;ol&gt;&lt;li&gt;iid Noise&lt;br /&gt;
가장 기본적인 time series 모델은 noise항으로만 이뤄진 경우입니다.(거의 현실세계에선 없다고 생각하시면 됩니다.)&lt;/li&gt;
&lt;li&gt;Binary Process&lt;br /&gt;
i.i.d Noise의 종류로, binary 분포를 따르는 noise인 경우입니다. 랜덤 변수들의 시퀀스 $\{X_t,\,\,t=1,2,\dots,\}$ 가 $P[X_t = 1]=p$ , $P[X_t = -1] = 1-p$ 를 따릅니다.&lt;/li&gt;
&lt;li&gt;Models with only Trend&lt;br /&gt;
trend요소와 noise항만 있는 경우입니다. 여기서 trend요소란 패턴이 선형관계를 가지고 있을 때입니다. 자세히 말하면, 시계열이 시간에 따라 증가, 감소, 또는 일정 수준을 유지하는 경우입니다.

$$X_t = m_t + Y_t$$

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/AjlrE80.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 2. Time series with only trend component&lt;/figcaption&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;Models with only Seaonality&lt;br /&gt;
seasonal요소와 noise항만 있는 경우입니다. 여기서 seasonal요소란 일정한 빈도로 주기적으로 반복되는 패턴을 말합니다. 반면에, 일정하지 않은 빈도로 발생하는 패턴은 Cycle이라 합니다.(여기서는 seasonal 기준으로 설명하겠습니다.)

$$X_t = S_t + Y_t$$&lt;/li&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/6NZcDiO.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 3. Times series with only seasonality(period=12month)&lt;/figcaption&gt;&lt;/p&gt;
&lt;/ol&gt;

&lt;h3&gt;1.3.3 A General Approach to Time Series Modeling&lt;/h3&gt;
&lt;p&gt;시계열 분석에 대해 깊게 들어가기 전에, 시계열 데이터 모델링하는 방법에 대해 대략적으로 알아봅시다.&lt;/p&gt;

&lt;p&gt;1) 그래프로 그린 후, 그래프 상에서 아래와 같은 요소가 있는지 체크한다.(Plot the series and examine the main features of the graph)&lt;br /&gt;&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;trend&lt;/li&gt;&lt;li&gt;a seasonal component&lt;/li&gt;&lt;li&gt;any apparent sharp changes in behavior&lt;/li&gt;&lt;li&gt;any outlying observations&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;2) 정상상태의 잔차를 얻기 위해, trend와 seasonality 요소를 제거한다. (Remove the trend and seasonal components to get stationary residuals)&lt;br /&gt;
     trend와 seasonality 요소를 제거하기 전에, 전처리를 해야하는 경우가 있습니다. 예를 들어, 아래와 같이 지수적으로 증가하는 경우에, 로그를 취해서 variance가 일정하도록 만든 후 모델링을 하면 정확도를 높일 수 있습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/V85l07h.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 1. 로그 취하기 전&lt;/figcaption&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/e0GKRKU.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 2. 로그 취한 후&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;이외에도 여러 방법이 있습니다. 추후에 설명하도록 하겠습니다. 어쨌든, 이 모든 방법들의 핵심은 &lt;b&gt;정상상태의 잔차&lt;/b&gt;를 만드는 것입니다.&lt;/p&gt;

&lt;p&gt;3) auto-correlation 함수, 여러 다양한 통계량을 이용하여 잔차를 핏팅할 모델을 선택한다. (Choose a model to fit the residuals, making use of various sample statistics including the sample autocorrelation function)&lt;/p&gt;

&lt;p&gt;4) 핏팅된 모델로 예측한다.&lt;br /&gt; 
     여기서 잔차를 예측하는 것이고, 예측된 잔차를 원래 예측해야 할 값으로 변환한다.&lt;/p&gt;

&lt;h2&gt;1.4. Stationary Models and the Autocorrelation Function&lt;/h2&gt;

&lt;p&gt;시계열 데이터가 정상상태(stationarity)를 가지기 위해서, 시계열이 확률적인 특징이 시간이 지남에 따라 변하지 않는다는 가정을 충족시켜야 합니다. 그러나 시계열 데이터는 trend와 seasonality요소로 인해, 평균과 분산이 변할 수 있습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;a time series ${{X_t, t=0, \pm1, …}}$ is said to be stationary if it has statistical properties similar to those of the “time-shifted” series ${{X_{t+h}, t=0, \pm1, …}}$ for each integer h.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Trends can result in a varying mean over time, whereas seasonality can result in a changing variance over time, both which define a time series as being non-stationary. Stationary datasets are those that have a stable mean and variance, and are in turn much easier to model.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;시계열에 대한 평균과 공분산은 아래와 같이 정의됩니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/65biJ1q.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 3. 시계열의 평균과 공분산&lt;/figcaption&gt;&lt;/p&gt;

&lt;h4&gt;Strict Stationarity vs. Weak Stationarity&lt;/h4&gt;
&lt;p&gt;엄격한 정상상태가 되려면,  $(X_1,\dots , X_n)$ 의 결합분포와 $(X_{1+h}, \dots, X_{n+h})$ 의 결합분포가 시간간격 h에 상관없이 동일해야 합니다. 그러나 이를 이론적으로 증명하기 어렵기 때문에, 약한 정상상태(weak stationarity)만을 만족하면 정상상태에 있다고 생각하고 시계열 문제를 풉니다. 약한 정상상태는 아래 조건을 만족합니다. 즉, 결합분포가 동일해야 한다는 강력한 조건이 사라졌기 때문에 약한 정상상태라고 하는 것입니다.&lt;/p&gt;

&lt;p&gt;\((1) \,\, E(X_t) = u\)
\((2) \,\, Cov(X_{t+h}, X_{t}) = \gamma_h, for\; all\; h\)
\((3)\,\, Var(X_t) = Var(X_{t+h})\)&lt;/p&gt;

&lt;p&gt;(2)식은 공분산은 t에 독립임을 의미합니다. 정상상태 시계열의 공분산은 아래와 같이 하나의 변수 h에 대해 나타낼 수 있습니다.&lt;/p&gt;

\[\gamma_X(h) = \gamma_X(h,0) = \gamma_X(t+h, t)\]

&lt;p&gt;이때 함수 $\gamma_X(\cdot)$ 을 lag h에 대한 auto-covariance 함수(ACVF)라 합니다. auto-correlation 함수(ACF)는 ACVF를 이용해 아래와 같이 정의됩니다.&lt;/p&gt;

\[\rho_X(h)=\frac{\gamma_X(h)}{\gamma_X(0)}=Cor(X_{t+h}, X_t)\]

&lt;h4&gt;White Noise&lt;/h4&gt;
&lt;p&gt;시계열 ${{X_t}}$ 가 독립적인 랜덤 변수의 시퀀스이고, 평균이 0이고, 분산이 $\sigma^2$ 이면, White Noise라 합니다. 아래는 White Noise의 조건입니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
\[E(X_t)=0\]
  &lt;/li&gt;
  &lt;li&gt;
\[V(X_t)=V(X_{t+h})=\sigma^2\]
  &lt;/li&gt;
  &lt;li&gt;
\[\gamma_X(t+h, t)=0\;(h\neq0)\]
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;1.4.1 The Sample Autocorrelation Function&lt;/h3&gt;

&lt;p&gt;관측 데이터 가지고 자기 상관의 정도를 볼때, sample auto-correlation 함수(sample ACF)를 사용합니다. Sample ACF는 ACF의 추정으로, 계산은 아래와 같습니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/jjzBk3z.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 4. Sample ACF&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;White Noise인 경우, 시계열 그래프와 ACF 그래프는 아래와 같습니다. lag가 1이상인 경우, 거의 ACF값이 0에 가까운 것을 볼 수 있고, 95% 신뢰구간 안에 들어와 있습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/RaoTZJj.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 5. White Noise ACF&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;아래는 그림 1. 그래프에 플롯된 데이터를 가지고 그린 ACF입니다. 보시면, ACF가 lag가 커짐에 따라 서서히 감소하는 형태를 띄는데 이는 trend가 있는 데이터에서 나타납니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/N6vk5oN.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 6. Sequence with trend ACF&lt;/figcaption&gt;&lt;/p&gt;

&lt;h2&gt;1.5. Estimation and Elimination of Trend and Seasonal Components&lt;/h2&gt;

&lt;p&gt;trend와 seasonality가 존재하는 시계열의 모델링인 경우, 아래와 같이 additive 형태를 띌 수 있습니다.&lt;/p&gt;

\[X_t = m_t + s_t + Y_t\]

&lt;p&gt;시계열 모델링의 최종 목표는 잔차항 $Y_t$ 가 정상상태에 놓이게 하는 것입니다. 따라서 잔차항을 분석하기 위해서 trend 요소 $m_t$ 와 seasonal 요소 $s_t$ 를 제거해야 합니다.&lt;/p&gt;

&lt;h3&gt;1.5.1. Estimation and Elimination of Trend in the Absence of Seasonality&lt;/h3&gt;
&lt;p&gt;seasonal 요소가 없고, trend요소만 있는 모델링은 아래와 같이 진행할 수 있습니다.&lt;/p&gt;

\[X_t = m_t + Y_t, \quad t=1, \dots ,n, \; where \; EY_t = 0\]

&lt;h4&gt;method1. Trend Estimation&lt;/h4&gt;

&lt;p&gt;trend 요소를 추정하는 방법은 Moving Average와 Smoothing을 이용하는 방법 2가지가 있습니다.&lt;/p&gt;

&lt;h5&gt;a) Smoothing with a finite moving average filter&lt;/h5&gt;

&lt;p&gt;과거 n개의 시점을 평균을 구해 다음 시점을 예측하는 방식입니다.&lt;/p&gt;

\[W_t = (2q+1)^{-1}\sum_{j=-q}^{q}X_{t-j}\]

&lt;p&gt;이때, $X_t = m_t + Y_t$ 이므로, 아래와 같은 식으로 유도됩니다.&lt;/p&gt;

\[W_t = (2q+1)^{-1}\sum_{j=-q}^{q}X_{t-j} = (2q+1)^{-1}\sum_{j=-q}^{q}m_{t-j} + (2q+1)^{-1}\sum_{j=-q}^{q}Y_{t-j}\]

&lt;p&gt;만약에 $m_t$ 가 대략 선형관계를 띄고 있다면 잔차항의 평균은 0에 가까울 것입니다. 즉, 트렌드가 선형관계를 띄고 있을 때, moving average filter를 씌어주면 trend요소만 추출할 수 있는 것을 의미합니다.&lt;/p&gt;

\[W_t = (2q+1)^{-1}\sum_{j=-q}^{q}X_{t-j} = (2q+1)^{-1}\sum_{j=-q}^{q}m_{t-j} + (2q+1)^{-1}\sum_{j=-q}^{q}Y_{t-j} \approx m_t\]

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/rEHZBt2.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 7. Moving average filter 취하기 전&lt;/figcaption&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/QPByqUu.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 8. Moving average filter 취한 후&lt;/figcaption&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/dPTzLn3.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 9. Trend 제거 후 잔차항&lt;/figcaption&gt;

위에 그림 7,8,9 를 살펴 봅시다. 그림 8은 그림 7에서 과거시점 5개를 이용하여 moving average 필터를 씌운 후입니다. 뚜렷한 트렌드가 있지 않음을 보실 수 있습니다. ~~잔차항에 대한 분석은 다시 한번 살펴봐야 할 것 같습니다.~~

Simple Moving Average Filter는 trend가 linear하고, Noise가 White Noise일 때, 시계열 데이터에서 trend요소를 잘 추출할 수 있습니다. 그러나 Non-linear한 trend라면, Noise가 White Noise라 하더라도, trend 추정이 올바르지 않습니다. 그럴 땐, 적절한 가중치를 부여하여 Moving Average Filter를 씌워야 합니다.

$$\sum_{j=-7}^{j=7}a_jX_{t-j} = \sum_{j=-7}^{j=7}a_j m_{t-j}+\sum_{j=-7}^{j=7}a_jY_{t-j} \approx \sum_{j=-7}^{j=7}a_j m_{t-j} = m_t$$

&lt;h5&gt;b) Exponential smoothing&lt;/h5&gt;
Moving averages는 과거 n개의 시점에 동일한 가중치를 부여하는 방법입니다. 그러나, 현재시점과 가까울수록 좀 더 현재시점에 영향을 많이 미치는 경우가 일반적으로 생각하기엔 자연스러울 수 있습니다. 예로 주식을 생각하면 될 것 같습니다. 따라서, Exponential smoothing 방법은 현재 시점에 가까울수록 더 큰 가중치를 주는 방법입니다. 

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/ciknR6Y.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 10. Exponential Smoothing&lt;/figcaption&gt;&lt;/p&gt;

Exponential Smoothing 수식은 아래와 같습니다.

$$\hat{m}_t = \alpha X_t + (1-\alpha)\hat{m}_{t-1},\,\,t=2, \dots, n,$$
$$\hat{m}_1=X_1$$

아래 그림은 그림 7을 exponential smoothing을 취한 trend 추정 그래프입니다.
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/hKOWuWu.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 11. Exponential Smoothing 취한 후&lt;/figcaption&gt;&lt;/p&gt;

&lt;h5&gt;c) Smoothing by elimination of high-frequency component&lt;/h5&gt;
trend를 추출하는 방법 중 하나로, 여러 frequency의 합으로 trend를 표현해서 이를 제거하는 것입니다(이 부분은 추후에 4장에 가서 다시 설명하도록 하겠습니다).

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/hn90Hgr.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 12. frequency합으로 smoothing을 취한 후( $\alpha=0.4$ )&lt;/figcaption&gt;&lt;/p&gt;

&lt;h5&gt;d) Polynomial fitting&lt;/h5&gt;
$m_t = a_0 + a_1t + a_2t^2 + \dots + a_nt^n$ 으로 모델링하여, $\sum_{t=1}^n(x_t-m_t)^2$ 을 최소화하는 방식으로 파라미터 $a_k,\,(k=0, \dots, k=n$ 을 구하는 방식으로 trend를 추정할 수 있습니다. 

&lt;del&gt;$X_t - Y_t = m_t$ 에서, $Y_t$ 는 stationary state을 가정하고 있기 때문에, polynomial model을 구축할 수 있는 것입니다.&lt;/del&gt;

&lt;h4&gt;method2. Trend Elimination by Differencing&lt;/h4&gt;
method1 방법은 trend를 추정한 뒤, 시계열 $\{X_t\}$ 에서 빼주는 방식으로 trend를 제거하였습니다. 이번엔 difference(차분)를 통해서 trend요소를 제거하는 방법을 알아보도록 하겠습니다. Lag-1 difference operator $\bigtriangledown$ 는 아래와 같습니다.

$$\bigtriangledown X_t = X_t-X_{t-1} = (1-B)X_t$$

B는 backward-shift operator로 $BX_t = X{t-1}$ 입니다. j lag difference는 $\bigtriangledown (X_t) = \bigtriangledown (\bigtriangledown^{j-1} (X_t))$ 입니다. 예를 들어, 2-lag difference는 아래와 같습니다.

$$ \begin{align*} \bigtriangledown^2 X_t&amp;amp;=\bigtriangledown (\bigtriangledown (X_t))=\bigtriangledown ((\bigtriangledown (X_t))\\&amp;amp;=(1-B)(1-B)X_t=(1-2B+B^2)X_t = X_t - 2X_{t-1} + X_{t-2}\end{align*} $$

&lt;h5&gt;Why difference helps eliminating trend components? (Maybe or seasonal components)&lt;/h5&gt;
여기서, 제가 공부하면서 궁금했던 포인트는 왜 difference가 trend 제거에 도움이 되는가? 였습니다. 제가 생각한 답은 아래와 같습니다. trend와 seasonal 요소를 제거하려는 이유는 '고정된 평균과 분산을 가지는 분포'를 가지기 위해서입니다. 그래야지 통계적 모델링이 가능하기 때문입니다. 즉 반대로 말하면, trend와 seasonal 요소는 시간에 따라 평균과 분산이 변함을 의미합니다. 즉 그 변하는 요소를 제거하기 위해서 difference를 하는 것입니다. 

difference를 통해서 변동성을 제거하는 건 고등학교 수학 때 배웠던 미분을 통해 이해할 수 있습니다. 예를 들어, 일차함수 $y=a+bx$ 는 x값에 따라 y값이 변합니다. 그러나 일차미분을 통해 구한 기울기 b값은 고정이 됩니다. 반면에 이차함수 $y=ax^2 + bx + c$ 는 이차미분을 통해 2a라는 고정값을 갖게 됩니다. 여기서 미분 과정을 difference라 생각하시면 됩니다.

&amp;gt; 영어로도 미분이 differentiation 임을 생각하면 와닿습니다.

일차함수 y는 변하는 특성 + 고정된 특성을 둘다 가지고 있는데 일차 미분을 통해 a라는 고정된 특성만을 추출하는 것입니다. 

만약에 trend가 일차함수와 같은 관계를 가지고 있다면 1-lag difference 만으로도 변동성을 잡을 수 있게 되는 것이지요. 마찬가지로 2-lag difference는 trend가 이차함수와 같은 관계를 가지고 있다면 적용되는 것입니다. 

그러나, 과도한 difference는 시계열을 과하게 변동성을 제거해 버려서, over-correction이 될 수도 있기 때문에 조심해야 합니다.

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/dPdnSMm.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 13. Difference 적용 전&lt;/figcaption&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/RPMUFSJ.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 14. Difference 적용 후&lt;/figcaption&gt;&lt;/p&gt;

&lt;h3&gt;1.5.2. Estimation and Elimination of Both Trend and Seasonality&lt;/h3&gt;

trend와 seasonal 요소가 다 있는 경우 아래와 같이 표현될 수 있습니다(additive model인 경우).&lt;del&gt;multiplicative model인 케이스도 있습니다.&lt;/del&gt;

$$X_t = m_t + s_t + Y_t, \,\, t=1, \dots, n,$$
$$where,\,\,EY_t = 0, s_{t+d}=s_t,\,\,and\,\,\sum_{j=1}^{d}s_j=0$$

두 가지 방법을 소개하겠습니다. 먼저, 첫번째 방법입니다.

&lt;h4&gt;method 1. Estimation of Trend and Seasonal components&lt;/h4&gt;
아래와 같은 데이터가 있을 때, trend와 seasonal 요소를 제거해 봅시다. 아래 시계열 같은 경우, 주기가 d=12로, 1년 단위로 싸이클이 반복되는 것을 확인할 수 있습니다.

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/hCcOOp9.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 15. Accidental Deaths, U.S.A., 1973-1978&lt;/figcaption&gt;&lt;/p&gt;

&lt;ol&gt;&lt;li&gt;먼저, trend 요소를 제외합니다. trend 요소를 제외하는 방법으로 moving average filter를 이용할 수 있습니다.&lt;br /&gt;&lt;br /&gt;
예를 들어, 시계열 시퀀스 $\{x_1, x_2, \dots, x_n\}$ 이고, 주기 period $d=2q$ 라 한다면, 아래와 같은 moving average filter 식을 세울 수 있습니다.

$$\hat{m_t} = (0.5x_{t-q} + x_{t-q+1} + \dots + x_{t+q-1} + 0.5x_{t+q})/d,\,\, q&amp;lt;t\leq n-q$$ 

&amp;gt; 양 끝에 0.5씩 붙는 이유는 분자의 갯수는 홀수개 즉 $2q+1$ 이지만, 분모는 짝수 $d=2q$ 이기 때문에, 양 끝에 항의 가중치를 0.5씩만 해주는 것입니다.

반면에, 주기가 $d=2q+1$ 이라면, 아래와 같은 식을 세울 수 있습니다.

$$\hat{m_t} = (2q+1)^{-1}\sum_{j=-q}^{q}X_{t-j}$$
&lt;/li&gt;
&lt;li&gt;그 다음은 seasonality 요소를 구하는 차례입니다. 먼저, 위에서 구한 trend요소를 원 시계열 데이터에서 $x_{k+jd} - \hat{m_{k+jd}}$ 와 같이 제거해야 합니다. 그런 다음, 동일한 주기에 해당하는 $x-\hat{m}$ 요소들을 가지고 평균 $w_k, \,\,(k=1, \dots, d)$ 를 구해줍니다. &lt;/li&gt;
&lt;li&gt;이 때, $w_k$ 들의 평균은 0이 아닐 수 있습니다. 따라서, seasonal 요소들의 평균이 0이 되도록 다시 한번 평균을 빼줍니다.&lt;del&gt;다시 한번 정규화가 되도록 해주는 것입니다.&lt;/del&gt;) 

$$\hat{s_k} = w_k - \frac{1}{d}\sum_{i=1}^{d}w_i,\,\,k=1, \dots, d$$

$$and, \,\, \hat{s_k}=\hat{s_{k-d}},\,k&amp;gt;d$$

따라서, deseaonalized된 데이터는 $d_t = x_t - \hat{s_t},\,\, t=1,\dots ,n$ 이며, detrended된 데이터는 $d_t = x_t - \hat{m_t},\,\, t=1, \dots, n$ 입니다.
&lt;/li&gt;
&lt;li&gt;마지막으로, noise 추정값은 trend와 seasonal 요소를 모두 제거한 항입니다.&lt;/li&gt;
&lt;li&gt;또한 trend 모델링을 위해, 다시 한번 parametric form으로 다시 한번 재추정하는 과정을 거칩니다. Parametric form으로 다시 한번 trend 요소를 재추정 하는 목적은 prediction과 simulation을 하기 위해서 입니다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/srdCVkU.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 16. Trend and seasonal decomposition 예시&lt;/figcaption&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/oYkGLqN.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 17. Trend and seasonal decomposition 예시&lt;/figcaption&gt;&lt;/p&gt;

&lt;h4&gt;method 2. Elimination of Trend and Seasonal components by Differencing&lt;/h4&gt;
Trend 요소를 Differencing 방법을 통해 제거한 것과 동일하게 진행됩니다. Differencing operator $\bigtriangledown_d$ 을 $X_t = m_t + s_t + Y_t$ 식 양변에 취해주면 아래와 같습니다.

$$\bigtriangledown_dX_t = m_t - m_{t-d} + Y_t - Y_{t-d}$$


&lt;h2&gt;1.6. Testing the Estimated Noise Sequence&lt;/h2&gt;

1.5까지 과정을 거치면 우린 Noise 항을 갖게 됩니다. 그러나 이 Noise 항이 White Noise 항인지는 확인이 필요합니다. 만약에 white noise 항이 맞다면, noise sequence를 모델링 하는 것입니다. 만약에 noise 항이 white noise가 아니라 여전히 depedency가 보인다면 다른 방법을 적용해야 합니다.

이번 챕터에서는 white noise인지를 확인하는 방법에 대해 살펴봅니다.

&lt;h3&gt;(a) The sample autocorrelation function&lt;/h3&gt;

Sample acf를 그려서, 95%신뢰구간 안에 대부분 들어와 있는지 확인합니다. 만약 2,3개 이상이 신뢰구간 밖에 있거나 1개가 유난히 구간 안에 멀리 벗어 났다면, 우린 white noise라고 세웠던 가설을 기각해야 합니다.

&lt;h3&gt;(b) The portmanteau test(Ljung-Box test)&lt;/h3&gt;

Portmanteau 검정 통계량은 &lt;b&gt;일정 기간 동안 일련의 관측치가 랜덤이고 독립적인지 여부를 검사하는데 사용합니다&lt;/b&gt;. 통계량은 아래와 같습니다(Box-pierece 검정이라고도 합니다.). 

$$Q = n\sum_{j=1}^{h}\hat{\rho(j)^2}$$

$\hat{\rho(j)}$ 가 0에 가깝다면, $\hat{\rho(j)^2}$ 은 더욱 0에 가까울 것이지만, 몇몇 $\hat{\rho(j)}$ 의 절대값이 크다면, 그 항들에 영향을 받아 전체적인 Q값도 커지게 될 것입니다. 

&amp;gt; h는 lag입니다. h를 무리하게 크게 잡는다면, Q값은 커질 위험이 있습니다. 적당한 h를 잡는 것이 중요합니다.

귀무가설은 시차 h에 대한 자기 상관이 0이라는 귀무가설을 검정합니다. 통계량이 지정된 임계값보다 크면 하나 이상의 시차에 대한 자기 상관이 0과 유의하게 다르며, 일정 기간 랜덤 및 독립적이지 않음을 뜻합니다.

아래는 좀 더 refined된 통계량으로 Ljung-Box 라 합니다.

$$Q = n(n+2)\sum_{k=1}^{h}\hat{\rho(k)}/(n-k)$$

그 밖에, turning point test, difference-sign test, rank test, fitting an autoregressive model, checking for normality등이 있습니다. 

&lt;hr /&gt;
이상으로, &amp;lt;Introduction to Time Series and forecasting 리뷰) 1. Introduction to Time Series&amp;gt; 포스팅을 마치겠습니다. 

&lt;hr /&gt;

&lt;ol&gt;&lt;li&gt;
&lt;a href=&quot;&quot; url=&quot;https://blog.naver.com/sw4r/221024668866&quot;&gt;Strict Stationarity vs. Weak Stationarity, https://blog.naver.com/sw4r/221024668866&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;고려대학교 김성범 교수님 &lt;예측모델&gt; 수업자료&amp;lt;/li&amp;gt;
&lt;li&gt;&lt;a href=&quot;&quot; url=&quot;https://otexts.com/fppkr/residuals.html&quot;&gt;portmanteau 검정 : https://otexts.com/fppkr/residuals.html&amp;lt;/li&amp;gt;&amp;lt;/ol&amp;gt;
&lt;/a&gt;&lt;/li&gt;&lt;/예측모델&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>RDD, Resilient Distributed DataSet에 대하여[1]</title>
   <link href="http://localhost:4000/spark%20programming/2020/11/20/rdd/"/>
   <updated>2020-11-20T00:00:00+09:00</updated>
   <id>http://localhost:4000/spark%20programming/2020/11/20/rdd</id>
   <content type="html">&lt;p&gt;이번 포스팅은 “빅데이터 분석을 위한 스파크2 프로그래밍 - Chaper2. RDD” 를 읽고 정리하였습니다. 정리 순서는 책 순서와 동일하고, 책을 읽어가면서 이해가 안되는 부분을 추가적으로 정리하였습니다.&lt;/p&gt;

&lt;h2&gt;2.1 RDD&lt;/h2&gt;
&lt;h3&gt;2.1.1 들어가기에 앞서&lt;/h3&gt;

&lt;p&gt;RDD를 공부하기 전 기억하고 넘어가야 할 것들에 대해 정리하였습니다.&lt;/p&gt;
&lt;h4&gt;1. 스파크 클러스터&lt;/h4&gt;
&lt;p&gt;클러스터란 여러 대의 서버가 마치 한대의 서버처럼 동작하는 것을 뜻합니다. 스파크는 클러스터 환경에서 동작하며 대량의 데이터를 여러 서버에서 병렬 처리합니다&lt;/p&gt;

&lt;h4&gt;2. 분산 데이터로서의 RDD&lt;/h4&gt;
&lt;p&gt;RDD는 Resilient Distrubuted Datasets으로, ‘회복력을 가진 분산 데이터 집합’이란 뜻입니다. (Resilient : 회복력이 있는) 데이터를 처리하는 과정에서 문제가 발생하더라도 스스로 복구할 수 있는 것을 의미합니다.
이는 그 다음 설명 &lt;b&gt;트랜스포메이션과 액션&lt;/b&gt;과 &lt;b&gt;지연(lazy) 동작과 최적화&lt;/b&gt; 부분과 함께 다시 설명드리도록 하겠습니다.&lt;/p&gt;

&lt;h4&gt;3. 트랜스포메이션과 액션&lt;/h4&gt;
&lt;p&gt;RDD가 제공하는 연산은 크게 트랜스포메이션과 액션이 있습니다. “연산”은 흔히 “메서드”로 이해하시면 됩니다.&lt;br /&gt;
트랜스포메이션은 RDD의 변형을 일으키는 연산이고, 실제로 동작이 수행되지는 않습니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/wWLMGK1.jpg&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 1. RDD 예시&lt;/figcaption&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/ooJKxAu.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 2.RDD 예시(2)&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;아래 예시를 보면, 데이터를 읽어 RDD를 생성해서 file변수에 저장한 뒤, flatMap -&amp;gt; map -&amp;gt; reduceByKey 함수를 거치면서 RDD[2], RDD[3], RDD[8]을 새로 생성하는 것을 볼 수 있습니다. 이렇게 transformation을 이전 RDD를 변형해서 새로운 RDD를 생성하는 것입니다.&lt;/p&gt;

&lt;p&gt;반면에, action은 동작을 수행해서 원하는 타입의 결과를 만들어내는 것이므로, saveAsTextFile로 수행됩니다. 따라서, saveAsTextFile은 action 연산에 해당됩니다.&lt;/p&gt;

&lt;h4&gt;4. 지연 동작과 최적화&lt;/h4&gt;
&lt;p&gt;지연 동작이란, 액션 연산이 수행되기 전까지 실제로 트랜스포메이션 연산을 수행하지 않는 것입니다. 이는 RDD의 특성 중 하나인 ‘회복력’과 관련있습니다. 액션 연산이 수행되기 전까지 동작이 &lt;b&gt;지연&lt;/b&gt;이 되는데, 대신에 RDD가 생성되는 방법을 기억하는 것입니다. 따라서 문제가 발생하더라도 기존에 RDD가 생성되는 방법을 기억하여 연산 수행에 문제가 없도록 하는 것입니다. 이는 위의 예시에서 reduceByKey까지는 실제로 트랜스포메이션 연산을 수행하는 것이 아니라 해당 연산을 순서대로 기억해놨다가, saveAsFile연산이 수행될 때(액션 연산이 수행될 때) 비로소 트랜스포메이션 연산도 수행된 것입니다.&lt;/p&gt;

&lt;p&gt;지연 동작 방식의 큰 장점은 &lt;b&gt;실행계획의 최적화&lt;/b&gt;입니다.&lt;/p&gt;

&lt;h4&gt;RDD의 불변성&lt;/h4&gt;
&lt;p&gt;오류로 인해 스파크의 데이터가 일부 유실되면, 데이터를 다시 만들어내는 방식으로 복구되는 것이 RDD의 불변성입니다. 이는 위에서 계속 언급한 “회복력”과 관련됩니다.&lt;/p&gt;

&lt;p&gt;RDD는 RDD1-&amp;gt;RDD2-&amp;gt; … 가 되면서 한번 만들어진 RDD는 내용이 변경되지 않습니다. RDD를 만드는 방법을 기억해서 문제가 발생 시 언제든지 똑같은 데이터를 생성할 수 있습니다.&lt;/p&gt;

&lt;h4&gt;5. 파티션과 HDFS&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;RDD데이터는 클러스터를 구성하는 여러 서버에 나뉘어서 저장됨&lt;/li&gt;
  &lt;li&gt;이 때, 분할된 데이터를 파티션 단위로 관리합니다.&lt;/li&gt;
  &lt;li&gt;HDFS는 하둡의 파일 시스템(hadoop distributed file system)&lt;/li&gt;
  &lt;li&gt;스파크는 하둡 파일 입출력 API에 의존성을 가지고 있음.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;6. Job, Executor, 드라이버 프로그램&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Job : 스파크 프로그램 실행하는 것 = 스파크 잡(job)을 실행하는 것&lt;/li&gt;
  &lt;li&gt;하나의 잡은 클러스터에서 병렬로 처리됨&lt;/li&gt;
  &lt;li&gt;이 때, 클러스터를 구성하는 각 서버마다 executor라는 프로세스가 생성&lt;/li&gt;
  &lt;li&gt;각 executor는 할당된 파티션 데이터를 처리함&lt;/li&gt;
  &lt;li&gt;드라이버란 ? 스파크에서 잡을 실행하는 프로그램으로, 메인함수를 가지고 있는 프로그램&lt;/li&gt;
  &lt;li&gt;드라이버에서 스파크 컨테스트를 생성하고 그 인스턴스를 포함하고 있는 프로그램&lt;/li&gt;
  &lt;li&gt;스파크컨테스트를 생성해 클러스터의 각 워커 노드들에게 작업을 지시하고 결과를 취합하는 역할을 수행&lt;/li&gt;
  &lt;li&gt;아래 코드를 보면, main함수 안에 sparkcontext를 생성하고 sc라는 인스턴스를 포함하고 있는 것을 볼 수 있음. 즉, main함수를 가지고 있는 프로그램이 ‘드라이버’에 해당됨&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Java&quot;&gt;Public static void main(String[] args){
	...
	JavaSparkContext s c = getSparkContext(&quot;WordCount&quot;, args[0]);
	...}
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;7. 함수의 전달&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;스파크는 함수형 프로그래밍 언어인 스칼라로 작성되어 “함수”를 다른 함수의 “매개변수”로서 전달 가능&lt;/li&gt;
  &lt;li&gt;아래 예제(Scala)를 보면 map의 인자에 ‘_+1’이 전달되는데, 익명 함수로 전달되는 것임&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Scala&quot;&gt;val rdd1 = sc.paralleize(1 to 10)
val rdd2 = rdd1.map(_+1)
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
  &lt;li&gt;파이썬으로 작성하면 아래와 같이, lambda 함수가 매개변수로 들어가게 됨&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;rdd1.map(lambda v:v+1)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;[참고]함수형 프로그래밍&lt;/h4&gt;
&lt;p&gt;함수형 프로그래밍과 객체 지향 프로그래밍의 차이를 통해 이해해보겠습니다. 객체 지향 프로그래밍은 객체 안에 상태를 저장하고, 해당 상태를 이용해서 제공할 수 있는(메소드)를 추가하고 상태변화를 ‘누가 어디까지 볼 수 있게 할지’를 설정하고 조정합니다. 따라서 적절한 상태 변경이 되도록 구성합니다. 반면에 함수형 프로그래밍은 상태 변경을 피하며 함수 간의 데이터 흐름을 사용합니다. 입력은 여러 함수들을 통해 흘러 다니게 됩니다. 따라서, 함수의 인자로 함수가 들어오고 반환의 결과로도 함수가 나올 수 있습니다.&lt;/p&gt;

&lt;h4&gt;함수 전달 시 유의할 점&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&quot;language-Scala&quot;&gt;Class PassingFunctionSample{
	val count=1
	def add(I: int):Int={
	count+i
	}
	
	def runMapSample(sc:SparkContext){
	val rdd1 = sc.parallelize(1 to 10);
	val rdd2 = rdd1.map(add)}
	}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;위와 같이 코드를 작성해서 실행하면, ‘java.io.NotSerializaionException’이라는 오류가 발생합니다. 이는 전달된 add함수가 클러스터를 구성하는 각 서버에서 동작할 수 있도록 전달되어야 하는데, 전달이 안되기 때문입니다. 그 이유는 add함수는 PassingFunctionSample의 메소드로 결국 클래스 PassingFunctionSample이 전체 다 전달되기 때문입니다. 해당 클래스는 Serializable 인터페이스를 구현하지 않습니다. 즉, 클래스가 각 서버에 전달될 수 있는 기능을 가지고 있지 않는 것입니다. 함수만 따로 전달되어야 하는 것입니다.&lt;/p&gt;

&lt;p&gt;스칼라 같은 경우 ‘싱글톤 객체’를 이용하여 해결 할 수 있습니다. 파이썬의 예제도 살펴보면, 아래는 클래스 전체가 전달되는 잘못된 예입니다.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;class PassingFunctionSample():

    def add1(self, i):
        return i + 1

    def runMapSample1(self, sc):
        rdd1 = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
        rdd2 = rdd1.map(self.add1) 
        # rdd2 = rdd1.map(add2)
        print(&quot;, &quot;.join(str(i) for i in rdd2.collect()))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;self로 인해 전체 클래스가 전달됩니다.(파이썬은 예외없이 실행되므로 유의할 것!)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;class PassingFunctionSample():

    @staticmethod
    def add1(self, i):
        return i + 1

    def runMapSample1(self, sc):
        rdd1 = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
        rdd2 = rdd1.map(add2)
        print(&quot;, &quot;.join(str(i) for i in rdd2.collect()))


if __name__ == &quot;__main__&quot;:

    def add2(i):
        return i + 1

    conf = SparkConf()
    sc = SparkContext(master=&quot;local[*]&quot;, appName=&quot;PassingFunctionSample&quot;, conf=conf)

    obj = PassingFunctionSample()
    obj.runMapSample1(sc)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;위와 같이 함수 add2가 독립적으로(클래스 전체가) 전달될 수 있도록 해야합니다.&lt;/p&gt;

&lt;h4&gt;변수 전달 시 유의할 점&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&quot;language-Scala&quot;&gt;class PassingFunctionSample {

	var increment = 1

  def runMapSample3(sc: SparkContext) {
    val rdd1 = sc.parallelize(1 to 10)
    val rdd2 = rdd1.map(_ + increment) \\익명함수 전달
    print(rdd2.collect.toList)
  }

  def runMapSample4(sc: SparkContext) {
    val rdd1 = sc.parallelize(1 to 10)
    val localIncrement = increment
    val rdd2 = rdd1.map(_ + localIncrement)
    print(rdd2.collect().toList)
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;runMapSample3 처럼 변수가 직접 전달되면 안되고, runMapSample4처럼 지역변수로 변환해서 전달해야 합니다. 그래야 나중에 변수가 변경되어 생기는 문제를 방지할 수 있습니다.&lt;/p&gt;

&lt;h4&gt;데이터 타입에 따른 RDD 연산&lt;/h4&gt;
&lt;p&gt;RDD 연산 함수에서 인자 타입을 보고 적절하게 맞는 연산 함수를 사용해야 합니다.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;이상으로 &amp;lt;RDD, Resilient Distributed DataSet에 대하여[1]&amp;gt; 마치겠습니다. 다음 포스팅에서 이어가도록 하겠습니다.&lt;/p&gt;

&lt;hr /&gt;
&lt;ol&gt;
  &lt;li&gt;함수형 언어, &lt;a href=&quot;https://sungjk.github.io/2017/07/17/fp.html&quot;&gt;https://sungjk.github.io/2017/07/17/fp.html&lt;/a&gt;, &lt;a href=&quot;https://docs.python.org/ko/3/howto/functional.html&quot;&gt;https://docs.python.org/ko/3/howto/functional.html&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</content>
 </entry>
 
 <entry>
   <title>스파크(Spark) 소개</title>
   <link href="http://localhost:4000/spark%20programming/2020/11/17/introspark/"/>
   <updated>2020-11-17T00:00:00+09:00</updated>
   <id>http://localhost:4000/spark%20programming/2020/11/17/introspark</id>
   <content type="html">&lt;p&gt;데이터 분석가의 역할이 점차 데이터 분석 영역에서 벗어나 조금씩 엔지니어의 영역까지 요구되고 있는 것 같아, 데이터 엔지니어링 관련 공부에 대한 필요성을 느꼈습니다. 데이터 분석을 직업으로 함에도 불구하고, 하둡이나 스파크 등과 같은 빅데이터 분석 쪽은 많이 접해보지 못했습니다. 따라서 이번 포스팅을 시작해서 해당 관련 포스팅을 틈틈히 올리도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;먼저, 교재 &amp;lt;빅데이터 분석을 위한 스파크2 프로그래밍, 대용량 데이터 처리부터 머신러닝까지 - 백성민 저자&amp;gt;를 읽고 포스팅을 진행하도록 하겠습니다. 본 포스팅은 &amp;lt;ch1. 스파크 소개&amp;gt; 에 대해 다뤘습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1&gt;빅데이터의 등장 및 정의&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;4차 산업 혁명과 더불어 데이터 처리 기술이 발달함&lt;/li&gt;
  &lt;li&gt;빅데이터를 크기(volume), 다양성(variety), 속도(velocity)를 이용해서 초기에 정의 내림
    &lt;ul&gt;
      &lt;li&gt;다양한 형태를 지닌 대량의 데이터가 빠른 속도로 쌓이고 있다면 이를 빅데이터라 부름&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;그러나 추후에 가변성(variability), 정확성(veracity), 복잡성(complexity), 시인성(visibility)가 추가됨&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;스파크&lt;/h1&gt;
&lt;p&gt;스파크는 하둡의 단점이 개선되어 나온 것으로, 하둡은 아래와 같음&lt;/p&gt;

&lt;h3&gt;하둡이란?&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;구글이 대용량 처리와 관련된 두 개의 논문을 더그 커팅이 실제 구현하면서 시작된 아파치 프로젝트임&lt;/li&gt;
  &lt;li&gt;분산 환경의 병렬처리 프레임워크로, 분산 파일 시스템인 HDFS와 데이터 처리를 위한 맵리듀스 프레임워크로 구성됨
    &lt;ul&gt;
      &lt;li&gt;이후에 CPU, 메모리 등 컴퓨팅 자원 관리를 전담하는 리소스 관림 시스템인 Yarn을 포함해, 기존 맵리듀스 프로그래밍 모델을 Yarn 기반으로 구축 관리할 수 있게 함&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;여러대의 서버를 하나의 클러스터로 구성하여, 클러스터 컴퓨팅 환경 제공&lt;/li&gt;
  &lt;li&gt;기본적인 동작 방법은, 데이터는  HDFS에 저장하고 데이터 처리를 맵리듀스 프로그래밍을 이용하여 HDFS 위에서 수행&lt;/li&gt;
  &lt;li&gt;&lt;b&gt;HDFS?&lt;/b&gt;
    &lt;ul&gt;
      &lt;li&gt;하나의 네임노드와 여러  데이터노드로 구성됨&lt;/li&gt;
      &lt;li&gt;하나의 네임노드가 다른 데이터노드를 관리하는 형태로 시스템이 돌아감.&lt;/li&gt;
      &lt;li&gt;전체 데이터는 일정한 크기(블록)로 나눠서 여러 데이터 노드에 분산되어 저장하고, 각 블록이 어디에 저장되어 있는지에 대한 메타 정보가 네임노드에 저장됨&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;500&quot; src=&quot;https://imgur.com/6WwaRY4.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 1. HDFS&lt;/figcaption&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;b&gt;맵리듀스 프레임워크?&lt;/b&gt;
    &lt;ul&gt;
      &lt;li&gt;데이터 처리 프레임워크로 데이터를 여러 개의 맵 프로세스와 리듀서 프로세스로 나눠서 처리하는 방식&lt;/li&gt;
      &lt;li&gt;위 그림에서 맵리듀스 잡이 실행되면 네임노드로부터 메타정보를 읽어서 데이터 위치를 확인하고, 데이터를 처리함&lt;/li&gt;
      &lt;li&gt;맵 프로세스는 여러 데이터 노드에 분산 저장된 데이터를 각 서버에서 병렬로 나누어 처리하며, 리듀서는 그러한 맵 프로세스들의 결과를 조합해 최종 결과를 만들어냄&lt;/li&gt;
      &lt;li&gt;그러나, 하둡의 맵리듀스 잡은 대부분의 연산 작업을 파일시스템 기반으로 처리하기 때문에 데이터 처리 성능이 떨어짐&lt;/li&gt;
      &lt;li&gt;반면에, 스파크는 메모리를 이용한 데이터 처리 방식을 제공함으로써 높은 성능을 보여줌&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;스파크란?&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;스파크는 하둡 기반의 맵리듀스 작업의 단점을 보완한 것으로, 데이터를 메모리 기반으로 처리하기 때문에 처리 성능이 향상됨&lt;/li&gt;
  &lt;li&gt;또한 작업을 실행하기 전에 최적의 처리 흐름을 찾는 과정을 포함함&lt;/li&gt;
  &lt;li&gt;맵리듀스에 비해 훨씬 다양한 데이터 처리 함수 제공&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;RDD, 데이터프레임, 데이터셋 소개와 연산&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;스파크 프로그램 내에서 ‘데이터’를 표현하고 처리하기 위한 프로그래밍 모델로, RDD, 데이터프레임(Dataframe), 데이터셋(Dataset)이 있음
    &lt;ul&gt;
      &lt;li&gt;데이터프레임과 데이터셋은 RDD가 보완되서 나온 것이므로, RDD가 기본임&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;RDD&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;RDD란, resilient distributed dataset으로, 병렬 처리가 가능한 요소로 구성되며 데이터를 처리하는 과정에서 일시적인 문제가 발생하더라도 스스로 에러를 복구할 수 있는 능력을 지닌 데이터 모델임&lt;/li&gt;
  &lt;li&gt;RDD에 속한 요소들은 파티션이라는 단위로 나눠질 수 있는데, 스파크는 데이터 처리 작업을 수행할 때, 파티션 단위로 나뉘어서 병렬 처리함. &lt;del&gt;하둡에서 맵 프로세스가 여러 데이터 노드에 분산 저장된 데이터를 각 서버에서 병렬 처리하는 것과 유사하다고 생각함&lt;/del&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;기존 RDD는 덧셈 연산, 그룹화 연산등을 적용 시, 파티션에 속한 데이터들이 네트워크를 통해 다른 서버로 이동하는 셔플링이 발생될 수 있음&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;그런데, 작업 도중에 장애가 발생하여 결과가 유실될 수 있으나, 스파크 같은 경우는 RDD의 생성 과정을 기록해 뒀다가 다시 복구해 주는 기능을 가지고 있음. 이를 수행 하기 위해선 한번 생성된 RDD는 바뀌지 않아야 하는 조건이 있음
    &lt;ul&gt;
      &lt;li&gt;이렇게 스파크에서 RDD 생성 과정을 기록해 둔 것을 리니지(lineage)라고 함&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;DAG&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;맵 리듀스 작업으로 데이터 처리 시, 데이터 처리 방법의 너무 복잡한 경우나 데이터의 크기가 너무 큰 경우 서버의 가용량을 넘겨서 문제가 발생하는 경우가 발생&lt;/li&gt;
  &lt;li&gt;따라서, 한번의 작업으로 모든 것을 끝낸다기 보다는 하나의 작업을 여러 개의 작은 작업으로 나눠 놓고 각 작업을 최적화해서 일련의 순서대로 나누어 실행하는 경우가 종종 있음&lt;/li&gt;
  &lt;li&gt;일련의 작업 흐름을 나타내는 워크플로우는 DAG(Directed acyclic graph)를 구성하고, 이를 이용해 일련의 작업을 수행하면 다양한 라이브러리를 연동해서 데이터 처리를 수행 가능
    &lt;ul&gt;
      &lt;li&gt;DAG? 란 꼭짓점과 방향성을 가진 엣지로 구성된 그래프 모델임&lt;/li&gt;
      &lt;li&gt;빅데이터는 DAG를 이용해 복잡한 일련의 작업 흐름을 나타냄&lt;/li&gt;
      &lt;li&gt;예) 우지 워크플로우 오픈소스&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;스파크 내에서의 DAG&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;스파크에서 DAG 처리를 담당하는 부분 : DAG스케쥴러&lt;/li&gt;
  &lt;li&gt;스파크는 전체 작업을 스테이지로 나누고, 다시 여러개의 태스크로 나눠서 수행&lt;/li&gt;
  &lt;li&gt;드라이버의 메인 함수에서는 스파크 애플리케이션과 스파크 클러스터 연동을 담당하는 스파크 컨텍스트가 있음. 스파크 컨텍스트를 이용해 잡을 실행하고 종료하는 역할을 함
    &lt;ul&gt;
      &lt;li&gt;드라이버란 ? RDD를 생성하고 각종 연산을 호출하는 프로그램&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;드라이버가 스파크 컨텍스트를 통해 RDD의 연산 정보를 DAG스케쥴러에 전달하면 스케쥴러는 이 정보를 가지고 실행계획을 수립. 그 후, 클러스터 매니저에게 전달함&lt;/li&gt;
  &lt;li&gt;DAG스케쥴러는 데이터에 대한 지역성을 높이는 전략과 관련된 것. 즉, 전체 데이터 처리 흐름을 분석해서 네트워크를 통한 데이터 이동이 최소화되도록 스테이지를 구성하는 것이 스케쥴링의 역할임
    &lt;ul&gt;
      &lt;li&gt;&lt;b&gt;데이터의 지역성&lt;/b&gt;이란 ? 데이터를 처리하는 장소로 따로 보내 처리하는 것이 아니라, 데이터가 저장되어 있는 서버(블록별로 저장되어 있음)에서 처리하는 것.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;데이터 이동이 최소화되도록 한다는 것은 네트워크를 통한 데이터 이동 즉 &lt;b&gt;“셔플링”&lt;/b&gt;을 최소화하는 것임&lt;/li&gt;
  &lt;li&gt;셔플링 관련 예시&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;방법 1)&lt;/li&gt; 
&lt;ol&gt;&lt;li&gt;10대 서버에 저장된 모든 로그를 상품에 따라 재분류 한다.&lt;/li&gt;
&lt;li&gt;상품별 총 판매건수를 계산한 후 원하는 상품의 정보만을 출력한다.&lt;/li&gt;&lt;/ol&gt;
&lt;li&gt;방법 2)&lt;/li&gt;
&lt;ol&gt;&lt;li&gt;전체 로그를 분류하기 전에 각 서버별로 원하는 상품 정보만 걸러낸 후 상품별 총 판매건수를 계산한다.&lt;/li&gt;&lt;li&gt;각 서버별로 따로 계산된 상품별 판매건수를 한 곳으로 모아서 더한 후 출력한다.&lt;/li&gt;&lt;/ol&gt;&lt;/ul&gt;

&lt;ul&gt;
  &lt;li&gt;방법 1의 경우, 10대 서버의 모든 로그 파일을 상품별로 재분류하는 것으로 작업을 시작하나, 동일한 상품번호를 가진 로그파일이 10대의 서버의 무작위로 흩어져 있음. 따라서, 재분류를 위한 “셔플링”이 대량으로 발생함&lt;/li&gt;
  &lt;li&gt;방법 2의 경우, 셔플을 수행하기 전에 각 서버에 먼저 상품별 부분 집계를 수행한 후 집계된 결과 파일만을 대상으로 네트워크를 통한 2차 합계 연산을 수행하기 때문에, 방법 1에 비해 셔플이 적음&lt;/li&gt;
  &lt;li&gt;&lt;b&gt;$\rightarrow$ 셔플이 최소화되는 방향으로 연산 수행해야 함&lt;/b&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;람다 아키텍쳐&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;빅데이터 처리를 위한 시스템을 구성하는 방법 중 하나로, 네이선 마츠가 제안한 아키텍쳐 모델&lt;/li&gt;
  &lt;li&gt;기존과 같은 대용량 데이터 처리 뿐만 아니라 실시간 로그 분석과 같은 실시간 처리도 중요해짐에 따라 두가지 요구를 충족시키는 아키텍쳐가 필요해졌음&lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img width=&quot;500&quot; src=&quot;https://t1.daumcdn.net/cfile/tistory/9995AE4F5BFCC50B20&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 2. 람다 아키텍쳐&lt;sup&gt;[1]&lt;/sup&gt;&lt;/figcaption&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;람다 아키텍쳐 운영 방법&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;&lt;li&gt;새로운 데이터는 일괄 처리 계층(Batch layer)과 속도 계층(speed layer) 모두에게 전달됨&lt;/li&gt;
&lt;li&gt;일괄 처리 계층(Batch layer)은 원본 데이터를 저장하고 일정 주기마다 한번 씩 일괄적으로 가공해서 배치뷰를 생성함. 이때 배치란, 특정시간마다 주기적으로 계산을 수행하는 것을 말함. 뷰라고 한 부분은 외부에 보여지는 데이터, 결과 데이터임&lt;/li&gt;
&lt;li&gt;속도 계층은 들어오는 데이터를 즉시 또는 매우 짧은 주기로 철리해 실시간 뷰(Real-time view)를 생성함&lt;/li&gt;
&lt;li&gt;서빙 계층(serving layer)은 실시간 뷰와 배치 뷰의 결과를 적절히 조합하여 사용자에게 데이터를 전달함. 물론 서빙 계층을 거치지 않고 배치뷰 또는 실시간뷰를 직접 조회(query)할 수 있음&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;즉, 일괄 처리 작업을 통해 데이터를 처리하지만 아직 배치 처리가 수행되지 않은 부분은 실시간 처리를 통해 보완함&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;이상으로, 본 포스팅을 마치겠습니다. 다음 포스팅은 &lt;a href=&quot;https://ralasun.github.io/spark%20programming/2020/11/20/rdd/&quot;&gt;&amp;lt;RDD, Resilient Distributed DataSet에 대하여[1]&amp;gt;&lt;/a&gt; 에 대해 진행하도록 하겠습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a name=&quot;lambda-archi&quot;&gt;1. &lt;a href=&quot;https://jhleed.tistory.com/122&quot;&gt;람다 아키텍쳐, https://jhleed.tistory.com/122&lt;/a&gt;&lt;/a&gt;&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Model-Free Policy Control, Monte Carlo와 Temporal Difference에 대하여</title>
   <link href="http://localhost:4000/reinforcement%20learning/2020/07/29/mc-td-control/"/>
   <updated>2020-07-29T00:00:00+09:00</updated>
   <id>http://localhost:4000/reinforcement%20learning/2020/07/29/mc-td-control</id>
   <content type="html">&lt;p&gt;이번 포스팅은 지난 포스팅 &lt;a href=&quot;https://ralasun.github.io/reinforcement%20learning/2020/07/28/mc-td-eval/&quot;&gt;Model-Free Policy Evaluation&lt;/a&gt;에 이어 Model-Free Policy Control에 대해 다루도록 하겠습니다. CS234 4강, Deep Mind의 David Silver 강화학습 강의 5강, Richard S. Sutton 교재 Reinforcement Learning: An Introduction의 Chapter 5, 6 기반으로 작성하였습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;지난 포스팅에서는 일정 정책 $\pi$ 아래 환경 모델을 모를 때 가치함수를 추정하는 방법인 Monte-Carlo(MC) policy evaluation과 Temporal Difference(TD) policy evaluation에 대해 다뤘습니다. 그러나 sequential decision prcoess 문제의 최종 목표는 최적화된 정책을 갖는 것(Control)입니다. 환경 모델을 알 때 Dynamic Programming(DP)는 policy iteration과 value iteration을 통해 최적 정책을 구할 수 있습니다. 환경 모델을 모를 때 최적 정책을 찾는 방법 Model-Free Control에 대해 자세히 다루기 전에 먼저, Generalized Policy Iteration에 대해 알아보겠습니다.&lt;/p&gt;

&lt;h2&gt;Generalized Policy Iteration&lt;/h2&gt;
&lt;p&gt;DP에서의 policy iteration을 다시 자세히 살펴봅시다. 정책 발전(policy improvement)를 greedy하게 하였으며, policy evaluation과 policy improvement를 번갈아 반복하는 policy iteration을 통해 최적 가치함수와 최적 정책을 구했습니다.&lt;/p&gt;

\[\pi_0 \overset E\to v_{\pi_0} \overset I\to \pi_1 \overset E\to v_{\pi_1} \overset I\to \pi_2 \overset E\to \cdots \overset I\to \pi_\ast \overset E\to v_\ast\]

&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/89000330-0ee5fb00-d332-11ea-82a4-d700a773ada1.png&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 1. Policy Iteration&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;&amp;lt;그림 1.&amp;gt;은 policy iteration을 그림으로 표현한 것입니다. 두 선은 각각 수렴된 가치함수와 정책들을 의미하고, 화살표는 policy evaluation과 policy improvement를 나타냅니다. 이 과정은 모두 결국 최적 정책과 최적 가치함수를 찾기 위한 것이기 때문에 두 선은 한 점에서 만납니다. 그런데, policy evaluation은 수렴할 때까지 시간이 오래 소요됩니다. 따라서, 위 가치함수 라인에 다다를 때까지 policy evaluation을 수행할 필요가 있을까요 ?&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/89000708-fd512300-d332-11ea-99f1-ebe65c2e5fc7.jpg&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 2. Value Iteration&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;policy evaluation은 수렴할 때까지 시간이 오래 소요되기 때문에, 수렴할 때까지 기다리는 것이 아니라, 좀 더 효율적으로 접근하는 방법이 value iteration입니다. 가치함수를 한 스텝에 대해서만 업데이트를 하고, greedy policy improvement를 수행하는 value iteration을 통해 최적 가치함수와 정책을 찾았습니다.&lt;/p&gt;

&lt;p&gt;위 두 방법 모두 결국 policy evaluation과정과 policy improvement과정의 상호작용으로 이뤄집니다. 두 과정 모두 안정화될 때, 즉 더 이상의 변화나 발전이 이뤄지지 않을 때, 그 때의 가치함수와 정책은 최적입니다. 따라서, 상호작용되는 과정이 조금씩은 차이가 있을 수 있지만 결국 둘의 상호작용으로 최적점에 다다르게 되는 것입니다. 이것이 바로 Generalized Policy Iteration(GPI)입니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img height=&quot;400&quot; src=&quot;https://user-images.githubusercontent.com/37501153/89001031-d34c3080-d333-11ea-85cd-0893f0f52fa7.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 3. Generalized Policy Iteration&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;Model-free control도 마찬가지로 GPI를 통해 최적 가치 함수와 최적 정책을 구합니다. Model-free control에 대해 알아보도록 하겠습니다. Model-free policy evaluation하는 방법으로 Monte-Carlo(MC)와 Temporal Difference(TD)가 있습니다. 마찬가지로, model-free control 하는 방법으로도 Monte-Carlo control와 Temporal-Difference control이 있습니다. 먼저, Monte-Carlo control부터 알아보겠습니다.&lt;/p&gt;

&lt;h2&gt;Monte-Carlo Control&lt;/h2&gt;
&lt;p&gt;지난 포스팅에서 알아본 monte-carlo estimation이 이제 control에 어떻게 사용되는지 생각해봅시다.&lt;/p&gt;

&lt;h3&gt;Monte Carlo Estimation of Action Values&lt;/h3&gt;
&lt;p&gt;Monte-Carlo control도 Monte-Carlo estimation과 함께 GPI를 통해 최적정책을 찾아나갑니다. 그러나, DP에서 다른 점이 있습니다. DP는 현재 상태 $s$ 에서 행동 $a$ 를 취했을 때, 받을 수 있는 보상과 다음 상태가 어떻게 될지 알 수 있습니다. 따라서 다음 상태로 올 수 있는 모든 후보들과 보상을 고려하여 최대 가치를 반환하는 다음 상태를 찾은 후 그 상태로 가게 되는 행동을 취합니다. 즉, 상태 가치 함수 정보만으로 충분합니다.&lt;/p&gt;

&lt;p&gt;그러나 model-free 환경의 문제점은 직접 경험하지 않는 이상 다음 상태와 보상이 어떻게 될지 알 수 없습니다. 따라서 상태 가치 함수만으로 행동을 선택할 때 충분한 정보를 제공하지 못합니다. 이러한 이유로 model-free control에서는 상태 가치 함수 $v(s)$ 에 대한 evaluation이 아니라, &lt;b&gt;상태-행동 가치 함수 $q(s,a)$ 에 대한 evaluation&lt;/b&gt;을 수행합니다. 상태 s에 대해 모든 행동 a에 대해 $q(s,a)$ 를 비교하여 가장 가치가 높은 행동을 선택하는 것이 상태 s에 대한 정책이 되는 것입니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;171&quot; alt=&quot;model-free-gpi&quot; src=&quot;https://user-images.githubusercontent.com/37501153/89005037-76a24300-d33e-11ea-88ad-2f2cdb180929.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 4. GPI with Q value&lt;/figcaption&gt;&lt;/p&gt;

&lt;h3&gt;Importance of Exploration&lt;/h3&gt;
&lt;p&gt;GPI는 ‘좋은’ 정책 $\pi$ 을 계속 찾아나가면 언젠간 최적 정책 $\pi_*$ 에 수렴합니다. 그러면 ‘좋은’ 정책 $\pi$ 는 ‘좋은’ $Q_\pi$ 추정치를 찾아야 합니다. 그래야지만 policy improvement를 통해 최적 정책을 찾아나갈 수 있기 때문입니다.&lt;/p&gt;

\[q_{\pi}(s, \pi'(s)) \geq v_\pi(s)\]

&lt;p&gt;‘좋은’ $Q_\pi$ 추정치는 어떻게 찾을까요? 가능한 한 나올 수 있는 모든 $(s,a)$ 시퀀스를 경험하면 됩니다. MC policy evaluation에서 true expected value에 수렴하기 위해서 에피소드 샘플링을 많이 해야 하는 것과 같습니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/89006289-5922a880-d341-11ea-9bfc-1fdf7d99a074.jpg&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 5. Greedy policy improvement in MDP and Model-Free&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;그러나 model-free 환경에서 모든 $(s,a)$ 쌍으로 구성된 모든 시퀀스를 경험하기는 어렵습니다. 경우의 수가 너무 많기 때문이고, 환경을 모르기 때문에 예측도 어렵습니다. &amp;lt;그림 5.&amp;gt;를 보면 DP같은 경우는 환경을 알기 때문에 V(s)를 추정하기 위해 다음에 나올 trasition model $P^{a}_{ss’}$ 와 함께 모든 상태 s를 고려할 수 있습니다. 따라서, ‘좋은’ 추정치를 계산할 수 있습니다. 즉, 이렇게 찾아진 가치 함수 추정치 기반으로 greedy하게 행동을 선택해도 policy improvement가 일어납니다( &lt;a href=&quot;https://ralasun.github.io/reinforcement%20learning/2020/07/13/dp/&quot;&gt;DP포스팅 policy improvement&lt;/a&gt; 참고 ).&lt;/p&gt;

&lt;p&gt;반면에, model-free 같은 경우, MC와 TD모두 샘플링을 통해 (s,a)를 경험해 나갑니다. 그렇기 때문에 많은 (s,a)쌍을 방문하지 못하는 문제가 발생합니다. 이로 인해 어떤 (s,a)에 대해서는 좋은 추정치를 얻지 못합니다. 따라서 부정확한 추정치 기반으로 greedy하게 행동을 선택하는 건 심각한 문제를 일으킵니다. Q(s,a)를 추정하는 이유는 상태 s에 있을 때, 여러 행동 a들을 비교하기 위해서입니다. 그러나 어떤 행동 a에 대해서 Q(s,a)가 나쁜 값을 가지게 된다면 공정한 비교가 되지 않습니다. 즉, 학습이 제대로 이뤄지지 않게 되는 것입니다. 이 문제가 바로 &lt;span style=&quot;color:red&quot;&gt;‘exploration’&lt;/span&gt; 문제입니다. 따라서 정책을 평가하기 위한 좋은 Q(s,a)를 구하기 위해선 충분하고 지속적인 탐험(continual exploration)이 보장되어야 합니다.&lt;/p&gt;

&lt;p&gt;충분하고 지속적인 탐험을 가장 심플하게 구현한 건 모든 행동들에 대해 선택할 가능성을 열어두는 것입니다. 이러한 방법 중 하나가 $\epsilon-greedy$ 입니다.&lt;/p&gt;

\[\begin{align*} \pi(a \mid s)&amp;amp;=m \underset a ax{\mathbb E[R_{t+1} = \gamma v_k(S_{t+1})|S_t=s, A_t=a]}\\&amp;amp;=m \underset a ax{\sum_{s',r}p(s',r|s,a)[r+\gamma v_k(s')]} \end{align*}\]

&lt;blockquote&gt;The simplest idea for ensuring continual exploration is that all actions are tried with non-zero probability.&lt;/blockquote&gt;

&lt;p&gt;$\epsilon-greedy$ 는 $\epsilon$ 의 확률로  행동을 랜덤하게 선택하고, $1-\epsilon$ 의 확률로 greedy한 행동을 선택합니다. $\frac{\epsilon}{m} + 1-\epsilon + \frac{\epsilon}{m}\times(m-1) = 1$ 이 되므로 $\epsilon-greedy$ 식을 아래와 같이 구축할 수 있습니다. 여전히 $\epsilon$ 의 확률로 탐험할 가능성을 두는 것입니다.&lt;/p&gt;

\[\pi(a \mid s) = 
	\begin{cases}
		\frac{\epsilon}{m} + 1-\epsilon &amp;amp; \quad \text{if} \quad a^{*}=arg \underset m maxQ(s,a) \\
		\frac{\epsilon}{m} &amp;amp; \quad \text{otherwise}
		\end{cases}\]

&lt;h3&gt; on-policy Monte-Carlo Control &lt;/h3&gt;
&lt;p&gt;&lt;i&gt;이 단락에서 다루는 MC Control은 on-policy 기반입니다. on-policy와 off-policy의 차이는 off-policy MC Control에서 설명하도록 하겠습니다.&lt;/i&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img width=&quot;835&quot; alt=&quot;mcgpi&quot; src=&quot;https://user-images.githubusercontent.com/37501153/89014690-d9043f00-d350-11ea-98b7-544f99f73981.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 6. Monte-Carlo Policy Iteration&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;진짜로 이제 MC기반의 Control에 대해 알아보겠습니다. 위에서 model-free인 경우도 Generalized Policy Iteration을 통해 최적 가치함수와 최적 정책을 찾는다고 하였습니다. MC기반 policy evaluation은 상태-행동 가치 함수인 $Q_\pi$ 를 찾는 것이고, MC기반 policy improvement는 $\epsilon-greedy$ 를 따릅니다. 그런데, &amp;lt;그림 6.&amp;gt;의 왼쪽 그림처럼, policy evaluation을 $Q_\pi$ 를 수많은 에피소드를 샘플링해서 수렴할 때 반복하는 건 너무 번거롭습니다. 따라서 DP의 value iteration처럼 에피소드 하나가 끝날 때까지만 상태-행동 가치 함수 Q를 업데이트하고, policy improvement를 수행합니다.&lt;/p&gt;

&lt;p&gt;이런 방식의 MC 기반 GPI가 과연 최적 정책을 찾게 해주는지에 대해선 아직 해결해야 할 문제가 남았습니다. $\epsilon-greedy$ 방식이 진짜로 정책을 발전 시키는지에 대한 문제와 하나의 최정정책, 가치함수로의 수렴하는지에 대한 문제입니다. 먼저 첫번째 문제부터 살펴보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;$\epsilon-greedy$ Improvement&lt;/b&gt;&lt;br /&gt;
$\epsilon-greedy$ 방식으로 정책을 발전시키려면 $V_{\pi_{i+1}}(s) \geq V_{\pi_{i}}(s)$ 를 만족해야 합니다. 아래는 이와 관련된 증명입니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/89119377-16033980-d4e9-11ea-8e30-c556dbf7263f.jpg&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 7. $\epsilon-greedy$ policy improvement&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;$Q^{\pi_i}(s,\pi_{i+1}(s)) \geq V_\pi(s)$ 이므로, $V_{\pi_{i+1}}(s) \geq V_{\pi_{i}}(s)$ 가 성립합니다. 이렇게 되는 자세한 과정은 지난 &lt;a href=&quot;https://ralasun.github.io/reinforcement%20learning/2020/07/13/dp/&quot;&gt;DP 포스팅 policy improvement&lt;/a&gt;쪽을 참고 바랍니다. 따라서, $\epsilon-greedy$ 에 의한 정책 발전이 일어납니다.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Greedy in the Limit of Infinite Exploration&lt;/b&gt;&lt;br /&gt;
정책 발전이 일어나는 것과 동시에, 매 스텝마다 정책을 발전시키려면 결국 greedy한 정책으로 수렴해야 합니다. $\epsilon-greedy$ 방식은 모든 행동이 선택될 확률이 non-zero probability라 가정을 하지만, 결국 iteration을 반복해 나가면서 하나의 행동에 대해 $\pi(a \mid s) = 1$ 의 확률을 가져야 되는 것입니다. 이것에 관한 내용을 Greedy in the Limit of Infinite Exploration(GLIE) 라 합니다. 따라서, GLIE를 만족해야 수렴된 정책을 가질 수 있습니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/89119773-c5411000-d4eb-11ea-9982-1904dd2dcfc3.jpg&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 8. Greedy in the Limit with Infinite Exploration(GLIE)&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;$e-greedy$ 가 GLIE를 만족하게 하는 가장 심플한 방법은 $\epsilon = \frac{1}{k}$ 로 하여 매 스텝마다 $\epsilon$ 을 감소시켜 0에 수렴하게 하는 것입니다. 따라서, GLIE Monte-Carlo Control 알고리즘을 정리하면 아래와 같습니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/89120084-4699a200-d4ee-11ea-8bfe-9c2cfe05a53a.jpg&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 9. On Policy Monte-Carlo Control&lt;/figcaption&gt;&lt;/p&gt;

&lt;h3&gt;off-policy Monte-Carlo&lt;/h3&gt;
&lt;p&gt;Off-policy MC에 대해 설명하기 전에 on-policy learning과 off-policy learning에 대해 알아보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;on-policy vs. off-policy&lt;/b&gt;&lt;br /&gt;
이제까지 설명한 MC control 방법은 on-policy control입니다. On-policy란 탐험할 때 따르는 정책과 찾고자 하는 최적 정책이 같은 경우입니다. $\epsilon-greedy$ MC control이 왜 on-policy인지 살펴보면 다음과 같습니다. 한 에피소드 내에서 매 상태 s마다 행동 a를 샘플링합니다. 이 때, $\epsilon$ 의 확률로 정책에 따른 행동 $a=arg \underset m maxQ(s,a)$ 을 샘플링합니다. 그리고 한 에피소드가 다 끝나고 정책을 업데이트 할 때도 이렇게 정책에 따른 행동들을 기반으로 가치함수를 업데이트하여 $\pi_k = \epsilon-greedy(Q)$ 로 정책을 발전시킵니다. 즉, &lt;span style=&quot;color:red&quot;&gt;기존 행동 샘플링할 때 사용된 정책 기반으로 정책을 발전시키는 것&lt;/span&gt;입니다. 이것이 바로 &lt;span style=&quot;color:red&quot;&gt;on-policy learning&lt;/span&gt;입니다.&lt;/p&gt;

&lt;blockquote&gt;On-policy learning is&lt;br /&gt;- learn on the job&lt;br /&gt;- learn about policy $\pi$ from experience sampled from $\pi$&lt;/blockquote&gt;

&lt;p&gt;그러나 이미 정책을 발전시키는 과정이 greedy한 정책을 한번 찾은 후, 그 정책 위에서 $\epsilon-greedy$ 같은 방법으로 탐험을 하는 것입니다. 그렇기 때문에 탐험을 하는 (s,a)공간이 매우 협소합니다. 이미 발전시킨 정책 위에서 탐험을 하기 때문에, (s,a) 공간위에서 보면 이미 발전시킨 정책 $\pi$ 에 해당되는 공간 근처에서만 탐험이 이뤄지는 것입니다. 마치 그림으로 표현하면 아래와 같을 수 있습니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/89122084-78ffcb00-d4ff-11ea-8a2a-89d288f13a3a.jpg&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 10. On-policy exploration&lt;/figcaption&gt;&lt;/p&gt;
&lt;p&gt;그렇다면 이를 해결할 수 있는 방법은 어떤 것이 있을까요? 바로, &lt;span style=&quot;color:red&quot;&gt;탐험하는 정책과 최적 정책을 찾기 위해 학습하는 정책을 분리하는 것&lt;/span&gt;입니다. 이것이 바로 &lt;span style=&quot;color:red&quot;&gt;off-policy learning&lt;/span&gt;입니다. 마치 분류 모델을 위한 지도학습을 진행 할 때, 모든 라벨에 해당되는 데이터가 존재하고, 분포도 고루 존재하면 학습이 더 잘되는 것과 비슷하다고 생각하면 됩니다. 좀 더 다양한 경험을 한 시퀀스 데이터가 많으면 best 답안에 가까운 정책을 찾을 수 있습니다. 그렇기 위해선 탐험의 범위가 넓어야 합니다.&lt;/p&gt;
&lt;blockquote&gt;Off-policy learning is&lt;br /&gt;- look over someone's shoulder&lt;br /&gt;- learn about policy $\pi$ from experience sampled from $\mu$&lt;/blockquote&gt;
&lt;p&gt;일반적으로, 학습하고자 하는 정책을 target policy라 하고, 학습을 위한 데이터를 생성하기 위한 정책을 behavior정책이라 합니다.
학습을 위한 데이터를 학습하고 하는 정책에서 ‘벗어나서’ 수집하기 때문에, ‘off-policy’라 합니다.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Importance Sampling&lt;/b&gt;&lt;br /&gt;
대부분의 off-policy 방식은 importance sampling을 이용합니다. Importance sampling이란 기댓값을 계산하고자 하는 확률 분포 $p(x)$ 의 확률 밀도 함수(probability density function, PDF)를 모르거나 안다고 해도 $p$ 에서 샘플을 생성하기 어려울 때, 비교적 샘플을 생성하기 쉬운 $q(x)$ 에서 샘플을 생성하여 $p$ 의 기댓값을 계산하는 것입니다.&lt;/p&gt;

\[\begin{align*} E_{x \sim p}[f(x)]&amp;amp;=\int p(x)f(x)dx\\&amp;amp;=\int \frac{p(x)}{q(x)}q(x)f(x)dx\\&amp;amp;=E_{x \sim q}[\frac{p(x)}{q(x)}f(x)] \end{align*}\]

&lt;p&gt;&lt;b&gt;Importance Sampling for Off-Policy Monte-Carlo&lt;/b&gt;&lt;br /&gt;
그렇다면 importance sampling을 off-policy MC에서 어떻게 이용하는지 알아보도록 하겠습니다. 위에서 설명한 importance sampling 개념대로 target policy와 behavior policy를 보면, 기댓값을 계산하고자 하는 확률 분포 $p(x)$ 에 해당하는 건 target policy $\mu$ 이고, 실제 샘플하는 분포 $q(x)$ 는 behavior policy $\pi$ 입니다. 우리가 계산하고자 하는 기댓값은 $V(s) = E[G_t \mid S_t=s]$ 이므로, 즉 두 분포의 비율 $\frac{\pi(A_t \mid S_t)}{\mu(A_t \mid S_t)}$ 을 $G_t$ 에 곱하여 기댓값을 계산해야 합니다. 그러나, 단일 샘플링이 아니라 전체 에피소드에 대한 샘플링이기 때문에 importance sampling한 $G_t^{\pi/\mu}$ 는 아래와 같습니다.&lt;/p&gt;

\[G_t^{\pi/\mu} = \frac{\pi(A_t \mid S_t)}{\mu(A_t \mid S_t)}\frac{\pi(A_{t+1} \mid S_{t+1})}{\mu(A_{t+1} \mid S_{t+1})} \dots \frac{\pi(A_{T} \mid S_{T})}{\mu(A_{T} \mid S_{T})}G_t\]

&lt;p&gt;따라서 MC policy evaluation에서 $G_t$ 가 아닌 $G_t^{\pi/\mu}$ 로 가치함수를 업데이트해주면 됩니다.&lt;/p&gt;

\[V(S_t) \leftarrow V(S_t) + \alpha(G_t^{\pi/\mu} - V(S_t))\]

&lt;p&gt;그러나 importance sampling 같은 경우 infinite variance를 가지는 단점이 있습니다. 이러한 이유로 수렴하기가 매우 어렵습니다. 따라서 현실적으로 importance sampling을 통한 off-policy Monte-Carlo방식은 사용되지 않습니다.&lt;/p&gt;

&lt;h2&gt;Temporal-Difference Control &lt;/h2&gt;
&lt;p&gt;다음은 Temporal-Difference(TD) Control에 대해 알아보겠습니다. On-policy TD control을 Sarsa이고, off-policy TD control을 Q-Learning이라 합니다. SARSA에 대해서 먼저 알아보겠습니다.&lt;/p&gt;

&lt;h3&gt;Sarsa : On-policy TD Control&lt;/h3&gt;
&lt;p&gt;TD control도 MC control과 마찬가지로, Generalized policy iteration(GPI) 을 따릅니다. Policy evaluation만 TD update을 이용하고 그 외 다른 건 모두 MC control와 같습니다.&lt;/p&gt;

\[Q(S,A) \leftarrow Q(S,A) + \alpha(R+\gamma Q(S',A') - Q(S,A))\]

&lt;p&gt;위 update 식에서 샘플링 단위가 (S, A, R, S’, A’)이기 때문에 Sarsa 라는 이름이 붙여졌습니다. Srasa 알고리즘 전체는 아래와 같습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img width=&quot;7500&quot; alt=&quot;sarsa&quot; src=&quot;https://user-images.githubusercontent.com/37501153/89125122-19adb500-d517-11ea-80e7-ed18b5449e7a.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 11. Sarsa&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;Sarsa도 on-policy MC에서 살펴본 것처럼 정책 발전 문제와 수렴 문제를 살펴보겠습니다. 정책 발전 문제는 on-policy MC와 동일하게 $\epsilon-greedy$ 를 사용하기 때문에 정책 발전은 일어납니다.&lt;/p&gt;

&lt;p&gt;반면에 수렴문제는 GLIE를 만족시키는 것 이외에 업데이트 스텝 크기인 $\alpha$ 에 대한 조건이 더 필요합니다. 그 이유는 MC와 다르게 TD는 스텝마다 업데이트가 일어나는 on-line 방식이기 때문에 스텝사이즈 크기에 따라 수렴이 되지 않고 발산이 될 수 있습니다. 스텝크기 $\alpha$ 는 Q 가치함수가 변화가 일어나야 하므로 충분히 크며 동시에 Q 가치함수가 수렴해야 하므로 충분히 작아야 합니다.&lt;/p&gt;

\[\sum_{t=1}^{\infty}\alpha_t=\infty\]

\[\sum_{t=1}^{\infty}\alpha^2_t&amp;lt;\infty\]

&lt;p&gt;그러나 실제 문제를 풀 때 $\alpha$ 를 결정하는 건 위의 이론을 이용하진 않고 domain-specific하게 또는 실험적으로 정한다고 합니다.&lt;/p&gt;

&lt;p&gt;sarsa 문제 예를 보겠습니다. 아래는 S에서 시작해서 G로 가야하는 문제입니다. 행동은 위, 아래, 좌, 우이며, 화살표가 있는 곳에서 아래에서 위로 바람이 불고 있습니다. 따라서 이 곳을 지날 때 실제 위로 가는 행동을 해도 실제 움직임은 대각선 우상향으로 가게 됩니다. 매 스텝마다 보상은 -1이며 discount factor 1입니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;490&quot; alt=&quot;sarsa-example&quot; src=&quot;https://user-images.githubusercontent.com/37501153/89125501-ed476800-d519-11ea-89ef-24c2ee66c8dc.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 12. Sarsa on the Windy Gridworld&lt;/figcaption&gt;&lt;/p&gt;
&lt;p&gt;&amp;lt;그림 12.&amp;gt; 그래프는 Sarsa 학습 결과입니다. 1에피소드가 끝날 때 까지 2000 스텝을 밟지만 그 다음부터 학습속도가 빨라지는 것을 볼 수 있습니다.&lt;/p&gt;

&lt;h3&gt;Q-Learning : Off-policy TD Control&lt;/h3&gt;
&lt;p&gt;다음은 off-policy TD 방식인 Q-Learning에 대해서 알아봅시다. Off-policy MC와 다르게 importance sampling이 필요 없습니다. Sarsa 에서 $Q(S_t, A_t)$ 를 업데이트 하기 위해, 정책 $\pi$ 에 따라 $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$ 을 샘플링 한 후, $Q(S_{t+1}, A_{t+1})$ 기반으로 현재 상태 $(S_t, A_t)$ 를 수정했습니다. 즉, 샘플링된 정책과 학습하는 정책이 일치합니다. 그러나 Q-Learning은 off-policy로 샘플링되는 정책(behavior policy)과 학습하는 정책(target policy)이 다릅니다.&lt;/p&gt;

\[Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha(R_{t+1}+\gamma Q(S_{t+1},A')-Q(S_t, A_t))\]

&lt;ul&gt;
  &lt;li&gt;Next action is chosen using behavior policy $A_{t+1} \sim \mu(\cdot \mid S_t)$&lt;/li&gt;
  &lt;li&gt;But we consider alternative successor action $A’ \sim \pi(\cdot \mid S_t)$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;다음 상태에 대한 행동을 behavior policy에 따라 선택하지만 실제 $Q(S_t, A_t)$ 를 업데이트하기 위해서 다음 상태에 대한 행동은 behavior policy와 다른 target policy에 의해 선택합니다. 즉, $A’$ 에 대해서 $Q(S_t, A_t)$ 를 업데이트하고 다음 업데이트 할 (s,a) 쌍은 $(S_{t+1}, A’)$ 이 아니라 behavior policy 따른 $(S_{t+1}, A_{t+1})$ 인 것입니다. 이 때, $A’=A$ 일수도 있고, $A’ \ne A$ 일수도 있습니다.&lt;/p&gt;

&lt;p&gt;Q-learning은 taret policy와 behavior policy를 같이 발전시켜 나갑니다. 이때, target policy $\mu$ 는 $Q(s,a)$ 에 관한 greedy policy이고 behavior policy $\pi$ 는 $Q(s,a)$ 에 관한 $\epsilon-greedy$ 입니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Target policy : $\pi(S_{t+1}) = arg \underset {a’} maxQ(S_{t+1},a’)$&lt;/li&gt;
  &lt;li&gt;Behavior policy : &lt;br /&gt;
\(\mu(a \mid s) = 
  \begin{cases}
      \frac{\epsilon}{m} + 1-\epsilon &amp;amp; \quad \text{if} \quad a^{*}=arg \underset m maxQ(s,a) \\
      \frac{\epsilon}{m} &amp;amp; \quad \text{otherwise}
      \end{cases}\)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;위의 behavior policy와 target policy를 가지고 Q-Learning 식을 다시 쓰면 아래와 같습니다.&lt;/p&gt;

\[\begin{align*} Q(S_t,A_t) &amp;amp;\leftarrow Q(S_t, A_t) + \gamma Q(S_{t+1},A')\\&amp;amp;\leftarrow Q(S_t, A_t) + \gamma Q(S_{t+1},arg \underset {a'} maxQ(S_{t+1},a')\\&amp;amp;\leftarrow Q(S_t, A_t) + \gamma m \underset {a'} ax Q(S_{t+1}, a') \end{align*}\]

&lt;p&gt;따라서, Q-learning 알고리즘은 아래와 같습니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img width=&quot;500&quot; alt=&quot;qlearning-alg&quot; src=&quot;https://user-images.githubusercontent.com/37501153/89126483-ecfe9b00-d520-11ea-8264-347a78f9b7d5.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 13. Q-Learning&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Sarsa vs. Q-Learning&lt;/b&gt;&lt;br /&gt;
그렇다면 Sarsa와 Q-Learning은 실제 학습 시 어떤 차이를 보일까요 ? 아래 cliff Waling 예제를 통해 확인해 보겠습니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img width=&quot;500&quot; alt=&quot;q-vs-sarsa&quot; src=&quot;https://user-images.githubusercontent.com/37501153/89126761-410a7f00-d523-11ea-8613-7dac00ed99d1.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 14. Cliff-walking 예&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;S에서 시작해서 G로 가는 문제입니다. The Cliff에 도달하면 R=-100을 받고 다시 S로 돌아갑니다. 다른 곳에 밟으면 R=-1을 받습니다. 행동은 위, 아래, 좌, 우이며, $\epsilon=0.1$ 입니다. 학습이 완료됐을 때 최적 정책 결과는 &amp;lt;그림 14.&amp;gt;에서 위에 있는 그림입니다. Sarsa는 safe path로 학습되지만 Q-learning은 optimal path로 학습됩니다.&lt;/p&gt;

&lt;p&gt;Sarsa는 학습되는 방향이 현재 따르는 정책에서 선택된 행동이 고려되기 때문에 path의 길이는 길지만 좀 더 안전한 길을 선택하게 됩니다. 반면에, Q-Learning 같은 경우 학습되는 방향이 현재 따르는 정책과 무관하기 때문에 현재 상태에서 가장 최적의 선택이 될 수 있는 길로 학습이 됩니다. 그렇기 때문에 Sarsa 같은 경우 보상의 합이 Q-Learning보다 크게 나타납니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;이상으로 이번 포스팅을 마치겠습니다. 읽어주셔서 감사합니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://untitledtblog.tistory.com/135&quot;&gt;Importance Sampling, https://untitledtblog.tistory.com/135&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

</content>
 </entry>
 
 <entry>
   <title>Model-Free Policy Evaluation, Monte Carlo와 Temporal Difference에 대하여</title>
   <link href="http://localhost:4000/reinforcement%20learning/2020/07/28/mc-td-eval/"/>
   <updated>2020-07-28T00:00:00+09:00</updated>
   <id>http://localhost:4000/reinforcement%20learning/2020/07/28/mc-td-eval</id>
   <content type="html">&lt;p&gt;이번 포스팅과 다음 포스팅은 유한개의 상태, 유한개의 행동에 대해 환경 모델을 모를 때, sequential decision process를 푸는 방법에 다룹니다. 지난 &lt;a href=&quot;https://ralasun.github.io/reinforcement%20learning/2020/07/13/dp/&quot;&gt;DP 포스팅&lt;/a&gt;에서, policy iteration을 이용하여 MDP를 풀었습니다. 마찬가지로 환경 모델을 알지 못한 경우도 유사하게 접근할 수 있습니다. policy iteration은 policy evaluation과 policy control로 나뉘는데 이번 포스팅은 policy evaluation을 푸는 방법에 다룰 것이고 다음 포스팅은 policy control에 대해 다루도록 하겠습니다. CS234 3강, Deep Mind의 David Silver 강화학습 강의 4강, Richard S. Sutton 교재 Reinforcement Learning: An Introduction의 Chapter 5, 6 기반으로 작성하였습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;policy evaluation은 현 정책이 얼마나 좋은지 평가하는 것으로, 현 정책 아래 가치함수를 구하는 것입니다. DP는 환경 모델을 알 때, 벨만 기대 방정식을 이용하여 iterative한 방법으로 현 정책 아래에서 가치함수를 구하는 과정입니다. 현 상태에서 특정 행동을 취할 때, 나올 수 있는 다음 상태와 받을 보상을 알고 있기 때문에 아래 식과 같이 expectation을 직접 “계산”을 할 수 있었습니다.&lt;/p&gt;

\[v_\pi(s) = \sum_a\pi(a|s)\sum_{s',r}p(s',r|s,a)[r+\gamma v_\pi(s')]\]

&lt;p&gt;(이러한 이유로, DP는 learning이 아니라 planning이라 했었습니다.) 그러나 환경 모델을 알지 못할 때, 즉, 상태 변환 모델과 보상 모델을 알지 못할 때 어떻게 정책을 평가할 수 있을까요? 바로 경험(experience)을 직접하는 것입니다. 시퀀스를 직접 밟아 나가면서 가치함수를 학습해 나가는 것입니다. 이 때, 경험을 통해 가치함수를 학습하는 방법을 두가지가 있습니다. Monte Carlo 방식와 Temporal Difference 방식입니다. 먼저, Monte Carlo policy evaluation 부터 알아보겠습니다.&lt;/p&gt;

&lt;h2&gt;Monte-Carlo Policy Evaluation&lt;/h2&gt;
&lt;p&gt;Monte-Carlo Policy Evaluation을 살펴보기 전에, Monte-Carlo 방식을 우선 알아보겠습니다.&lt;/p&gt;

&lt;h3&gt;Monte-Carlo Methods&lt;/h3&gt;
&lt;p&gt;Monte-Carlo 방법은 무작위 샘플링을 통해 우리가 알아보고자 하는 시스템의 분포를 추정하는 것입니다. 아래 그림은 Monte-Carlo 방식으로 원의 넓이를 추정하는 것입니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img alt=&quot;mc-pi&quot; width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/88665561-c2c06e00-d119-11ea-82cd-672553bac47d.gif&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 1. Monte-Carlo 예&lt;/figcaption&gt;&lt;/p&gt;
&lt;p&gt;원의 넓이를 구하는 방법이 너무나 복잡하거나 알 수 없다고 가정해 봅시다. 이런 상황에서 원의 넓이를 가장 쉽게 구하는 방법은 점을 무수히 많이 뿌려본 뒤, 사각형의 넓이 $\times$  (원 안에 들어온 점의 갯수 / 전체 점의 갯수) 로 원의 넓이를 추정할 수 있습니다. 이처럼 Monte-Carlo방식은 점을 무수히 많이 찍는 것처럼 무작위 샘플링으로 데이터를 많이 수집하게 되면 우리가 알고자 하는 시스템의 분포를 추정할 수 있다는 개념입니다.&lt;/p&gt;

&lt;h3&gt;Monte-Carlo Policy Evaluation&lt;/h3&gt;
&lt;p&gt;따라서, Monte-Carlo Policy Evaluation이란 에이전트가 환경과 직접 상호작용하여 상태, 행동, 보상으로 이뤄진 시퀀스를 무수히 많이 &lt;span style=&quot;color:red&quot;&gt;sampling&lt;/span&gt;하여 경험을 얻고, 그 경험을 바탕으로 가치함수를 구하는 것입니다. 즉, 무작위 샘플링을 통해 환경 모델을 내재적으로 추정하는 것이죠. ‘내재적’이라 표현한 이유는 추정하고자 하는 것이 상태 변이 확률 또는 보상 확률이 아니라 value function이기 때문입니다. 결국 value function를 구하기 위해선 환경 정보를 알고 있어야 구할 수 있는데(DP에서의 Policy Evaluation 참고) 샘플링을 통해 이를 구하는 것이 내재적으로 환경모델을 추정하는 거라고 생각할 수 있는 것이죠.&lt;/p&gt;

&lt;p&gt;다시 정리하면 Monte-Carlo 방식은 환경에 대한 정보없이, 오로지 ‘경험(experience)’를 통해 학습하는 것입니다.&lt;/p&gt;
&lt;blockquote&gt;사실 학습은 policy evaluation과 policy control의 상호작용으로 이뤄집니다. 본 포스팅은 학습이라 표현하지만, policy evaluation에 초점을 맞춰 작성하였습니다.&lt;/blockquote&gt;
&lt;p&gt;value function의 정의를 다시 살펴보면,&lt;/p&gt;

\[v_\pi(s) = E_\pi[G_t|s_t=s]\]

&lt;p&gt;상태 s에서의 return $G_t$ 에 대한 기댓값입니다. Monte-Carlo 방식으로 value function을 구하면 샘플링한 많은 경험들 중에서 상태 s에서부터의 return $G_t$ 를 직접 구한 뒤, 경험의 갯수만큼 나눠주면 됩니다. 즉, return에 대한 기댓값이 아니라 &lt;span style=&quot;color:red&quot;&gt;return에 관한 평균값&lt;/span&gt;입니다.&lt;/p&gt;
&lt;blockquote&gt;Monte-Carlo policy evaluation uses empirical mean return instead of expected return. In other words, Policy is evaluated based on averaging sample returns.&lt;/blockquote&gt;
&lt;p&gt;아래 예시를 통해, 어떻게 계산하는지 알아봅시다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/88672014-eedfed00-d121-11ea-92ba-d14ed0f22508.jpeg&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 2. Monte-Carlo Policy Evaluation 예&lt;/figcaption&gt;&lt;/p&gt;
&lt;p&gt;&amp;lt;그림 2&amp;gt;와 같이, C1에서 시작한 시퀀스들에 대한 return 값들이 있습니다. $s_t = C1$ 의 value function을 Monte-Carlo 방식으로 추정하면 (-2.25-3.125-3.41-3.20)/4 = -3.0 이 됩니다.&lt;/p&gt;

&lt;p&gt;그러나 Monte-Carlo 방식으로 가치함수를 추정하려면 샘플 시퀀스인 episode가 끝나야 합니다. &lt;span style=&quot;color:red&quot;&gt;즉, 모든 에피소드가 끝나야 Monte-Carlo 방식을 적용할 수 있습니다.&lt;/span&gt; 따라서 에피소드가 끝날 때 까지 기다린 후, 평균값을 업데이트하는 방식으로 적용할 수 있습니다.&lt;/p&gt;

&lt;h4&gt;first-visit MC vs. every-visit MC&lt;/h4&gt;
&lt;p&gt;Monte-Carlo 방식은 두 가지가 있습니다. first-visit MC와 every-visit MC입니다. 한 에피소드에서 같은 상태를 여러번 반복해서 지나갈 수 있습니다. 이 때, 첫번째 상태에 대한 return값만 value function 업데이트에 이용하고, 나머지는 무시하는 방법이 first-visit MC이고 모든 경우를 고려한 것이 every-visit MC입니다. 아래 그림은 first-visit MC policy evaluation과 every-visit MC policy evaluation 순서입니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;400&quot; src=&quot;https://user-images.githubusercontent.com/37501153/88681428-86e2d400-d12c-11ea-92b5-383464f1e817.jpg&quot; /&gt;
&lt;img width=&quot;400&quot; src=&quot;https://user-images.githubusercontent.com/37501153/88682203-5e0f0e80-d12d-11ea-95f4-983819379c31.jpg&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 3. first-visit/every-visit MC Policy Evaluation&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;일반적으로 first-visit MC를 많이 씁니다. 그렇다면 first-visit과 every-visit은 어떤 차이가 있을까요? first-visit MC 같은 경우, 각 상태에 대한 return들은 모두 독립입니다. 왜냐하면 샘플링된 episode가 독립이므로 first-visit만 고려하기 때문에, 각 상태에 대한 return $G_t$ 은 서로 관련이 없고 독립입니다. 즉, 상태 s에 대한 return $G_t$ 는 $v_\pi(s)$ 분포에서, i.i.d성질을 지니게 됩니다(independent and identically distributed). 따라서, 대수 법칙(law of large numbers)에 따라, 상태 s에 대한 return 값을 무수히 많이 샘플링 하게 된다면, return에 대한 평균은 우리가 구하고 싶은 상태 s의 value function 기댓값인 $\mathbb{E_\pi}[G_t \mid s_t=s]$ 에 수렴합니다.&lt;/p&gt;

&lt;p&gt;쉽게 다시 설명하겠습니다. 어떤 상태 s에 대한 가치를 구할 때마다 항상 다르게 나올 수 있습니다. 그런데 충분히 많이 상태 s를 밟는다면 대표적으로 많이 나오는 값이나 그 값 주변 값이 자주 등장하겠지요. 즉, 우리는 $v_\pi(s)$ 가 분포를 이룬다고 생각할 수 있습니다. 그런데 우리가 구하고 싶은 건 분포안에서 $v_\pi(s)$ 를 대표하는 값을 찾고 싶은 것입니다. 즉, 자주 등장하는 값을 말입니다. 따라서, 그 분포의 평균인 기댓값 $\mathbb{E_\pi}[G_t \mid s_t=s]$ 을 말입니다. first-visit MC 방식으로 샘플링한 $G_t$ 는 i.i.d성질을 띄기 때문에, 분포를 정확히 모르지만(분포를 안다면 굳이 샘플링 하지 않고 바로 기댓값이 계산이 가능하겠죠?) 결국 $v_\pi(s)$ 분포를 추정할 수 있고, 이는 우리가 구하고 싶은 기댓값에 수렴할 수 있음을 의미합니다.&lt;/p&gt;

&lt;p&gt;따라서, &lt;span style=&quot;color:red&quot;&gt;first-visit MC 방식에 의한 추정은 unbiased한 성질을 지닙니다.&lt;/span&gt; 반면에, &lt;span style=&quot;color:red&quot;&gt;every-visit MC 방식에 의한 추정은 biased한 성질을 띕니다.&lt;/span&gt; 한 에피소드 내에서 같은 상태를 여러 번 반복해서 지나갔다면, 그 상태들 간은 독립적이지 않고, 상관관계를 가지게 됩니다. 따라서, i.i.d하지 않기 때문에 biased합니다. 그렇기 때문에 MC 방식에 의한 policy evaluation은 first-visit MC를 선호하는 편이라 합니다(sutton and barto교재 및 stanford강의 참고).&lt;/p&gt;

&lt;h4&gt;Incremental Monte-Carlo Updates&lt;/h4&gt;
&lt;p&gt;Value function을 업데이트하는 방식을 에피소드가 끝날 때마다 마치 온라인 방식처럼 순차적으로 업데이트할 수 있습니다. 시퀀스 $x_1, x_2, \dots$ 에 대한 평균 $\mu_1, \mu_2, \dots$ 가 있을 때,&lt;/p&gt;

\[\begin{align*} \mu_k&amp;amp;=\frac{1}{k}\sum_{j=1}^{k}x_j\\&amp;amp;=\frac{1}{k}\left(x_k+\sum_{j=1}^{k-1}x_j\right)\\&amp;amp;=\frac{1}{k}\left(x_k + (k-1)\mu_{k-1}\right)\\&amp;amp;=\mu_{k-1}+\frac{1}{k}\left(x_k-\mu_{k-1}\right)\end{align*}\]

&lt;p&gt;입니다. 따라서, episode $S_1, A_1, R_2, \dots, S_T$ 가 끝날 때마다 아래와 같이 업데이트 할 수 있습니다.&lt;/p&gt;

\[N(S_t) \leftarrow N(S_t) + 1\]

\[V(S_t) \leftarrow V(S_t) + \frac{1}{N(S_t)}\left(G_t-V(S_t)\right)\]

&lt;p&gt;두번째 식을 마치 $\left(G_t-V(S_t)\right)$ 를 새로운 데이터와 기존 평균과의 오차 즉 에러항으로 본다면, 기존 평균값을 오차의 방향으로 1/k만큼 수정해 나간다고 해석할 수 있습니다.&lt;/p&gt;

&lt;p&gt;위의 업데이트 방식은 맨 처음에 샘플한 에피소드부터, 가장 최근에 샘플한 에피소드까지 모두 중요하게 생각함을 의미합니다. 왜냐하면 동등하게 에피소드 개수만큼으로 나누고 있기 때문입니다. 하지만 시간에 따라 조금씩 변하는 문제 같은 경우(non-stationary)에 위와 같은 업데이트 방식은 적합하지 않습니다. 따라서, 새 에피소드와 기존 평균사이의 오차를 항상 일정 크기만큼 업데이트하여 시간이 지날수록 오래된 과거는 잊고 가장 최근 사건을 좀 더 기억할 수 있게끔 해줍니다.&lt;/p&gt;

\[V(S_t) \leftarrow V(S_t) + \alpha\left(G_t-V(S_t)\right)\]

&lt;h2&gt;Temporal-Difference Policy Evaluation&lt;/h2&gt;
&lt;p&gt;다음은 Temporal-Difference Policy Evaluation에 대해 알아보겠습니다. Temporal-Difference(TD) 도 Monte-Carlo(MC) 와 마찬가지로 환경 모델을 알지 못할 때(model-free), 직접 경험하여 Sequential decision process 문제를 푸는 방법입니다. Temporal-Difference 학습은 Monte-Carlo와 Dynamic Programming을 합쳐 놓은 방식입니다.  MC처럼, 환경모델을 알지 못하기 때문에 직접 &lt;span style=&quot;color:red&quot;&gt;sampling&lt;/span&gt;한 데이터를 통해 학습을 해야 합니다. DP처럼, 에피소드가 끝날 때까지 기다리지 않고 다른 가치 추정치를 가지고 현재 상태 가치를 추정합니다. 이를 &lt;span style=&quot;color:red&quot;&gt;bootstrap&lt;/span&gt;이라 합니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/88749790-01444000-d18f-11ea-8e52-bc959e3e20ef.jpeg&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 4. DP에서의 bootstrap&lt;/figcaption&gt;&lt;/p&gt;
&lt;p&gt;TD는 MC와 다르게 &lt;span style=&quot;color:red&quot;&gt;무한한 에피소드에 대해서도 적용&lt;/span&gt;할 수 있습니다. 그 이유는 MC는 업데이트를 하기 위해서 한 에피소드가 끝날 때까지 기다려야 합니다. 그래야 return $G_t$ 를 구한 뒤, 업데이트를 할 수 있기 때문입니다. &lt;b&gt;따라서, MC는 에피소드 샘플링을 통해 실제 return $G_t$ 을 향해 $V(S_t)$ 를 수정해나갑니다.&lt;/b&gt;&lt;/p&gt;

\[V(S_t) \leftarrow V(S_t) + \alpha\left(G_t-V(S_t)\right)\]

&lt;p&gt;반면에, TD는 에피소드가 끝날 때까지 기다릴 필요 없이, 다음 상태를 밟을 때까지만 기다렸다가 업데이트합니다. 그렇기 때문에 에피소드가 끝나지 않는 시퀀스에 대해서도 적용할 수 있으며, 시퀀스를 밟아나가면서 그때그때 가치함수를 수정해 나갈 수 있습니다. 이러한 특징으로 인해, TD 방법은 &lt;span style=&quot;color:red&quot;&gt;online learning이 가능&lt;/span&gt;합니다. 이 부분이 TD의 매우 큰 장점입니다.&lt;/p&gt;

\[V(S_t) \leftarrow + \alpha[R_{t+1}+\gamma V(S_{t+1})-V(S_t)]\]

&lt;p&gt;TD policy evaluation을 상세히 살펴보면, DP에서의 벨만 기대 방정식을 이용한 policy evaluation과 유사한 것을 보실 수 있습니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/88760395-e7fbbd80-d1a7-11ea-8e22-a202c0814dbc.jpeg&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 5. DP와 TD&lt;/figcaption&gt;&lt;/p&gt;
&lt;p&gt;그러나 DP는 환경모델을 알기 때문에, 다음 상태가 될 수 있는 모든 후보들을 고려하여 가중 평균을 한 추정값으로 다음 상태의 가치 추정값만을 가지로 현재 상태의 가치를 업데이트합니다. 반면에, TD는 환경에 대한 정보가 없기 때문에 다음 상태까지 직접 밟아보는 것입니다. 이것을 ‘다음 상태 s’를 직접 샘플링하였다’라고 합니다. 그러나 DP처럼 다른 상태의 추정값을 가지고 현재 상태값을 수정하고자 합니다. 이를 bootstrap이라 합니다. 즉, TD에서의 bootstrap은 아래 그림 처럼 이해할 수 있습니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/88756436-9bf84b00-d19e-11ea-9c9f-94791c8b32f6.jpg&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 6. TD policy evaluation by bootstrapping&lt;/figcaption&gt;&lt;/p&gt;
&lt;p&gt;따라서, MC는 실제 return $G_t$ 를 향해 $V(S_t)$ 를 수정해 나가지만 &lt;b&gt;TD는 estimate $G_t$ 을 향해 $V(S_t)$ 를 고쳐나가면서 시퀀스를 진행합니다.&lt;/b&gt;&lt;/p&gt;

&lt;blockquote&gt;&lt;ul&gt;
&lt;li&gt;MC Policy Evaluation :&lt;br /&gt;update value $V(S_t)$ toward actual return $G_t$&lt;/li&gt;
&lt;li&gt;TD Policy Evaluation :&lt;br /&gt; update value $V(S_t)$ toward estimated return $R_{t+1} + \gamma V(S_{t+1})-V(S_t)$&lt;/li&gt;&lt;/ul&gt;&lt;/blockquote&gt;

&lt;h3&gt;General form of update rule&lt;/h3&gt;
&lt;p&gt;위의 MC/TD 업데이트 식은 일반적으로 아래와 같은 형태를 띕니다.&lt;/p&gt;

\[NewEstimate \leftarrow OldEstimate + StepSize \left[Target - OldEstimate\right]\]

&lt;p&gt;$\left[Target - OldEstimate\right]$ 는 오차를 나타냅니다. MC와 TD 모두 Target을 향해 기존 Estimate을 업데이트합니다. 그러나, 기존 estimate을 새로운 target으로 교체하는 건 위험합니다. 왜냐하면 초기단계에서는 새로운 target이 우리가 찾는 정답이 아닐 수도 있기 때문에, target과 기존 estimate의 오차의 일부만큼만 조금씩 수정해 나갑니다. MC target은 $G_t$ 이고, TD target은 $R_{t+1}+\gamma V(S_{t+1})$ 입니다. 또한, MC error는 $G_t - V(S_t)$ 이고, TD error는 $R_{t+1}+\gamma V(S_{t+1}) - V(S_t)$ 입니다. 보통  TD error는 $\delta_t$ 로 표현합니다. 왜냐하면, 같은 에피소드 내에서, TD error는 매 t step마다 다르기 때문입니다.&lt;/p&gt;

&lt;h3&gt; Temporal Difference Policy Evaluation Algorithm &lt;/h3&gt;
&lt;p&gt;TD policy evaluation 알고리즘 순서도는 아래와 같습니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/88760737-93a50d80-d1a8-11ea-8094-57124c27e696.png&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 7. TD(0) policy evaluation algorithm&lt;/figcaption&gt;&lt;/p&gt;
&lt;p&gt;일정 정책 $\pi$ 아래, (S, A, R, S’)를 샘플링하고, 업데이트합니다. 그런 다음 (S’, A’, R, S’‘)를 샘플링하고 업데이트합니다. 이 과정을 V(S)가 수렴할 때까지 반복합니다. 여기서 한가지 의문점이 있습니다. MC방식은 unbiased estimator이기 때문에 대수의 법칙에 따라, true expected estimate에 수렴한다고 하였습니다. 과연 TD방식은 수렴할까요? 이는 &lt;a href=&quot;https://ralasun.github.io/reinforcement%20learning/2020/07/29/mc-td-control/&quot;&gt;다음 포스팅 Model-Free Control&lt;/a&gt;에서 다루도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;TD policy evaluation 예를 살펴보겠습니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/88761833-eaabe200-d1aa-11ea-8c41-5b60f2d76259.jpg&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 8. TD policy evaluation 예(1)&lt;/figcaption&gt;&lt;/p&gt;
&lt;p&gt;먼저 $(s_3, a_1, 0, s_2)$ 에 대해 $v(s_3)$ 를 업데이트하고, 그 다음 $(s_2, a_1, 0, s_2)$ 에 대해 $v(s_2)$ 를, $(s_2, a_1, 0, s_1)$ 에 대해 $v(s_2)$ 를, 마지막으로 $(s_1, a_1, 1, terminal)$ 에 대해 $v(s_1)$ 을 업데이트하면 한 에피소드에 대해 업데이트를 완료하게 됩니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/88763595-59d70580-d1ae-11ea-8d31-d892a0e42267.jpeg&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 9. TD policy evaluation 예(2)&lt;/figcaption&gt;&lt;/p&gt;

&lt;h2&gt;Monte-Carlo vs. Temporal-Difference&lt;/h2&gt;
&lt;p&gt;이제까진 MC와 TD방식으로 policy evaluation하는 것을 보았습니다. 그러면 두 방식의 특성을 비교하겠습니다.&lt;/p&gt;

&lt;h3&gt;Bias/Variance Trade-Off&lt;/h3&gt;
&lt;p&gt;MC와 TD의 특징을 bias-variance trade-off 관점에서 보겠습니다. MC는 위에서 설명한 것처럼, return $G_t$ 는 $v_\pi(S_t)$ 의 unbiased estimate 입니다. 따라서 &lt;span style=&quot;color:red&quot;&gt;MC는 low bias의 특징&lt;/span&gt;을 띕니다. 반면에, TD는 bootstrap 기반이기 때문에 TD target $R_{t+1} + \gamma V(S_{t+1})$ 은 $v_\pi(S_t)$ 의 biased estimate 입니다. 따라서 &lt;span style=&quot;color:red&quot;&gt;TD는 high bias 특징&lt;/span&gt;을 가집니다.&lt;/p&gt;

&lt;p&gt;그러나, variance관점에서 두 방식은 반대입니다. MC같은 경우, 한 에피소드가 끝날 때까지 계속 샘플링을 해야합니다. 이로인해, random성이 많이 증가하게 되죠. 반면에, TD같은 경우 업데이트를 위해 (s, a, r, s’)을 한번만 샘플링 하기 때문에 MC에 비해 random성이 작습니다. 이러한 특징으로 인해, &lt;span style=&quot;color:red&quot;&gt;MC는 high variance를, TD는 low variance&lt;/span&gt;를 갖습니다.&lt;/p&gt;
&lt;blockquote&gt;&lt;ul&gt;&lt;li&gt;Return depends on many random actions, transitions, rewards&lt;/li&gt;&lt;li&gt;TD target depends on one random action, transition, reward&lt;/li&gt;&lt;/ul&gt;&lt;/blockquote&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/88870731-15507600-d251-11ea-96b6-5d3850150ecf.jpeg&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 10. Graphical Illustration of Bias-Variance trade off&lt;/figcaption&gt;&lt;/p&gt;

&lt;h3&gt;Properties of MC and TD&lt;/h3&gt;
&lt;p&gt;위의 bias-variance trade-off 성질로 인해 MC와 TD는 아래와 같은 특성을 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;MC : high variance and zero bias&lt;/b&gt;&lt;br /&gt;
MC는 zero bias이기 때문에 초기값에 상관없이 항상 수렴하게 됩니다. 이러한 수렴을 잘하는 특징 덕분에 좋은 근사 가치 함수도 갖게 됩니다(value function approximation, 추후에 포스팅 예정). 그러나, high variance인해 항상 true expected value에 수렴함에 불구하고, 언제 수렴할지는 불분명합니다. 왜냐하면, high variance로 인해 수렴할 때까지 굉장히 많은 에피소드 샘플링이 필요하기 때문입니다. 그리고 한 에피소드가 끝날 때까지 기다려야 하는데, 에피소드의 길이가 긴 경우 더욱 적용하기 어렵습니다. 따라서 실용성 측면에서 떨어지는 단점이 있습니다.&lt;/p&gt;

&lt;p&gt;MC가 zero bias를 가질 수 있는 이유는 $G_t$ 가 i.i.d성질을 가지기 때문이라고 설명하였습니다. 이는 $V(S_t)$ 를 계산하는데 $S_t$ 의 markov property를 이용하지 않음을 뜻합니다. 따라서, Markov domain인 아닌 경우 MC를 적용하여 문제를 해결할 수 있습니다(handling non-markovian domains).&lt;/p&gt;

&lt;p&gt;&lt;b&gt;TD : low variance and high bias&lt;/b&gt;&lt;br /&gt;
반면에 TD는 low variance로 인해 수렴이 가능하다면, MC에 비해 수렴지점까지 빨리 도달할 수 있습니다(그림 11,12 참고). 하지만 초기값에 따라 수렴여부가 달라지고(sensitive to initial value), 그리고 근사 가치 함수를 찾지 못할 수도 있습니다. 하지만, on-line 학습이 가능하기 때문에, 따라서 쉽게 적용할 수 있습니다.&lt;/p&gt;

&lt;p&gt;TD는 $V(S_t)$ 를 계산하기 위해서 $V(S_{t+1})$ estimate 을 이용합니다(bootstrap). 이는 MC와는 다르게 markov property를 이용합니다. 따라서, TD는 Markovian domain에서 적용가능합니다.&lt;/p&gt;

&lt;p&gt;위에서 설명한 MC와 TD의 특성을 정리하면 아래와 같습니다.&lt;/p&gt;

&lt;table style=&quot;width:100%&quot;&gt;
  &lt;tr&gt;
    &lt;th&gt;&lt;b&gt;Monte Carlo&lt;/b&gt;&lt;/th&gt;
    &lt;th&gt;&lt;b&gt;Temporal Difference&lt;/b&gt;&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;
    &lt;ul&gt;&lt;li&gt;high variance and zero bias&lt;/li&gt;&lt;li&gt;good convergence properties, even with function approximation&lt;/li&gt;&lt;li&gt;not very sensitive to initial value&lt;/li&gt;&lt;li&gt;very simple to understand but may not be efficient due to applying only to episodic task&lt;/li&gt;&lt;li&gt;can apply both to markov domain and non-markov domain&lt;/li&gt;&lt;li&gt;sample and no bootstrap&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
    &lt;td&gt;&lt;ul&gt;&lt;li&gt;low variance and high bias&lt;/li&gt;&lt;li&gt;could converge to true estimate, but it could fail with function approximation&lt;/li&gt;&lt;li&gt;more sensitive to initial value&lt;/li&gt;&lt;li&gt;usually more efficient than MC&lt;/li&gt;&lt;li&gt;can apply to markov domain&lt;/li&gt;&lt;li&gt;sample and bootstrap&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;위 표에서 마지막 특성에 관하여 MC, TD와 DP 사이의 관계를 back-up diagram과 함께 잘 나타낸 그림이 있습니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/89119964-23222780-d4ed-11ea-97b9-335a7d66c5d4.jpg&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 11. TD, MC and DP&lt;/figcaption&gt;&lt;/p&gt;
&lt;p&gt;DP같은 경우, 환경 모델을 잘 알기 때문에 다음 스텝에 대해 full backup을 그린 관계와 같습니다. 그러나 에피소드가 끝날 때까지 backup을 그릴 필요가 없기 때문에 shallow-backup이고, bootstrap을 이용합니다. 반면에, TD같은 경우, 다음 상태에 대해 한 상태에 대해서만 샘플링을 하기 때문에 sample-backup이며, DP와 마찬가지로 한 스텝만 내다보기 때문에 shallow-backup이며 bootstrap을 이용합니다. 마지막으로 MC같은 경우, 모든 에피소드가 끝날 때까지 기다려야 하기 때문에 deep-backup이고, 샘플링을 하여 경험을 쌓기 때문에 sample-backup입니다. 그러나 bootstrap을 이용하지 않습니다.&lt;/p&gt;

&lt;p&gt;TD가 MC보다 수렴이 더 빠른 것에 대해 수학적으로 증명된 적은 없습니다. 그러나, 실험적으로 확인했을 때 TD가 MC보다 수렴이 빠릅니다. 이에 관해 Sutton과 berto교재에 MC와 TD의 수렴에 관한 예제가 있습니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/88873551-d671ee80-d257-11ea-9f09-c62328dee7f3.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 12. Random Walk 예(1)&lt;/figcaption&gt;&lt;/p&gt;
&lt;p&gt;위의 예는 C에서 시작하여 각 스텝마다 왼쪽 또는 오른쪽으로 0.5의 확률로 동일하게 갈 수 있다고 할 때, 양 끝의 사각형에 도달하면 에피소드가 끝나는 문제입니다. 각 상태 A, B, C, D, E에서 value를 MC와 TD를 이용하여 구한 결과는 아래와 같습니다. 이때, 각 상태에서의 true value는 1/6, 2/3, 3/6, 4/6, 5/6입니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/88898226-624f3f00-d287-11ea-97fc-0029133d082c.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 13. Random Walk 예(2)&lt;/figcaption&gt;&lt;/p&gt;
&lt;p&gt;왼쪽 그림은 TD(0)를 여러 에피소드를 거쳤을 때, 각 상태에서의 value입니다. 100 에피소드 정도 진행했을 때, true value에 수렴하는 것을 확인할 수 있습니다. 오른쪽 그림은 step size $\alpha$ 를 달리했을 때 각각 MC와 TD에서의 RMS error 입니다. 실험적으로 TD가 MC보다 더 빨리 수렴합니다.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;differences between MC and TD trough Batch Update&lt;/b&gt;&lt;br /&gt;
MC와 TD의 작동원리를 보여주는 사례를 하나 더 소개하겠습니다. 먼저 그전에 batch update에 대해 설명하도록 하겠습니다. k개의 에피소드 또는 k개의 스텝을 미리 샘플링 해 놓은 뒤, k개의 MC 또는 TD 방식의 error를 각각 구해서 다 합한 후, 한 번 update를 하는 방식을 batch update이라 합니다. 아래 예를 각각 MC와 TD 방식의 batch update로 풀어보겠습니다.&lt;/p&gt;

&lt;p&gt;아래와 같이 8개의 에피소드가 있습니다. 첫번째 에피소드는 A에서 시작해서 reward를 0을 받고, 그 다음 B로 가고 reward를 0을 받고 끝납니다. 그 다음 여섯개 에피소드는 B에서 시작해서 reward를 1을 받고 끝납니다. 마지막 하나는 B에서 시작해서 reward를 1을 받고 끝납니다. 이때, MC와 TD방식으로 V(A), V(B)가 각각 어떻게 될까요 ?&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/88902645-75fda400-d28d-11ea-952d-ced88051631e.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 14. Batch update 예시&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;MC방식으로 한다면, $V(A)=0, V(B)=\frac{3}{4}$ 입니다. 그러나 TD방식으로 한다면 $V(B)=0$ 이지만 $V(A)=\frac{3}{4}$ 입니다. $V(A)$ 에서 차이가 나는 이유는 MC는 mean squared error를 최소화하는 방식으로 해답을 구하지만, TD는 markov model의 likelihood를 최대화하는 방식으로 해답을 구하기 때문입니다.&lt;/p&gt;

\[\sum_{k=1}^{K}\sum_{t=1}^{T_k}\left(G_t^k - V(s_t^k)\right)^2\]

&lt;ul&gt;&lt;li&gt;MC converges to solution with minimum mean-squared error&lt;/li&gt;&lt;li&gt;Best fit to the observed returns&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;MC는 관측된 return이 true value estimate과의 차이가 최소화시키는 방향으로 갑니다. 반면에, TD는 markov property 성질을 이용하기 때문에 MDP를 해결하는 방향으로 갑니다. 실제 구현은 아니지만 내재적으로는 마치 환경모델의 transition model과 reward model의 maximum likelihood를 구한 뒤 DP를 푸는 방식과 유사하게 작동하는 것입니다. 실제로 위의 예제를 아래 방식으로 환경 모델을 구한 뒤 DP로 접근하면 똑같은 해답을 구할 수 있습니다.&lt;/p&gt;

\[\hat P^a_{s,s'} = \frac{1}{N(s,a)}\sum_{k=1}{K}\sum_{t=1}{T_k}\mathbf 1(s_t^k, a_t^k, s_{t+1}^k = s, a, s')\]

\[\hat R^a_s = \frac{1}{N(s,a)}\sum_{k=1}{K}\sum_{t=1}{T_k}\mathbf 1(s_t^k, a_t^k=s,a)r^k_t\]

&lt;ul&gt;&lt;li&gt;TD(0) converges to solution of max likelihood Markov model&lt;/li&gt;&lt;li&gt;Solution to the MDP $&amp;lt;S, A, \hat P, \hat R, \gamma &amp;gt;$ that best fits the data&lt;/li&gt;&lt;/ul&gt;

&lt;hr /&gt;
&lt;p&gt;이상으로 이번 포스팅을 마치겠습니다. 다음 포스팅은 &lt;a href=&quot;https://ralasun.github.io/reinforcement%20learning/2020/07/29/mc-td-control/&quot;&gt;Model-Free Control&lt;/a&gt;에 대해 진행하겠습니다.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Dynamic Programming, Policy Iteration부터 Value Iteration까지</title>
   <link href="http://localhost:4000/reinforcement%20learning/2020/07/13/dp/"/>
   <updated>2020-07-13T00:00:00+09:00</updated>
   <id>http://localhost:4000/reinforcement%20learning/2020/07/13/dp</id>
   <content type="html">&lt;p&gt;지난 &lt;a href=&quot;https://ralasun.github.io/reinforcement%20learning/2020/07/12/mdp/&quot;&gt;MDP 포스팅&lt;/a&gt;에 이어서, 이번 포스팅은 MDP를 iterative하게 푸는 방법 중 하나인 Dynamic Programming(DP)에 대해서 다룹니다. CS234 2강, Deep Mind의 David Silver 강화학습 강의 3강, Richard S. Sutton 교재 Reinforcement Learning: An Introduction의 Chapter 4 기반으로 작성하였습니다. 또한, 대부분 수식 표기법은 Sutton 교재를 따랐습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;일반적으로 Dynamic Programming란 복잡한 문제를 간단한 여러 개의 문제로 나누어 푸는 방법을 말합니다. 지난 시간에서 벨만 방정식(벨만 기대 방정식, 벨만 최적 방정식)은 recursive한 관계를 가지고 있기 때문에, 벨만 방정식을 풀기 위한 솔루션으로 DP 사용이 적합하다고 할 수 있습니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/87278417-5e998980-c51f-11ea-9be2-b1b55ad4c070.jpeg&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 1. Dynamic programming 조건&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;따라서, 상태 $s \in S$, 행동 $a \in A$, 보상 $r \in R$ 인 환경 모델 $p(s’,s|r,a)$ 을 아는 상황에서, 벨만 기대 방정식과 벨만 최적 방정식의 recursive한 성질을 이용하여 최적 가치 함수 $v_\ast, q_\ast$ 를 구하는 것이 Dynamic Programming을 이용한 MDP 를 푸는 것입니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;700&quot; src=&quot;https://user-images.githubusercontent.com/37501153/87279033-08c5e100-c521-11ea-8ccc-de84ed44d45f.jpeg&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 2. 벨만 방정식 : Recursive 관계&lt;/figcaption&gt;&lt;/p&gt;
&lt;blockquote&gt;MDP문제는 환경모델을 완벽하게 아는 상황이기 때문에, dynamic programming은 'reinforcement learning'이 아니라 'planning' 방법입니다.&lt;/blockquote&gt;
&lt;p&gt;DP설명은 finite MDP에 유한하여 설명하도록 하겠습니다. 일반적으로 continuous MDP문제는 DP방법이 아닌 다른 방법을 이용하여 풀기 때문입니다.&lt;/p&gt;

&lt;p&gt;지난 &lt;a href=&quot;https://ralasun.github.io/reinforcement%20learning/2020/07/11/introRL(2)&quot;&gt;강화학습 소개[2] 포스팅&lt;/a&gt;에서, sequential decision making 문제 종류로 evaluation(prediction)과 control을 소개하였습니다. evaluation은 일정 정책 아래, 기대보상을 추정하여 현재 따르는 정책의 좋고/나쁨을 평가하는 것입니다. 즉, 현재 정책의 평가가 되는 것입니다. control은 정책들의 평가를 기반으로 최적의 정책을 찾는 것입니다. evaluation과 control은 독립적인 과정이 아니라 서로 연계되어 있는 과정이라 하였습니다. 마찬가지로 Dynamic Programing도 evaluation에 해당하는 Policy Evaluation과 control에 해당하는 Policy Improvement로 구성됩니다. 각각에 대해 알아봅시다.&lt;/p&gt;
&lt;blockquote&gt;DP설명은 finite MDP에 유한하여 설명하도록 하겠습니다. 일반적으로 continuous MDP문제는 DP방법이 아닌 다른 방법을 이용하여 풀기 때문입니다.&lt;/blockquote&gt;

&lt;h2&gt;Policy Evaluation&lt;/h2&gt;
&lt;p&gt;Policy evaluation은 벨만 기대 방정식을 이용하여 iterative한 방법으로 현 정책 아래의 가치함수를 구하는 과정입니다. 아래 벨만 기대 방정식을&lt;/p&gt;

\[v_\pi(s)=\sum_a\pi(a|s)\sum_{s',r}p(s',r|s,a)[r+\gamma v_\pi(s')]\]

&lt;p&gt;update rule의 관계를 가진 방정식으로 취급한 뒤,&lt;/p&gt;

\[v_{k+1}(s)=\sum_a\pi(a|s)\sum_{s',r}p(s',r|s,a)[r+\gamma v_{k}(s')]\]

&lt;p&gt;k=0부터 수렴할 때까지 반복적으로 계산하는 것입니다. 즉 가치 함수를 초기화한 후, $v_0 \to v_1 \to v_2 \to \cdots \to v_\pi$ 으로 수렴할 때까지 &lt;span style=&quot;color:red&quot;&gt;&lt;b&gt;모든 상태에 대해서 동시에 업데이트&lt;/b&gt;&lt;/span&gt;하는 것입니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt; 
&lt;img width=&quot;500&quot; src=&quot;https://i.imgur.com/WzCwUj1.jpg&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 3. Iterative Policy Evaluation&lt;/figcaption&gt;&lt;/p&gt;
&lt;p&gt;이를 back-up diagram으로 다시 표현해 봅시다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt; 
&lt;img width=&quot;500&quot; src=&quot;https://imgur.com/OKUnBIF.jpg&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 4. Back-up diagram for iterative policy evaluation&lt;/figcaption&gt;&lt;/p&gt;
&lt;p&gt;그림 4.를 보시면, k 스텝에서 다음 상태를 이용하여 k+1 스텝의 현재 상태를 업데이트합니다. 또한 업데이트되는 방식은 다음 상태에서 나올 수 있는 누적보상의 가중 평균으로 계산됩니다(벨만 기대 방정식이기 때문입니다).&lt;/p&gt;

&lt;blockquote&gt;To produce each successive approximation, $v_{k+1}$ from $v_{k}$, iterative policy evaluation applies the same operation to each state s: it replaces the old value of s with a new value obtained form the old values of the successor states of s and the expected immediate rewards, along all the one-step transitions possible under the policy being evaluated. - Sutton and Barto, Reinforcement Learning : An Introduction&lt;/blockquote&gt;

&lt;p&gt;아래 그리드월드 예제로, policy evaluation을 살펴봅시다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;500&quot; src=&quot;https://imgur.com/XQHYowT.jpg&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 5. 그리드월드 예제&lt;/figcaption&gt;&lt;/p&gt;
&lt;p&gt;처음 시작 상태에서 여러 경로를 다니다가 회색색깔에 도착하면 끝나는 게임이 있다고 합시다. 각 상태마다 받는 보상은 -1이고, 행동 좌,우,위,아래 방향에 대해 갈 확률은 0.25라 한다면, k=0, k=1, k= $\infty$ 을 수렴할 때까지 반복하면 각 $v_k$ 에 대해 각 상태의 가치함수 값은 아래 그림과 같습니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;400&quot; src=&quot;https://i.imgur.com/usVVNHF.jpg&quot; /&gt;
&lt;img width=&quot;400&quot; src=&quot;https://imgur.com/44Y2N5r.jpg&quot; /&gt; 
&lt;figcaption align=&quot;center&quot;&gt;그림 6. 그리드월드 예제 - policy evaluation&lt;/figcaption&gt;&lt;/p&gt;
&lt;p&gt;왼쪽 행은 가치함수 결과이고, 오른쪽 행은 각 가치함수에서 greedy한 전략을 보여줍니다. 그러나 위 예제같은 경우는 간단한 케이스이어서 빨리 수렴에 도달합니다. 일반적으로 상태 집합의 크기 $|S|$ 가 큰 경우, 수렴할 때까지의 속도가 매우 느릴 수도 있기 때문에, 아래 알고리즘과 같이 어느 정도 수렴조건을 만족하면 다음 스텝으로 넘어가는 방법을 주로 택합니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;500&quot; src=&quot;https://imgur.com/l5QDFUd.jpg&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 6. Policy evaluation 알고리즘&lt;/figcaption&gt;&lt;/p&gt;

&lt;h2&gt;Policy Improvement&lt;/h2&gt;
&lt;p&gt;결국 현재 정책을 평가하는 이유는 더 나은 정책을 찾기 위한 것입니다. 그렇다면 현재 정책 평가한 것을 기반으로 어떻게 더 나은 정책을 찾는지 알아보도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;임의의 정책 $\pi$ 아래 policy evaluation을 통해 $v_\pi$ 를 구했다고 하겠습니다. 그림 6.에서 처럼 수렴된 $v_\pi$ 에 대한 greedy policy가 있을 것입니다. 하지만 그 greedy policy 이외의 다른 행동 $a$ 을 선택하고, 즉, $a \neg \pi(s)$ 하고, 기존 정책 $\pi$ 를 따른다고 했을 때,&lt;/p&gt;

\[q_\pi(s,a) = \sum_{s',r}p(s',r|s,a)[r+\gamma v_\pi(s')]\]

&lt;p&gt;기존 정책에 따른 $v_\pi(s)$ 보다 크다면 새로 선택된 행동 a가 발전된 전략일 것입니다(policy improvement).&lt;/p&gt;

\[q_\pi(s, \pi'(s)) \geq v_\pi(s)\]

&lt;p&gt;&lt;span style=&quot;color:gray&quot;&gt;처음에, 이 부분을 혼자 공부할 때, 이해하기 어려웠던 부분이 ‘greedy policy 이외의 다른 행동 $a$ 를 선택하고 기존 정책을 따른다는 부분’이었습니다. 저는 이 부분은 아래와 같이 이해하였습니다.&lt;/span&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;https://imgur.com/n7Z1Gy4.jpg&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 7. &lt;/figcaption&gt;&lt;/p&gt;
&lt;p&gt;그러나, 지난 &lt;a href=&quot;https://ralasun.github.io/reinforcement%20learning/2020/07/12/mdp/&quot;&gt;MDP 포스팅&lt;/a&gt;에서 더 나은 정책이 되려면 $q_\pi(s, \pi’(s)) \geq v_\pi(s)$ 가 아닌 $v_{\pi}(s) \geq v_{\pi’}(s)$ 를 만족해야 합니다. 이를 유도하는 수학적 증명은 아래와 같습니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;https://imgur.com/OLBeIIc.jpg&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 8.&lt;/figcaption&gt;&lt;/p&gt;
&lt;p&gt;따라서, greedy하게 policy improvement하는 방식을 수식으로 깔끔하게 정리하면&lt;/p&gt;

\[\begin{align*} \pi'{\left(s\right)} =&amp;amp;{arg \underset a max}{q_\pi(s,a)}\\=&amp;amp;{arg \underset amax}{\mathbb E[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s,A_t=a]}\\=&amp;amp;{arg \underset amax}{\sum_{s',r}p(s',r|s,a)[r + \gamma v_\pi(s')]}\end{align*}\]

&lt;p&gt;입니다. 즉 기존 정책에서 발전된 새로운 정책 $\pi’$ 가 되었습니다.&lt;/p&gt;

&lt;p&gt;만약에, 새로운 정책 $\pi’$ 가 기존 정책 $\pi$ 에서 더이상의 발전이 없다면, $v_\pi=v_\pi’$ 이를 만족하기 때문에, 아래 식이 성립됩니다.&lt;/p&gt;

\[\begin{align*} v_\pi'(s) =&amp;amp;{m \underset aax}{\mathbb E[R_{t+1}+\gamma v_\pi'(S_{t+1})|S_t=s, A_t=a]}\\=&amp;amp;{m \underset aax}\sum_{s',r}p(s',r|s,a)[r+\gamma v_\pi'(s')] \end{align*}\]

&lt;p&gt;위의 식을 가만보면, 어디서 많이 봤습니다. 바로 &lt;b&gt;벨만 최적 방정식&lt;/b&gt;입니다. 즉, 더이상 발전이 없을 때, $v_\pi’$ 는 최적정책임을 의미합니다.&lt;/p&gt;

&lt;h2&gt;Policy Iteration&lt;/h2&gt;
&lt;p&gt;최적 정책을 찾기 위해서 결국 evaluation과 imporvement과정을 번갈아 가면서 정책이 더 이상 발전이 없을 때까지 진행해야 합니다. 이를 policy iteration이라 합니다.&lt;/p&gt;

\[\pi_0 \overset E\to v_{\pi_0} \overset I\to \pi_1 \overset E\to v_{\pi_1} \overset I\to \pi_2 \overset E\to \cdots \overset I\to \pi_\ast \overset E\to v_\ast\]

&lt;p&gt;E는 evalution이고, I는 improvement를 뜻합니다. Finite MDP인 경우 정책 후보의 갯수도 유한하기 때문에 반드시 언젠간 수렴합니다. Policy iteration 알고리즘은 아래 그림과 같습니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/87383157-9f52da80-c5d3-11ea-84c9-28fa2ec398cf.jpeg&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 9. Policy iteration&lt;/figcaption&gt;&lt;/p&gt;
&lt;p&gt;그림 9.를 보면, policy improvement를 한 후, 다시 policy evalutation을 할 때, 이전 정책 $/pi$ 에 관한 $v_\pi$ 로 초기값으로 하여 진행합니다.&lt;/p&gt;

&lt;h2&gt;Value Iteration&lt;/h2&gt;
&lt;p&gt;최적 정책을 찾는 방법엔 policy iteration 말고 value iteration도 있습니다. Value iteration에 대해 설명하기 전에 먼저 벨만 최적 방정식에서의 optimality의 개념을 다시 한번 생각해 봅시다.&lt;/p&gt;

&lt;h3&gt;Principle of Optimality&lt;/h3&gt;
&lt;p&gt;벨만 최적 방정식을 다시 한번 살펴보면,&lt;/p&gt;

\[\begin{align*} v_\pi'(s) =&amp;amp;{m \underset aax}{\mathbb E[R_{t+1}+\gamma v_\pi'(S_{t+1})|S_t=s, A_t=a]}\\=&amp;amp;{m \underset aax}\sum_{s',r}p(s',r|s,a)[r+\gamma v_\pi'(s')] \end{align*}\]

&lt;p&gt;상태 s에서 optimal value를 가지려면, 다음 상태 s’까지 진행해봐야 상태 s의 가치가 최적인지 아닌지 판단할 수 있습니다. 아래와 같이 $s_t$ 가 terminal state인 시퀀스가 있다고 한다면,&lt;/p&gt;

\[s_0 \to s_1 \to s_2 \to \cdots \to s_t\]

&lt;p&gt;상태 $s_0$ 의 가치는 $s_1$ 에 도착해야 알고, $s_1$ 의 가치는 $s_2$ 에 도착해야 알고, …, $s_{t-1}$ 의 가치는 $s_t$ 에 도착해야 압니다. 즉 $s_t$ 의 최적가치를 알고 있어야 처음 상태 $s_0$ 의 최적가치값을 알 수 있단 얘기입니다. 아래 예를 살펴보겠습니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/87384499-e1c9e680-c5d6-11ea-946e-bb180a6ef863.jpeg&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 10. value iteration 예시&lt;/figcaption&gt;&lt;/p&gt;
&lt;p&gt;회색인 부분이 도달해야 하는 골이라 하면, 골까지 가는 가장 짧은 경로를 찾는 문제입니다. 흰색 칸을 밟을 때마다 받는 보상은 -1, 회색 칸을 밟으면 보상 0을 받는다고 할 때, 처음 상태가 정해진 것이 아니라면 당연히 회색 부분 근처 칸에서 시작하는게 최적일 것입니다. 그리고 회색 칸은 종결지점이기 때문에 회색 칸 이후로 더이상의 시퀀스가 존재하지 않아, 회색 칸의 최적가치는 즉각적인 보상인 0일 것입니다. 그렇다면 골에서 가장 멀리 있는 맨 오른쪽 칸의 최적 가치는 어떻게 구할까요? &lt;b&gt;골의 최적가치가 골에서 가까운 위치부터 퍼져나가 맨 오른쪽 칸의 최적 가치를 계산할 수 있도록 도달해야 합니다.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;그러나, DP에서 모든 상태에 대한 업데이트를 동시에 진행하기 때문에, 골의 최적가치가 퍼져나가 다른 상태의 최적가치를 구할 수 있을 때까지 &lt;b&gt;여러 번 반복 진행&lt;/b&gt;해야 합니다. 이것이 바로 “Value Iteration”입니다.&lt;br /&gt;
&lt;br /&gt;
Value iteration을 수식으로 표현하면 아래와 같습니다.&lt;/p&gt;

\[\begin{align*} v_{k+1}&amp;amp;=m \underset a ax{\mathbb E[R_{t+1} = \gamma v_k(S_{t+1})|S_t=s, A_t=a]}\\&amp;amp;=m \underset a ax{\sum_{s',r}p(s',r|s,a)[r+\gamma v_k(s')]} \end{align*}\]

&lt;p&gt;위 수식을 보시면, 벨만 최적 방정식과 유사합니다. 즉, value iteration은 벨만 최적 방정식을 업데이트 형식으로 바뀐 것입니다.&lt;/p&gt;

&lt;blockquote&gt;policy evalutation은 벨만 기대 방정식을 업데이트 형식으로 바꾼 것이고, value iteration은 벨만 최적 방정식을 업데이트 형식으로 바뀐 것입니다.&lt;/blockquote&gt;

&lt;p&gt;value iteration은 policy iteration 처럼 명시적인 정책 발전 과정을 중간에 생략하고, 최적 가치 함수를 바로 계산하여 마지막에 정책 발전을 한번만 수행하여 최적 정책을 얻는 과정이라 생각할 수 있습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;이상으로 이번 포스팅을 마치겠습니다. 읽어주셔서 감사합니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;http://web.stanford.edu/class/cs234/slides/lecture2.pdf&quot;&gt;CS234 Winter 2019 course Lecture 2&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://incompleteideas.net/book/bookdraft2017nov5.pdf&quot;&gt;Richard S. Sutton and Andre G. Barto : Reinforcement Learning : An Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.davidsilver.uk/wp-content/uploads/2020/03/DP.pdf&quot;&gt;David Silver Lecture 3&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ko.wikipedia.org/wiki/동적_계획법&quot;&gt;위키백과, 동적계획법&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</content>
 </entry>
 
 <entry>
   <title>Markov Process에서 Markov Decision Process까지</title>
   <link href="http://localhost:4000/reinforcement%20learning/2020/07/12/mdp/"/>
   <updated>2020-07-12T00:00:00+09:00</updated>
   <id>http://localhost:4000/reinforcement%20learning/2020/07/12/mdp</id>
   <content type="html">&lt;p&gt;이전 포스팅 &lt;a href=&quot;https://ralasun.github.io/reinforcement%20learning/2020/07/11/introRL(1)&quot;&gt;강화학습 소개[1]&lt;/a&gt;, &lt;a href=&quot;https://ralasun.github.io/reinforcement%20learning/2020/07/11/introRL(2)&quot;&gt;강화학습 소개[2]&lt;/a&gt;에 이어서, MDP에 대해 다룹니다. CS234 2강, Deep Mind의 David Silver 강화학습 강의 2강, Richard S. Sutton 교재 Reinforcement Learning: An Introduction의 Chapter 3 기반으로 작성하였습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;강화학습은 sequential decision process 문제를 푸는 방법입니다. 그렇다면 sequential decision process를 풀기 위해서 수학적으로 표현해야 하는데 이것이 바로 Markov Decision Process(MDP)입니다. 또한 MDP는 에이전트 상태가 마코브 성질을 따르는 경우이기 때문에, 환경모델을 완벽하게 아는 Fully Observability를 가집니다.&lt;/p&gt;
&lt;blockquote&gt;MDPs are a mathematically idealized form of the reinforecement learning problem for which precise theoretical statements can be made. - Sutton and Barto, Reinforcement Learning: An Introduction&lt;/blockquote&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/87244726-49b6ea80-c47a-11ea-92aa-0e293341492a.jpeg&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 1. Markov Property&lt;/figcaption&gt;
&lt;/p&gt;

&lt;p&gt;지난 포스팅에서 대부분의 문제들은 그러나 Partial Observability를 가지는 POMDP라 하였습니다. 그러나, POMDP를 풀기 위해서도 MDP가 중요합니다. 그 이유는 POMDP는 MDP의 상태를 히스토리로 두고 풀 수 있기 때문입니다.&lt;/p&gt;

&lt;p&gt;MDP를 자세히 이해하기 위해서 Markov process(Markov Chain)와 Markov Reward Process를 먼저 살펴본 뒤, MDP를 살펴보도록 하겠습니다.&lt;/p&gt;

&lt;h1&gt;Markov Process&lt;/h1&gt;
&lt;p&gt;Markov Process(Markov chain)은 마코브 성질을 가지는 랜덤 상태 $S_1, S_2, \dots$ 들의 시퀀스입니다. Finite Markov Process인 경우 상태들의 집합은 유한개로 구성됩니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/87244920-efb72480-c47b-11ea-9a04-dd74b9832c99.jpeg&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 2. Markov process&lt;/figcaption&gt;
&lt;/p&gt;
&lt;p&gt;상태들간의 변환 확률 행렬(state transition matrix)은 현재 상태에서 다른 상태로 갈 확률을 모든 상태에 대해 행렬 형태로 나타낸 것입니다. 현재 상태 s에서 다음 상태 s’로 갈 확률은&lt;/p&gt;

\[P_{ss'}= \mathbf P[S_{t+1}=s'|S_t=s]\]

&lt;p&gt;입니다. 따라서, 상태 변환 확률 행렬 $\mathit P$ 는 아래와 같습니다. 각 행의 합은 1이 됩니다.&lt;/p&gt;

\[\mathit P = \left( \begin{matrix}
		\mathit P_{11} &amp;amp; \cdots &amp;amp; \mathit P_{1n}\\
		\vdots &amp;amp; \ddots &amp;amp; \vdots\\
		\mathit P_{n1} &amp;amp; \cdots &amp;amp; \mathit P_{nn}\\
		\end{matrix} \right)\]

&lt;p&gt;Markov Process 예를 들어봅시다. 아래 예는 학생들의 수업을 듣는 패턴을 Markov Process로 나타낸 것입니다. 동그라미는 학생들의 상태(facebook, class1, …)이며, 화살표는 각 상태에서 다른 상태로 넘어갈 확률을 나타냅니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/87245061-2a6d8c80-c47d-11ea-8f14-2af98ec61e0c.jpeg&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 2. Student Markov Process &lt;/figcaption&gt;&lt;/p&gt;
&lt;p&gt;그림 2.를 보면, 시작하는 상태가 같아도 밟고 지나가는 상태들의 경우가 모두 다를 수 있습니다. 예를 들어서, Class1에서 시작하여도&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;C1 C2 C3 Pass Sleep&lt;/li&gt;
&lt;li&gt;C1 fb fb C1 C2 Sleep&lt;/li&gt;
&lt;li&gt;C1 C2 C3 Pub C2 C3 Pass Sleep&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;이처럼, 실제로 이렇게 샘플된 시퀀스를 &lt;b&gt;에피소드(episode)&lt;/b&gt;라 부릅니다.&lt;/p&gt;

&lt;h1&gt;Markov Reward Process&lt;/h1&gt;
&lt;p&gt;다음으로는 Markov Reward Process(MRP)를 살펴봅시다. MRP는 Markov chain에 reward가 더해진 것입니다. 임의의 상태들의 시퀀스를 상태 변환 확률에 따라 밟아가면서 각 상태에 도착할 때마다 보상을 얼마나 받는지도 시퀀스로서 파악하는 것입니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/87245196-740aa700-c47e-11ea-9b03-e4659ba28c84.png&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 3. Markov Reward Process&lt;/figcaption&gt;&lt;/p&gt;
&lt;p&gt;$R_s$ 는 보상함수로, 상태 $S_s$ 일 때, 받을 수 있는 즉각적인 보상에 대한 기댓값이다. 여기서 중요한 점은 &lt;b&gt;앞으로 받을 보상들을 고려한 누적 보상값이 아닌 즉각적으로 받는 보상(immediate reward)&lt;/b&gt;입니다.&lt;/p&gt;

&lt;p&gt;지난 포스팅에서 환경모델은 크게 상태변이모델과 보상모델로 구성된다고 했습니다. 따라서, 상태변이확률과 보상함수를 결합하여 환경 모델을 아래와 같이 표현할 수도 있습니다. 이는 현재 상태 t-1 스텝에서, 다음 스텝에 받을 보상과 상태가 r과 s’이 될 확률입니다.&lt;/p&gt;

\[p(s',r|s) = P[S_{t+1}=s', R_{t+1}=r|S_{t}=s]\]

&lt;p&gt;아래는 학생 Markov Reward Process 예시입니다. 빨간색으로 표시된 숫자가 각 상태에서 받는 즉각적인 보상입니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;350&quot; src=&quot;https://user-images.githubusercontent.com/37501153/87245285-2e9aa980-c47f-11ea-9981-0f512f0078c0.png&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 4. Student MRP&lt;/figcaption&gt;&lt;/p&gt;

&lt;h2&gt;return and value function&lt;/h2&gt;
&lt;p&gt;MRP에서 Reward는 즉각적인 보상입니다. 그러나 우리가 궁극적으로 하고 싶은 건 매 스텝마다 받는 보상을 누적했을 때, 이 누적값이 최대화가 되도록 하는 것입니다(reward hypothesis - &lt;a href=&quot;https://ralasun.github.io/reinforcement%20learning/2020/07/11/introRL(1)&quot;&gt;지난 포스팅 참조&lt;/a&gt;.) 따라서, 누적된 보상은 어떻게 구할까요 ? 이를 위해 필요한 개념이 return과 value function입니다.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Return and Horizon&lt;/b&gt;&lt;br /&gt;
먼저, horizon에 대한 개념을 살펴봅시다. horizon은 에피소드에서의 t 스텝 갯수입니다. 유한 개일수도 무한 개일수도 있습니다. 유한개일 경우 finite MRP(또는 finite MDP)라 합니다.&lt;/p&gt;

&lt;p&gt;Return은 t 스텝에서부터 horizon까지 디스카운트된 누적 보상(discounted sum of rewards)의 합입니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/87245504-de244b80-c480-11ea-9786-3093366daf5e.png&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 5. Return&lt;/figcaption&gt;&lt;/p&gt;
&lt;p&gt;discount factor $\gamma \in [0,1]$ 은 미래 보상을 현재 가치로 환산해주는 요소입니다. 왜 현재 가치로 환산해야 할까요? 여러가지 이유가 있습니다. 먼저 수학적으로 계산 시 수렴해야 하기 때문입니다. 그렇지 않으면 반환값이나 앞으로 설명할 가치함수가 전혀 수렴되지 않기 때문이죠. 다른 이유로는 미래에 대한 불확실성 때문입니다. 금융에서 이자를 떠올리시면 됩니다. discount factor가 1에 가까울수록 미래보상을 더 중요한거고 0에 가까울수록 현재보상이 더 중요한 것입니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/87245719-9f8f9080-c482-11ea-88b0-3bc33ad3889a.jpeg&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 6. Return(2)&lt;/figcaption&gt;&lt;/p&gt;
&lt;p&gt;만약에 종결 상태가 있는 경우, 즉 horizon이 유한한 경우, $\gamma=1$ 로 둘 수 있습니다.&lt;/p&gt;

&lt;p&gt;아래 그림은 student MRP에서, 각 episode마다 return을 계산한 것입니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/87245967-e088a480-c484-11ea-9845-f576166f57e4.png&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 7. Return 예시&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Value Function&lt;/b&gt;&lt;br /&gt;
가치함수는 현재 놓여진 상태가 얼마나 좋은지를 알려주는 함수입니다. ‘얼마나 좋은지’에 대한 개념은 결국 현재 상태에서 앞으로 시퀀스를 밟아나갈 때 받을 누적보상이 얼마나 클까?와 관련됩니다. 따라서 가치 함수의 정의는 return의 기댓값입니다. MRP에서는 현재 에이전트의 행동에 관한 요소가 없기 때문에, 상태 가치 함수이지만, 추후에 MDP는 행동요소가 포함되어 있기 때문에 MDP에서의 가치함수는 상태-행동 가치 함수입니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/87245875-2133ee00-c484-11ea-8587-50bf90c43442.png&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 8. value function&lt;/figcaption&gt;&lt;/p&gt;
&lt;p&gt;상태 가치 함수는 어떻게 계산할 수 있을까요? 가장 간단하게는 에피소드를 엄청나게 많이 샘플링하는 것입니다. 그 다음 각 에피소드마다 return을 계산하고, 그 return값들을 평균내면 됩니다. 이를 simulation 이라 합니다. 그러나 마코브 성질을 이용하면 가치함수는 recursive한 형태로 변하게 됩니다. 이것이 바로 bellman equation입니다.&lt;/p&gt;

&lt;h2&gt;Bellman Equation for MRPs&lt;/h2&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/87246187-a9b38e00-c486-11ea-8150-6f1a7354d6be.jpeg&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 9. Bellman equation 유도&lt;/figcaption&gt;&lt;/p&gt;
&lt;p&gt;가치함수는 위 그림처럼 현 상태에서의 즉각적인 보상 $R_{t+1}$ 와 다음 상태에서 받을 누적 보상 $\gamma v(S_{t+1})$ 로 분해되며 recursive한 구조를 가집니다. 이는 &lt;b&gt;현재 상태 가치와 다음 상태 가치사이의 관계&lt;/b&gt;를 나타냅니다. 이 방정식이 바로 벨만 방정식(Bellman Equation)입니다.&lt;/p&gt;

&lt;p&gt;벨만 방정식은 현재 상태와 다음 상태 사이의 관계를 나타내기 때문에 아래와 같이 back-up diagram으로 많이 표현합니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/87246759-4c214080-c48a-11ea-8b7f-c0a7800fef30.jpeg&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 10. back-up diagram for bellman equation&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;finite MDP인 경우, 아래 그림과 같이 행렬방정식으로 표현됩니다. 벨만 방정식이 선형방정식으로, 아래와 같이 직접적으로 풀 수 있으나 계산 복잡도가 높습니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/87246872-fa2cea80-c48a-11ea-8a25-a57082c47176.jpeg&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 11. Analytic Solution for Value of MRP&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;상태, 행동집합의 크기가 작은 경우엔 행렬방정식으로 풀 수 있지만 크기가 큰 경우에는 불가능합니다. 따라서 이런 경우엔 iterative한 방법으로 방정식을 풀 수 있습니다. iterative 방법들에 대해선 추후에 다루도록 하겠습니다.&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;dynamic programming&lt;/li&gt;
&lt;li&gt;monte-carlo evaluation&lt;/li&gt;
&lt;li&gt;temporal difference learning&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;이제까지 MRP를 정의하였고 MRP를 정의내리기 위해 필요한  return, 가치함수, 가치함수로 유도되는 벨만방정식도 살펴보았습니다. 이제 Markov Decision Process를 살필 준비가 되었습니다.&lt;/p&gt;

&lt;h1&gt;Markov Decision Process&lt;/h1&gt;
&lt;p&gt;Markov Decision Process(MDP)는 MRP에 행동(actions)이 더해진 것입니다. 즉, 명시적인 의사결정이 등장합니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/87247097-9acfda00-c48c-11ea-9c4a-7732a7c7e9d0.png&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 11. Markov  Decision Process	&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;위에서 언급했듯이, 환경모델은 상태변이모델과 보상모델로 이뤄졌기 때문에, MRP에서 이를 하나로 나타낼 수 있었습니다. 마찬가지로, MDP에서 환경모델은 행동(action)까지 고려한 통합된 환경모델은 아래와 같습니다.&lt;/p&gt;

\[p(s',r|s, a) = P[S_{t+1}=s', R_{t+1}=r|S_{t}=s, A_{t}=a]\]

&lt;h2&gt;Policies&lt;/h2&gt;
&lt;p&gt;MDP에서 좋은 의사결정을 하기 위해, 에이전트 내부에 행동 전략을 가지고 있어야 합니다. 이를 policy라 합니다. 정책의 정의는 아래 식처럼,&lt;/p&gt;

\[\pi(a|s)= P[A_t=a|S_t=s]\]

&lt;p&gt;현재 상태 $S_t=s$ 에서, 모든 행동들에 대한 확률 분포입니다. 상태는 마코브 성질을 가지므로, 현재 상태만으로도 의사결정 시 충분한 근거가 될 수 있습니다. 따라서, 현재상태만 조건으로 가진 조건부 확률분포가 되는 것입니다. 또한, MDP의 policy는 시간에 따라 변하지 않습니다(stationary). 이 말은 시간이 지남에 따라 에이전트가 동일한 상태를 여러번 지나간다 해도 그 상태에 있을 때의 행동전략은 변하지 않는다는 뜻입니다.&lt;/p&gt;

&lt;p&gt;MDP와 명시적인 policy가 있다면, 이는 MRP문제와 동일합니다. MDP의 보상함수는&lt;/p&gt;

\[R^{\pi}(s) = \sum_{a \in A}\pi(a|s)R(s,a)\]

&lt;p&gt;는 policy와 가중평균으로 MRP의 보상함수로 바뀝니다. 마찬가지로, MDP의 상태변이함수도&lt;/p&gt;

\[P^{\pi}(s'|s) = \sum_{a \in A}\pi(a|s)P(s'|s,a)\]

&lt;p&gt;policy와의 가중평균으로 MRP의 상태변이함수가 됩니다. 이 두식의 변환은 결국 MDP에서의 벨만방정식을 풀 때, MRP에서 사용한 방법(simulation, analytic solution, iterative method)을 동일하게 사용해도 되는 것을 뜻합니다.&lt;/p&gt;

&lt;h2&gt;Value Function and Bellman Expectation Equation&lt;/h2&gt;
&lt;p&gt;MDP아래에서 Value Function을 다시 살펴봅시다. 기존에 상태만은 고려한 state value function이 있고, 이젠 행동까지 고려한 state-action value function이 있습니다.&lt;/p&gt;

\[v_{\pi}(s) = \mathbf E[R_{t+1}+\gamma v_{\pi}(S_{t+1})|S_t=s]\]

\[q_{\pi}(s,a) = \mathbf E[R_{t+1}+\gamma q_{\pi}(S_{t+1}, A_{t+1})|S_t=s, A_t=a]\]

&lt;p&gt;State value function을 현 상태와 다음 상태사이의 관계로 분해한 것처럼, state-action value function도 동일한 방식으로 분해할 수 있습니다. 이렇게 분해된 식은 Bellman expectation equation 또는 bellman equation이라 합니다.&lt;/p&gt;

&lt;blockquote&gt;$v_{\pi}$, $q_{\pi}$ 의미는 정책 $\pi$ 에 따라 행동했을 때의 가치함수를 의미합니다.&lt;/blockquote&gt;

&lt;p&gt;MRP에서, 벨만 방정식을 back-up diagram으로 나타낸 것처럼 $v_{\pi}$, $q_{\pi}$ 도 back-up diagram으로 표현가능합니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/37501153/87249534-914d6e80-c49a-11ea-8ae2-7e9d042ab586.jpg&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 12. 4종류 bellman equation&lt;/figcaption&gt;&lt;/p&gt;
&lt;p&gt;총 4종류의 back-up diagram이 나오며, 이는 4종류의 bellman equation을 뜻합니다.&lt;/p&gt;

&lt;h2&gt;Bellman Optimal Equation&lt;/h2&gt;
&lt;p&gt;이제까지 알아본 가치함수(또는 벨만 기대 방정식)는 일정 정책 아래에서의 가치를 구한 것이기 때문에, 정책의 가치라고도 생각할 수 있습니다. 그러나 강화학습의 목표는 reward hypothesis에 따라, 누적보상이 최대가 되는 “최적 정책”을 찾는 것입니다. 그럼 최적 정책은 어떻게 찾을까요? 여러 정책들 간의 비교를 통해서 찾을 수 있습니다. 이와 관련된 개념이 ‘partial ordering’입니다.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;partial ordering&lt;/b&gt;&lt;br /&gt;
여러 정책들 간 비교가 가능하다는 건 ‘이 정책이 다른 정책보다 낫다’가 수학적으로 비교가 가능하다는 것입니다. 따라서 이 수학적 비교의 척도가 되는 것이 가치함수간의 비교입니다.
정책 $\pi$ 가 다른 정책 $\pi’$ 보다 나을려면, 각 정책 아래 가치함수를 구했을 때 모든 상태에 대해서 $v_{\pi}(s) \geq v_{\pi’}(s)$ 입니다.&lt;/p&gt;

\[\pi \geq \pi', if\,\,and\,\,only\,\,if\,\,v_{\pi}(s) \geq v_{\pi'}(s), for \,\,all\,\,\,s \in S\]

&lt;p&gt;즉, 최소한 하나의 정책이 다른 정책보다 같거나 나은 정책이 존재한다는 것입니다. 이것이 바로 최적 정책(optimal policy) $\pi_\ast$ 이고 이때 가치 함수를 최적 가치 함수(optimal value function) $v_\ast(s)$ 라 합니다. 가치함수의 종류에는 상태-가치 함수와 상태-행동 가치 함수가 있습니다. 최적 상태-가치 함수(optimal state-value function) $v_*(s)$ 는&lt;/p&gt;

\[v_*(s) = \underset{\pi}max\,v_{\pi}(s)\]

&lt;p&gt;이고, 최적 상태-행동 가치함수(optimal state-action value function) $q_*(s,a)$ 는&lt;/p&gt;

\[q_*(s,a)  = \underset{\pi}max\,q_{\pi}(s,a)\]

&lt;p&gt;입니다. MDP에서 최적 가치 함수를 찾았다면, 이는 결국 일련의 최고의 결정을 수행할 수 있는 것을 뜻하고, sequntial decision making 문제를 “해결”한 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Bellman Optimality Equations&lt;/b&gt;&lt;br /&gt;
상태 가치 함수와 상태-행동 가치 함수를  back-up diagram을 이용하여 4종류의 bellman expectation equation을 세울 수 있었습니다. 마찬가지로, 최적 상태 가치 함수와 최적 상태-행동 가치 함수를 같은 방식으로 4종류의 bellman optimality equation을 세울 수 있습니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/37501153/87273683-c1395800-c514-11ea-9df3-bdf38cafc876.jpg&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 13. Bellman Optimality Equation&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;벨만 기대 방정식과 벨만 최적 방정식은 현 상태와 이전 상태 사이에서의 recursive한 관계를 가진다는 것이 특징입니다. MDP문제를 푸는 방법(벨만 방정식을 푸는 방법)중 하나인 Dynamic Progamming은 바로 이 recursive한 관계를 이용하여 iterative하게 해답을 찾아나가는 과정입니다. DP는 추후 포스팅에서 다루도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Finding an Optimality and Solving the Bellman Optimality Equation&lt;/b&gt;&lt;br /&gt;
이제까지 최적정책의 정의와 최적정책을 찾기 위한 최적 가치 함수에 대해서 알아봤습니다. 그런데 아직 해결이 안된 부분이 있습니다. 바로, &lt;b&gt;최적 가치 함수를 이용하여 어떻게 최적 정책을 찾을까?&lt;/b&gt;에 관한 물음과 &lt;b&gt;상태의 갯수가 많은 상황에서, 즉 복잡도가 높은 MDP문제에서 방정식을 어떻게 풀까?&lt;/b&gt;에 관한 물음입니다. 먼저 전자부터 살펴보겠습니다.&lt;/p&gt;

&lt;p&gt;벨만 최적 방정식을 풀어서 $v_\ast$ 를 구했다면, 정책을 구하는 건 어렵지 않습니다. 그림 13.에서 1번, 3번 최적방정식에서,&lt;/p&gt;

\[\pi_\ast(s) = arg \underset a max(q_\ast(s,a))\]

&lt;p&gt;$q_\ast(s,a)$ 가 최대가 되는 행동 a 가 바로 상태 s에 대한 최적 정책입니다. Recursive한 관계에서 살펴보면,&lt;/p&gt;

\[\pi_\ast(s) = arg \underset a max(R^a_s + \gamma\sum_{s'}P^a_{ss'}v_\ast(s'))\]

&lt;p&gt;와 같습니다. 즉, 최적 정책을 찾을 땐 greedy하게 찾습니다. greedy한 이유는 정책의 행동을 선택할 때, 앞으로의 모든 상황을 고려하는 것이 아니라 다음 상태의 상황만을 고려하기 때문입니다. 그러나 greedy하게 선택해도 될까요 ? 정답은 yes 입니다. 왜냐하면 &lt;strong&gt;이미 가치함수를 구하는 과정에서 미래 상황까지 고려한 가치를 구했기 때문에 이것을 기반으로 한 greedy 선택 안에는 이미 long-term sequence를 고려한 것&lt;/strong&gt;입니다.&lt;/p&gt;

&lt;p&gt;마지막으로, 방정식을 푸는 방법에 대한 물음입니다. 이미 이전에 벨만 기대 방정식을 푸는 방법에 대해서 살펴보았습니다. 벨만 기대 방정식은 linear equation이기 때문에, 복잡도가 높지 않은 MDP 문제에서 analytic하게 구할 수 있습니다. 그러나 복잡도가 높은 MDP문제는 불가능하므로, iterative method인 dynammic progamming, monte-carlo evalution, Temporal difference가 있다고 하였습니다. 반면에, 벨만 최적 방정식은 non-linear equation이기 때문에 analytic하게 풀 수는 없습니다. 따라서 위에서 언급한 iterative method를 적용해야 합니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;이상으로 MDP 포스팅을 마치겠습니다. 다음 포스팅은 Dynamic Progamming에 대해 진행하겠습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;http://web.stanford.edu/class/cs234/slides/lecture2.pdf&quot;&gt;CS234 Winter 2019 course Lecture 2&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://incompleteideas.net/book/bookdraft2017nov5.pdf&quot;&gt;Richard S. Sutton and Andre G. Barto : Reinforcement Learning : An Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.davidsilver.uk/wp-content/uploads/2020/03/MDP.pdf&quot;&gt;David Silver Lecture 2&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</content>
 </entry>
 
 <entry>
   <title>Reinforcement Learning 소개[2]</title>
   <link href="http://localhost:4000/reinforcement%20learning/2020/07/11/introRL(2)/"/>
   <updated>2020-07-11T00:00:00+09:00</updated>
   <id>http://localhost:4000/reinforcement%20learning/2020/07/11/introRL(2)</id>
   <content type="html">&lt;p&gt;이번 포스팅은 &lt;a href=&quot;https://ralasun.github.io/reinforcement%20learning/2020/07/11/introRL(1)/&quot;&gt;강화학습 소개[1]&lt;/a&gt;에 이어서 진행합니다. CS234 1강, Deep Mind의 David Silver 강화학습 강의 1강, Richard S. Sutton 교재 Reinforcement Learning: An Introduction의 Chapter 1 기반으로 작성하였습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;지난 포스팅에서, 강화학습의 특성과 강화학습 문제를 정의하기 위해 필요한 요소, 강화학습 문제의 종류에 대해서 알아봤습니다. 에이전트는 에이전트의 상태를 가지고 어떻게 좋은 행동을 선택할 수 있을까요 ? 이 문제에 대한 답을 하기 전에, 에이전트가 상태 이외에 어떠한 요소를 가지고 있어야 하는지 알아봅시다.&lt;/p&gt;

&lt;h2&gt;Major Components of an RL Agent&lt;/h2&gt;
&lt;p&gt;강화학습 에이전트를 구성하는 요소는 크게 정책(policy), 가치 함수(value function), 모델(Model)입니다.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Policy&lt;/b&gt;&lt;br /&gt;
정책은 에이전트의 행동전략(agent’s behavior)입니다. 정책은 일종의 함수로, 주체의 상태를 행동으로 맵핑합니다. 정책의 종류는 deterministic policy $a = \pi(s)$ 와 stochastic policy와 $\pi(a|s) = P [A_t=a|S_t=s]$ 가 있습니다.&lt;/p&gt;
&lt;blockquote&gt;A policy is a map from state to action&lt;/blockquote&gt;

&lt;p&gt;아래와 같이 화성탐사기 예를 들어봅시다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/87240302-5c69f900-c453-11ea-87ba-7b34cd3fbdb1.jpg&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 1. 화성탐사기 예제&lt;/figcaption&gt;
&lt;/p&gt;
&lt;p&gt;위의 그림과 같이, 화성탐사기가 도달할 수 있는 상태는 총 7가지 상태이고, 각 상태에서 취할 수 있는 행동은 왼쪽/오른쪽 두가지입니다. s1 상태에서 +1 보상을, s7 상태에서 +10 보상을, 나머지 상태에서 0의 보상을 받습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/87240430-82dc6400-c454-11ea-9687-455bbccff299.jpg&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 2. 화성 탐사기 정책 예시&lt;/figcaption&gt;
&lt;/p&gt;
&lt;p&gt;위 예는 화성 탐사가 가질 수 있는 정책 중 하나입니다(사실 이 예제는 너무 단순해서 이 정책이 최적 정책이긴 합니다). s7상태에서 가장 큰 보상을 받기 때문에 어떤 상태에서 시작하던 간에 오른쪽으로 가는 것이 최고의 정책이죠. 또한 이 정책의 특성은 deterministic입니다. 그 이유는 각 상태에서 나올 수 있는 모든 행동들의 가능성을 보여주는 것이 아니라, 한가지 행동만을 출력하기 때문입니다.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Value Function&lt;/b&gt;&lt;br /&gt;
다음은 가치 함수입니다. 가치 함수는 특정 정책 아래, 현재 에이전트 상태에서 앞으로 받을 미래 보상까지 고려한 누적보상 예측값입니다. 에이전트는 이 가치값을 기반으로 현 상태의 좋고 나쁨을 판단합니다. 또한 정책 간 가치함수를 비교를 통한 행동 선택의 기반이 되기도 합니다.&lt;/p&gt;

\[V_{\pi} = E_{\pi}[R_{t+1} + \gamma R_{t+2} + \gamma^2R_{t+3} + \dots|S_t=s]\]

&lt;p&gt;$\gamma$ 는 discount factor로, 현재 보상과 미래 보상간의 중요도 차이를 보여줍니다. 추후에 더 자세히 설명하도록 하겠습니다. 아래는 가치함수의 예입니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/87240661-69d4b280-c456-11ea-8279-8c3fa528358e.jpg&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 3. 화성 탐사기 가치함수 예시&lt;/figcaption&gt;
&lt;/p&gt;
&lt;p&gt;화성 탐사 문제를 그림 1. 처럼 정의했을 때, 각 상태에서 가질 수 있는 화성 탐사 에이전트의 가치함수입니다.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Model&lt;/b&gt;&lt;br /&gt;
모델이란 환경(정확히, 주체가 영향을 받고 있는 환경)이 주체의 행동에 따라 어떻게 변하는지에 관한 모델입니다. 즉 환경의 동적모델(dynamic models of the environment)이죠.&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;A model predicts what the environment will do next&lt;/li&gt;
&lt;li&gt;$P$ predicts the next state&lt;/li&gt;
&lt;li&gt;$R$ predicts the next immediate reward&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

\[P^a_{ss'} = P[S_{t+1}=s'|S_t=s, A_t=a]\]

\[R^a_s = E[R_{t+1}|S_t=s,A_t=a]\]

&lt;p&gt;환경의 모델은 크게 두 가지가 있습니다. 변이모델(transition/dynamics model)과 보상입니다. 변이모델은 에이전트의 행동에 따라 다음 상태에 대한 정보를 알려줍니다. 보상모델은 에이전트가 선택한 행동에 대해 변이모델에 따라 다음 상태에 갔을 때 받는 즉각적인 보상입니다. 아래는 모델에 관한 화성탐사 예시입니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/87240816-e3b96b80-c457-11ea-90ce-96d711a65cc4.jpg&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 4. 화성 탐사기 모델 예시&lt;/figcaption&gt;
&lt;/p&gt;

&lt;h2&gt;Categorizing RL Agents&lt;/h2&gt;
&lt;p&gt;에이전트를 구성하는 요소로는 모델, 가치함수, 정책임을 알았습니다. 그러나 사실 구성요소를 어떻게 조합하느냐에 따라 강화학습 에이전트의 종류를 몇가지로 나눌 수 있습니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;300&quot; src=&quot;https://user-images.githubusercontent.com/37501153/87240991-88887880-c459-11ea-881b-49b8789bde78.jpg&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 5. RL Agent Taxonomy&lt;/figcaption&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Value Based&lt;/li&gt;
에이전트가 행동을 선택할 때, 가치함수의 결과값이 가장 높은 쪽으로 선택하는 것입니다. 이때, 정책이 명시적으로 표현되지 않고 내재적으로 표현됩니다.
&lt;li&gt;Policy Based&lt;/li&gt;
명시적인 정책을 가진 에이전트입니다. 즉, 특성 상태에서 어떤 행동이 확률적으로 높은지를 보고 행동하는 것입니다.
&lt;li&gt;Actor-Critic&lt;/li&gt;
위 Value-based와 policy based가 합쳐진 상태입니다.
&lt;li&gt;Model Free&lt;/li&gt;
변이확률과 보상함수에 대한 정보가 없는 경우입니다. 에이전트는 환경모델을 알 수 없으나 경험을 해나가면서 환경을 이해해 나가면서 문제를 해결하는 케이스입니다.
&lt;li&gt;Model Based&lt;/li&gt;
환경 모델을 구축하여 문제를 푸는 케이스입니다.&lt;/ul&gt;

&lt;h2&gt;Key Challenges in Learning to Make Sequences of Good Decisions&lt;/h2&gt;
&lt;p&gt;연속적인 의사 결정 문제는 문제의 상태에 따라 다르게 접근해야 합니다. 어떤 문제 같은 경우, 환경의 모델을 완벽하게 아는 경우가 있을 수 있습니다. 예를 들어, 무인 헬리콥터에서, 헬리콥터가 있는 환경에서 바람의 속도, 바람의 방향, 장애물의 위치, 날씨, 온도등 헬리콥터에 영향을 줄 수 있는 모든 요소들을 다 파악할 수 있으면 환경 모델을 완벽하게 아는 경우입니다. 이런 경우, 우리는 헬리콥터를 움직일 때마다 어떠한 결과를 초래하고 헬리콥터 움직임에 좋은지 나쁜지를 일일이 “계산”할 수 있습니다. 이런 경우를 Planning이라 합니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/87241351-46f9cc80-c45d-11ea-8551-ac02e8a60e28.jpg&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 6. Planning&lt;/figcaption&gt;
&lt;/p&gt;
&lt;p&gt;하지만, 대부분의 경우, 환경 모델을 완벽하게 아는 것은 불가능합니다. 따라서 환경에 직접 부딪혀 가면서(환경과 상호작용하면서) 경험을 통해 환경모델을 간접적으로 익히는 것입니다. 따라서 에이전트는 환경과의 상호작용을 통해 얻은 경험을 바탕으로 자신만의 전략을 구축해 나가는 것입니다. 포커 게임에서, 상대방이 가지고 있는 패나 전략을 알 순 없지만 상대방과 여러 번 게임을 통해 상대방 전략을 간접적으로 익힐 수 있습니다. 이렇게 푸는 방법이 “Reinforcement Learning”입니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/87241443-12d2db80-c45e-11ea-8d6d-dfda09341bfe.jpg&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 7. Reinforcement Learning&lt;/figcaption&gt;
&lt;/p&gt;

&lt;h2&gt;Exploration and Exploitation&lt;/h2&gt;
&lt;p&gt;강화학습은 &lt;b&gt;환경과의 상호작용을 통해 경험을 쌓아가면서 에이전트의 전략을 스스로 구축해나가는 것&lt;/b&gt;이라 하였습니다. 좋은 전략을 구축하기 위해, 실패도 해보고 성공도 해보면서 배워나가야 합니다. 그러나 어느 정도 경험을 했다면, 이 경험을 바탕으로 대략적인 전략을 구축해 나가야합니다. 이와 관련 강화학습 문제가 exploration과 exploitation입니다.&lt;/p&gt;

&lt;p&gt;exploration이란 추후에 에이전트가 더 좋은 결정을 내릴수도 있기 때문에 새로운 결정을 시도해보는 과정입니다. 반면에, exploitation은 이제까지의 경험을 바탕으로 결정을 내리는 과정입니다. 이 둘 사이는 trade-off관계에 있습니다. exploration을 많이 하면 새로운 시도로 좋은 의사 결정 전략을 구축할 수 있지만 반면에 지금 당장 받을 보상을 희생해야 합니다. 이 둘 사이의 trade-off관계에 관한 예를 들어봅시다. 외식을 하기 위해 식당을 선택하는 경우, 이제까지 경험을 바탕으로 좋아하는 식당을 가는 건 exploitation이고, 새로운 식당을 시도해 보는 건 exploration입니다. 이 외에 다른 예는 아래와 같습니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/87241596-c25c7d80-c45f-11ea-8454-ed31384441ef.jpg&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 8. exploration and exploitation&lt;/figcaption&gt;
&lt;/p&gt;

&lt;h2&gt;Evalutation(Prediction) and Control&lt;/h2&gt;
&lt;p&gt;에이전트는 좋은 경험을 쌓기 위해서, exploration과 exploitation을 균형있게 활용해야합니다. 하지만  어떻게 활용하면서 경험을 쌓고, 전략을 구축하는 걸까요? 이제 관한 문제가 evalution과 control입니다.&lt;/p&gt;

&lt;p&gt;evaluation은 일정 정책 아래, 기대보상을 추정하여 현재 따르는 정책이 좋고/나쁨을 평가하는 것입니다. 기대보상 추정은 결국 가치함수를 구하는 것입니다. 가치함수 $V_{\pi}(s)$ 는 현 정책을 따랐을 때, 상태 $s$ 의 가치입니다. 직접적으로는 현 상태의 가치지만 현 정책 아래에서 계산한 것이기 때문에 현 정책의 가치로도 생각할 수 있습니다. control은 최적정책을 찾는 것입니다. evalutation과정을 통해 정책의 가치를 평가했다면, 이 평가를 기반으로 더 나은 정책이 있는지를 찾는 과정입니다.&lt;/p&gt;

&lt;p&gt;evaluation과 control은 독립적인 과정이 아닙니다. evaluation을 해야 control을 하고, control을 해야 evalutation을 할 수 있습니다. 즉 서로 맞물려서 최적의 정책을 찾아 나가는 것입니다. 아래 그리드월드 예제를 들어봅시다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/87241960-4b28e880-c463-11ea-9aea-94314e98f3e9.jpg&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 9. evalutation(prediction)&lt;/figcaption&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/87241958-46643480-c463-11ea-8d86-772141a0e043.jpg&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 10. control&lt;/figcaption&gt;
&lt;/p&gt;

&lt;p&gt;그림 9에서 주어진 정책은 모든 행동(좌, 우, 위, 아래)으로 갈 확률이 0.25로 동일합니다. 이 정책 아래 각 상태(그리드월드 한 칸)의 가치를 평가하면 오른쪽 숫자로 채워진 테이블이 됩니다. 최적 정책을 최적 가치함수 기반으로 찾을 수 있습니다. 그림 10처럼 최적 가치함수가 가운데 표처럼 구해졌다고 합시다. 이 기반으로 구한 최적 정책은 오른쪽 표와 같습니다. A’상태에서의 최적 정책은 위로 올라가는 것입니다. 왜냐하면 A’ 주변 가치함수 값들은 14.4, 17.8, 14.4입니다. A’에서 가장 높은 17.8의 가치를 가진 상태로 가는 것이 최적이기 때문입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;이상으로, Reinforcement Learning 소개[2] 포스팅을 마치겠습니다. 다음 포스팅은 &lt;a href=&quot;https://ralasun.github.io/reinforcement%20learning/2020/07/12/mdp/&quot;&gt;Markov Decision Process&lt;/a&gt;을 알아보도록 하겠습니다.&lt;/p&gt;

&lt;hr /&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;http://web.stanford.edu/class/cs234/slides/lecture1.pdf&quot;&gt;CS234 Winter 2019 course Lecture 1&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://incompleteideas.net/book/bookdraft2017nov5.pdf&quot;&gt;Richard S. Sutton and Andre G. Barto : Reinforcement Learning : An Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.davidsilver.uk/wp-content/uploads/2020/03/intro_RL.pdf&quot;&gt;David Silver Lecture 1&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

</content>
 </entry>
 
 <entry>
   <title>Reinforcement Learning 소개[1]</title>
   <link href="http://localhost:4000/reinforcement%20learning/2020/07/11/introRL(1)/"/>
   <updated>2020-07-11T00:00:00+09:00</updated>
   <id>http://localhost:4000/reinforcement%20learning/2020/07/11/introRL(1)</id>
   <content type="html">&lt;p&gt;이번 포스팅은 강화학습이 기존에 알려진 여러 방법론들과의 비교를 통한 강화학습 특성과 구성요소를 다룹니다. CS234 1강, Deep Mind의 David Silver 강화학습 강의 1강, Richard S. Sutton 교재 Reinforcement Learning: An Introduction의 Chapter 1 기반으로 작성하였습니다.&lt;/p&gt;

&lt;hr /&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;450&quot; alt=&quot;introRL&quot; src=&quot;https://user-images.githubusercontent.com/37501153/87217901-2f511400-c389-11ea-96de-492485cf0b9d.png&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 1.&lt;/figcaption&gt;
&lt;/p&gt;
&lt;p&gt;아래 그림과 같이, Computer Science, Engineering, Mathematics 등 다양한 분야에서 여러 문제들을 풀기 위한 방법론들이 있습니다. 예를 들어, Pyschology 분야의 Classical/Operant Conditioning은 동물들의 의사결정에 대해 연구하고, Enginnering 분야의 Optimal Control와 Mathematics 분야의 Operation Research는 자연 현상을 일련의 시퀀스로 파악하여 공학적 또는 수학적 관점에서 ‘어떻게 하면 최상의 결과를 얻을까?’ 를 연구합니다. 즉, 이러한 연구들의 공통점은 각 분야에서 정의한 문제 해결을 위해 &lt;b&gt;과학적 의사결정(scientific decision making)&lt;/b&gt;을 연구한다는 것입니다.&lt;/p&gt;

&lt;p&gt;이와 마찬가지로, 강화학습도 &lt;b&gt;“좋은 의사결정을 내리기 위한 방법”&lt;/b&gt;에 관한 연구입니다. 특히, &lt;b&gt;“순차적인 의사결정이 필요한 문제”&lt;/b&gt;를 풀기 위한 방법론입니다(Learn to make good sequences of decisions). 여기서, “good decisions”은 결국 최적의 해결책(optimal soltuion)을 찾는 것을 의미하고, “learn”은 학습하는 대상이 처한 상황이 어떤지 모른 채, 직접 부딪혀 나가면서 경험을 통해 배워나가는 것을 의미합니다. 이는 마치, 사람이 학습해 나가는 방법과도 유사하죠.&lt;/p&gt;

&lt;blockquote&gt;Sutton 교재에서, 강화학습이 사람의 학습 방법과 유사하다고 기술되어 있습니다. 유아기 때, 걷기까지 걷는 방법을 알려주는 선생님이 존재하지 않고, 아기가 스스로 여러번 시도와 실패 끝에 걷는 방법을 터득합니다. 강화학습도 이러한 측면에서의 특성을 가지고 있습니다.&lt;/blockquote&gt;

&lt;h1&gt;Characteristics of Reinforcement Learning&lt;/h1&gt;
&lt;p&gt;강화학습의 특징을 다른 방법론과 비교를 통해 알아봅시다. 강화학습의 특징을 우선 정리하고, 그 후 비교를 통해 구체적으로 알아볼 것입니다. 강화학습은 아래와 같이 4가지 특징을 가지고 있습니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Optimization&lt;/li&gt;
&lt;li&gt;Delayed consequences&lt;/li&gt;
&lt;li&gt;Exploration&lt;/li&gt;
&lt;li&gt;Generalization&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;
&lt;b&gt;1. Optimization&lt;/b&gt;&lt;br /&gt;
good decision이란 최적의 해결책(optimal solution)에 해당된다고 하였습니다. 즉, Optimization은 강화학습의 목적에 해당되며, 그 목적은 좋은 결정을 내리기 위한 최적의 방법을 찾는 것입니다.
&lt;blockquote&gt;Goal is to find an optimal way to make decisions&lt;/blockquote&gt;
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;2. Delayed Consequences&lt;/b&gt;&lt;br /&gt;
순차적인 의사결정 문제에서, 현재 내린 결정은 후에 일어날 상황에 영향을 줄 수 있습니다. 예를 들어, 돈을 저축하는 건 현재 시점에선 마이너스 행위일 수도 있지만, 만기 이후를 생각하면 플러스 행위입니다. 즉, 현재 내린 결정에 대한 영향력을 확실히 알 수가 없고(delayed consequences), 이로 인해 결정의 좋고 나쁨을 평가하는 것이 어렵습니다.  
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;3. Exploration&lt;/b&gt;&lt;br /&gt;
위에서, 강화'학습'은 에이전트(학습하는 사람, 기계 등을 지칭)가 학습하는 상황/대상에 대한 어떠한 정보가 없기 때문에 스스로 배워나가는 것이라 하였습니다. 따라서, 에이전트는 무수히 많은 의사결정을 통해 탐험을 해야합니다. 자전거를 타는 기술을 익히기 위해 수많은 실패를 하는 것처럼 말입니다. 그러나, 이 '탐험'도 '잘'해야 합니다. 어떠한 탐험을 하느냐에 따라 경험하는 것이 다르기 때문입니다. 이렇게 얻어진 경험이 좋은 경험일 수도 나쁜 경험일 수도 있습니다.&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;4. Generalization&lt;/b&gt;&lt;br /&gt;
여러 머신러닝 방법론과 마찬가지로, 강화학습은 특정 문제만 풀수 있는 에이전트가 아니라 일반화된 문제를 풀 수 있는 에이전트를 학습하고 싶습니다. 바둑을 예로 들어봅시다. 강화학습을 통해 바둑게임 에이전트를 만들고자 할 때, 대전을 하는 상대방이 어떠한 전략을 가지고 있던 간에 항상 이길 수 있는 에이전트를 만드는 것이 목표지 특정 전략에만 강한 에이전트를 만들고 싶은 것이 아닙니다. &lt;br /&gt;&lt;br /&gt;
위와 같은 이유로, rule-based 방식으로 순차적 의사결정문제를 풀기가 어렵습니다. rule-based 기반 해결책은 generalization 특성을 갖지 못하기 때문입니다. 
&lt;blockquote&gt;pre-programmed policy is hard to get generalization on the problem we want to tackle.&lt;/blockquote&gt;
&lt;/p&gt;
&lt;p&gt;‌
강화학습은 위와 같이 4가지 특성을 가지고 있습니다. 이제 여러 방법론(Planning, supervised learning, unsupervised learning, imitation learning)과 비교를 통해 위 4가지 특성에 대해 강화학습이 다른 방법론과 어떻게 다른지 알아 봅시다.&lt;/p&gt;

&lt;h2&gt;AI Planning vs Reinforcement Learning&lt;/h2&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img alt=&quot;planning&quot; width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/87219220-56f9a980-c394-11ea-933d-1e59431206ac.jpeg&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 2. planning&lt;/figcaption&gt;
&lt;/p&gt;
&lt;p&gt;Planning이란 에이전트가 학습하는 환경에 대한 정보를 완벽히 알고 있는 경우입니다. 그림 2.는 유명한 아타리 게임입니다. 만약에 아타리 게임의 에이전트가 게임 콘솔 안의 모든 게임 알고리즘과 하드웨어 작동 방식등 완벽하게 알고 있다고 해봅시다.(&lt;del&gt;거의 불가능한 상황이긴 합니다.&lt;/del&gt;)&lt;/p&gt;
&lt;blockquote&gt;
Agent just computes good sequence of decisions but given model of how decisions impact world
&lt;/blockquote&gt;

&lt;p&gt;이 말은 에이전트가 현재 게임 상황에서 왼쪽/오른쪽 움직임에 대해 나올 결과를 완벽히 알 수 있다는 뜻입니다. 즉, 에이전트는 더이상 ‘학습’이 아니라 어떻게 의사 결정을 내릴지 ‘계획’하면 되는 것이지요. 따라서, planning은 4가지 특성 중, exploration은 해당되지 않습니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Optimization&lt;/li&gt;
&lt;li&gt;Delayed consequences&lt;/li&gt;
&lt;li&gt;&lt;del&gt;Exploration&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;Generalization&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Supervised/Unsupervised Learning vs Reinforcement Learning&lt;/h2&gt;
&lt;p&gt;지도학습은 라벨이 있는 데이터 셋 ${{(x_1,y_1), \dots ,(x_i,y_i))}}$ 을 학습 한 후, 입력 $x_i$ 가 들어오면 입력에 대한 라벨 $\hat y_i$ 를 예측하는 문제입니다. 반면에 비지도 학습은 라벨이 없는 데이터 셋에 대하여 ${{x_1, \dots ,x_i}}$ 에 대하여 학습을 통해 데이터 셋의 구조를 파악하는 것입니다. 강화학습과의 차이점은 데이터 셋의 유무에 있습니다. 강화학습은 어떻게 보면, 탐험을 통해 데이터 셋을 스스로 구축해 나가는 것이라 볼 수 있지만, 지도/비지도 학습은 이 경험에 해당되는 데이터 셋이 주어진 것이기 때문에, 탐험을 할 필요가 없습니다. 또다른 차이점은 의사결정에 해당되는 라벨 예측 행위가 추후 또다른 예측 행위에 영향을 주지 않습니다. 따라서, exploration과 delayed consequences가 없습니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Optimization&lt;/li&gt;
&lt;li&gt;&lt;del&gt;Delayed consequences&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;del&gt;Exploration&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;Generalization&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Imitation Learning vs. Reinforcement Learning&lt;/h2&gt;
&lt;p&gt;둘의 차이점을 비교하기 전에, &lt;a href=&quot;https://medium.com/@SmartLabAI/a-brief-overview-of-imitation-learning-8a8a75c44a9c&quot;&gt;imitation learning&lt;/a&gt;이 뭔지 간략하게 알아봅시다. 좋은 의사결정을 내리기 위한 에이전트를 강화학습을 통해 만들기 위해선 에이전트가 탐험 시 내린 의사결정에 대한 좋고 나쁨을 알려줘야 합니다. 우리는 이를 ‘보상’이라 합니다. 그러나 현실 문제에서 정확한 보상함수를 정의내리기가 어렵습니다. 따라서 이를 해결하기 위해 나온 방법이 imitation learning입니다. 에이전트가 직접 탐험하는 것이 아니라 모방하고 싶은 에이전트의 행동을 지도학습 방식으로 해결하는 것입니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img alt=&quot;imitation learning&quot; width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/87220082-f9695b00-c39b-11ea-8488-2e9906448f0e.jpg&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 3. Imitation Learning&lt;/figcaption&gt;
&lt;/p&gt;
&lt;p&gt;따라서, imitation learning은 모방하고 싶은 에이전트의 경험을 데이터 셋으로서 활용하기 때문에 exploration 요소가 없습니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Optimization&lt;/li&gt;
&lt;li&gt;Delayed consequences&lt;/li&gt;
&lt;li&gt;&lt;del&gt;Exploration&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;Generalization&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;The Reinforcement Learning Problem&lt;/h1&gt;
&lt;p&gt;이번 섹션에서는 강화학습 문제를 정의하기 위해 필요한 요소들을 알아봅시다.&lt;/p&gt;

&lt;h2&gt;Rewards &amp;amp; Sequential Decision Making&lt;/h2&gt;
&lt;p&gt;좋은 행동을 하는 에이전트를 강화학습을 통해 만들기 위해선 에이전트가 탐험할 때 결정한 행동에 대한 좋고 나쁨을 알려줘야 합니다. 우리는 이를 ‘보상’이라 합니다.&lt;/p&gt;
&lt;blockquote&gt;강화학습에서 '의사 결정'을 '행동'이라 부릅니다.&lt;/blockquote&gt;
&lt;p&gt;보상 $R_t$ 는 $t$ 스텝에서 에이전트가 의사결정을 잘 내리고 있는지에 대해 환경이 주는 즉각적인 피드백 지표(imediate reward)입니다. 즉, 에이전트의 목표는 매 스텝마다 받는 보상을 누적했을 때, 이 누적값이 최대화가 되도록 의사결정을 하는 것입니다. 이 말은 에이전트는 당장 받는 보상이 아니라 앞으로 받을 보상을 고려해서 행동한다는 것입니다. 이러한 아이디어는 가설로 구축할 수 있습니다.&lt;/p&gt;
&lt;p&gt;&lt;blockquote&gt;&lt;b&gt;reward hypothesis&lt;/b&gt;&lt;br /&gt;
That all of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal.&lt;/blockquote&gt;
&lt;/p&gt;
&lt;p&gt;예를 들어, 게임같은 경우 이기면 (+)보상을, 질 경우 (-)보상으로 보상을 정의내릴 수 있습니다. 또한 발전소를 컨트롤 하는 경우, 전력을 생산하는 경우엔 (+)보상을 줄 수 있지만 만약에 안전 임계치를 초과한 경우 (-)보상으로 정의내릴 수 있습니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img alt=&quot;sequential-decision-making&quot; width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/87220849-6e3f9380-c3a2-11ea-8f9b-cf5320abca64.jpg&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 4. Sequential Decision Making&lt;/figcaption&gt;
&lt;/p&gt;
&lt;p&gt;따라서, 강화학습의 목표는 순차적인 의사결정을 통해 누적 보상의 기댓값을 최대화하는 것입니다. 현재 행동은 앞으로 할 행동들에 영향을 줄 수 있으며 더 많은 보상을 나중에 받기 위해 현재 당장 받을 즉각적인 보상은 포기할 수도 있습니다.&lt;/p&gt;

&lt;p&gt;그러면 보상은 누가 주는 걸까요 ? 바로 에이전트가 놓여있는 환경입니다. 에이전트 이외 그 밖의 요소가 환경이 될 수 있습니다. 예를 들어, 주식시장을 살펴봅시다. 주식의 매매 여부를 결정하는 주체는 에이전트이고, 팔기, 사기, 그대로 두기는 행동(의사결정)입니다. 만약에 에이전트가 주식을 파는 행동을 하였다면, 이 행동의 좋고 나쁨은 어떻게 결정될까요? 우리는 흔히, 내가 판 주식이 올랐다면 이 행동은 나쁜거라고 생각할 수 있습니다&lt;del&gt;(왜냐면, 더 있다가 팔면 좋았을 테니깐요.)&lt;/del&gt;. 그럼 내가 판 주식이 오르게 하는 건 어떤걸까요? 바로 주체 이외의 여러 요소들에 의해 결정됩니다. 그 주식과 관련된 여러 기업일 수도 있고, 정치도 해당될 수 있고, 여러 가지 요소가 주식에 영향을 줍니다. 바로 이러한 부분이 강화학습에서의 ‘환경’입니다.&lt;/p&gt;
&lt;blockquote&gt;예상할 수 있듯이, 환경을 정의내리기엔 매우 어렵습니다.&lt;/blockquote&gt;
&lt;p&gt;따라서, 강화학습의 목표를 다시 정의내리자면, &lt;b&gt;주체는 &lt;em&gt;“환경과의 상호작용”&lt;/em&gt;을 통해서 누적 보상을 최대화&lt;/b&gt;하는 것입니다. 그림 4와 그림5는 주체가 환경과의 상호작용하는 일련의 과정을 보여줍니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;500&quot; alt=&quot;agent-env-interaction&quot; src=&quot;https://user-images.githubusercontent.com/37501153/87221282-c4620600-c3a5-11ea-8a6e-adcce86b754e.png&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 5. 주체와 환경간의 상호작용&lt;/figcaption&gt;
&lt;/p&gt;
&lt;p&gt;매 스텝에서, 에이전트가 누적보상이 최대가 되도록 행동 $A_t$ (action) 을 결정하면, 환경은 선택한 행동에 대한 결과 $O_{t+1}$ (observation) 와 행동에 대한 실제 보상 $R_{t+1}$ (imediate reward) 을 알려줍니다. 주식시장을 다시 예로 들어보면, 주체가 주식을 파는 것이 행동 결정이고, 추후에 그 기업의 주식이 오르는 것이 행동에 대한 결과이며, 이에 대해 돈을 잃는 것이 실제 보상입니다.&lt;/p&gt;

&lt;h2&gt;History and State&lt;/h2&gt;
&lt;p&gt;연속적인 의사 결정(sequential decision making)이기 때문에, 매스텝마다 행동 $A$, 관찰 $O$, 보상 $R$이 발생합니다. 따라서 발생한 모든 행동, 관찰, 보상에 대한 시퀀스를 히스토리(history)라고 합니다.&lt;/p&gt;

\[H_t = O_1, R_1, A_1, \dots, A_{t-1}, O_t, R_t\]

&lt;blockquote&gt;The history is the sequence of observations, actions, rewards. In other words, It is all observable variables up to time t&lt;/blockquote&gt;
&lt;p&gt;따라서, 에이전트는 히스토리 기반으로 다음에 취할 행동을 선택합니다. 왜냐하면, 히스토리는 이전에 발생한 모든 일들을 다 기록하기 때문에 꽤나 행동 선택에 꽤나 괜찮은 근거가 될 수 있기 때문입니다.&lt;/p&gt;

&lt;p&gt;그러나, 행동을 선택할 때마다, 매번 이전 과거 정보를 파악하는 건 힘든 일입니다. 따라서 에이전트는 다음 행동을 선택하는데 &lt;b&gt;상태(state)정보&lt;/b&gt;를 이용합니다.
상태정보가 행동 선택의 근거가 되기 위해선 상태는 과거 히스토리 정보를 담고 있어야 합니다. 따라서, 수학적으로 표현하면 상태는 히스토리의 함수입니다.&lt;/p&gt;

\[S_t = f(H_t)\]

&lt;blockquote&gt;State is information assumed to determine what happens next&lt;/blockquote&gt;

&lt;p&gt;상태에는 크게 Environment State(World State), Agent State, Information State가 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Environment State(World State)&lt;/b&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img alt=&quot;environment state&quot; width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/87238822-aa2a3580-c442-11ea-8b87-a098c7288a58.jpeg&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 6. Environment State&lt;/figcaption&gt;
&lt;/p&gt;
&lt;p&gt;환경 상태 $S^e_t$ 는 주체 이외의 환경에 대한 상태로, 예를 들어 게임에서는 게임의 콘솔 내부일수도 있고, 주식시장에서는 주식에 영향을 주는 모든 요소일수 있습니다. 에이전트는 사실 환경을 볼 수 없으며 볼 수 있다 하더라도 불필요한 정보들이 많을 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Agent State&lt;/b&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img alt=&quot;agent state&quot; width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/87238872-49e7c380-c443-11ea-880a-40d6a5738804.jpeg&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 7. Agent State&lt;/figcaption&gt;
&lt;/p&gt;
&lt;p&gt;에이전트 상태 $S^a_t$ 는 행동하는 주체의 상태를 표현한 것입니다(the agent’s internal representation). 에이전트는 이 상태를 기반으로 다음 행동을 선택하고, 강화학습 시 사용되는 상태 정보입니다. 또한, 히스토리의 함수 $S_t^a=f(H_t)$ 로 표현될 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Information State&lt;/b&gt;&lt;br /&gt;
정보 상태(information state)는 마코브 성질을 가지는 마코브 상태입니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/37501153/87239025-2d4c8b00-c445-11ea-9c11-473ca87cd331.jpg&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 8. Markov State&lt;/figcaption&gt;
&lt;/p&gt;
&lt;p&gt;$S_t$ 가 주어졌을 때 $S_{t+1}$ 의 확률은 $t$ 시점까지의 모든 상태가 주어졌을 때 $S_{t+1}$ 의 확률과 같으면 마코브 상태입니다. 즉, 현재상태 이전의 과거정보들은 미래정보에 대해 아무런 영향을 주지 않는다는 것입니다. 그 이유는 이미 현재 상태는 과거 정보를 충분히 포함하고 있기 때문에, 이 정보만으로 미래를 파악하기에 충분하다는 것입니다.&lt;/p&gt;

&lt;blockquote&gt;The state is sufficient statistic of the future.&lt;br /&gt;
The future is independent of the past given the present
$H_{1:t} \to S_t \to H_{t+1:\infty}$&lt;/blockquote&gt;
&lt;p&gt;마코브 상태로는 환경상태 $S^e_t$ 와 히스토리 $H_t$ 입니다. $S^e_t$ 는 주체한테 미치는 영향을 모두 포함하고 있기 때문에 마코브 상태이고 마찬가지로 $H_t$도 관찰가능한 일련의 모든 시퀀스를 포함하고 있기 때문에 역시 마코브 상태입니다.&lt;/p&gt;

&lt;h2&gt;MDP and POMDP&lt;/h2&gt;
&lt;p&gt;강화학습 문제를 정의하기 위해서, 상태, 행동, 보상에 대한 정의가 필요합니다. 하지만 상태는 마코브 상태일수도 있고 아닐 수도 있습니다. 그럼 각각에 따라 강화학습 문제를 접근하는 방법도 달라집니다. 아래에서 더 살펴봅시다.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Fully Observable Environments&lt;/b&gt;&lt;br /&gt;
에이전트가 환경 상태를 직접적으로 관찰할 수 있을 때, 에이전트는 Fully Observability를 가집니다. 이는 결국 에이전트 상태가 환경상태와 동일한 경우입니다.&lt;/p&gt;

\[O_t = S^a_t = S^e_t\]

&lt;p&gt;일반적으로, 에이전트가 Fully Observability를 가질 때, Markov Decision Process(MDP)를 따른다고 합니다.&lt;/p&gt;

&lt;p&gt;아래와 같이 MDP의 예로 화성탐사 문제를 정의해봅시다.&amp;lt;p align='center'&amp;gt;
&lt;img width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/87240302-5c69f900-c453-11ea-87ba-7b34cd3fbdb1.jpg&quot; /&gt;&lt;/p&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 9. 화성탐사기 예제&lt;/figcaption&gt;
&lt;p&gt;&amp;lt;/p&amp;gt;
위의 그림과 같이, 화성탐사기가 도달할 수 있는 상태는 총 7가지 상태이고, 각 상태에서 취할 수 있는 행동은 왼쪽/오른쪽 두가지입니다. s1 상태에 있으면, +1보상을, s7상태에 있으면 +10보상을 받고, 나머지 상태에서는 0의 보상을 받습니다. 이처럼, 화성탐사기가 어느 상태에 있을 때 어떤 보상을 받을지 다 알고 있는 상황이 화성탐사기 에이전트가 환경상태임을 의미합니다.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Partially Observable Environments&lt;/b&gt;&lt;br /&gt;
에이전트가 환경을 간접적으로 관찰할 수 밖에 없을 때, 에이전트는 Partial observability를 가집니다. 예를 들어, 로봇은 카메라 센서로만 인식할 수 있는 장애물만 파악할 수 있습니다. 카드게임에서는 상대방의 카드패는 알 수 없고, 본인이 가진 카드만 알 수 있습니다.&lt;/p&gt;

\[O_t = S^a_t \neq S^e_t\]

&lt;p&gt;이 경우 에이전트 상태는 환경 상태와 동일하지 않으며, 에이전트가 partially observable Markov Decision process(POMDP)를 따릅니다.&lt;/p&gt;

&lt;p&gt;에이전트는 본인의 상태를 반드시 정의내려야 합니다. 전체 히스토리를 에이전트 상태로 둘 수도 있고, ‘환경에 대한 정보가 ~할 것이다’라는 믿음으로 구축할 수도 있습니다. 아니면, RNN을 이용하여 인코딩된 벡터로 에이전트 상태를 나타낼 수도 있습니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;use history : $S_t^a = H_t$ &lt;/li&gt;
&lt;li&gt;&lt;span style=&quot;color:red&quot;&gt;Beliefs&lt;/span&gt; of environment state : $S_t^a = (P[S^e_t = s^1], \dots, P[s^e_t = s^n])$ &lt;/li&gt;
&lt;li&gt;Recurrent neural network : $S^a_t = \sigma(S^a_{t-1}W_s+O_tW_o)$ &lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;&lt;b&gt;Deterministic ? Stochastic?&lt;/b&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/37501153/87239676-bf0bc680-c44c-11ea-821e-7f85a6a00d21.jpg&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 10. Deterministic vs. Stochastic&lt;/figcaption&gt;
&lt;/p&gt;
&lt;p&gt;강화학습 문제를 MDP인지 POMDP로 보는 관점말고도 deterministic한지 stochastic한지 보는 관점도 있습니다. deterministic한 강화학습 문제는 환경이 에이전트의 행동에 따라 변할 때 그 결과 오로지 하나의 결과만 보여줍니다(single observation and reward). 그러나, stochastic한 강화학습 문제는 에이전트의 행동에 따라 환경이 변할 때, 가능성 있는 여러 결과를 보여줍니다. 물론 그 결과가 나올 확률과 함께 말이죠.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;이번 포스팅은 여기까지 마치겠습니다. &lt;a href=&quot;https://ralasun.github.io/reinforcement%20learning/2020/07/11/introRL(2)&quot;&gt;Reinforcement Learning 소개(2)&lt;/a&gt;에 이어서 포스팅하도록 하겠습니다.&lt;/p&gt;

&lt;hr /&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;http://web.stanford.edu/class/cs234/slides/lecture1.pdf&quot;&gt;CS234 Winter 2019 course Lecture 1&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://incompleteideas.net/book/bookdraft2017nov5.pdf&quot;&gt;Richard S. Sutton and Andre G. Barto : Reinforcement Learning : An Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.davidsilver.uk/wp-content/uploads/2020/03/intro_RL.pdf&quot;&gt;David Silver Lecture 1&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/@SmartLabAI/a-brief-overview-of-imitation-learning-8a8a75c44a9c&quot;&gt;Imitation learning : a brief over view of imitation learning, https://medium.com/@SmartLabAI/a-brief-overview-of-imitation-learning-8a8a75c44a9c&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

</content>
 </entry>
 
 <entry>
   <title>Junction Tree Variational Autoencoder for Molecular Graph Generation 논문 리뷰</title>
   <link href="http://localhost:4000/deep%20learning/2020/07/06/jtvae/"/>
   <updated>2020-07-06T00:00:00+09:00</updated>
   <id>http://localhost:4000/deep%20learning/2020/07/06/jtvae</id>
   <content type="html">&lt;p&gt;Junction Tree Variational Auto-encoder(JT-VAE)는 기존 SMILES string 기반 생성 모델들이 SMILES string을 사용하는 것에 문제를 제기하여 캐릭터가 아니라 molecular graph가 직접 입력으로 들어가는 모델입니다. 또한, 유효한 화합물 구조를 생성하기 위해 Junction Tree Algorithm에서 아이디어를 착안하여 모델을 제시하였습니다.&lt;/p&gt;

&lt;h2&gt; Problem &lt;/h2&gt;
&lt;p&gt;SMILES string을 입력으로 하는 것은 크리티컬한 2가지 문제가 발생합니다. 먼저, &lt;strong&gt;SMILES 표현은 화합물간 유사도를 담아내지 못합니다.&lt;/strong&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;400&quot; alt=&quot;similarity&quot; src=&quot;https://user-images.githubusercontent.com/37501153/86242796-34230480-bbe0-11ea-8f46-7d7856206408.png&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 1&lt;/figcaption&gt;
&lt;/p&gt;
&lt;p&gt;위 그림 1.을 보면, 두 화합물의 구조는 유사하지만 SMILES으로 나타냈을 땐 전혀 다른 표현이 됩니다. 따라서, SMILES 표현의 한계로 인해 VAE와 같은 생성모델들이 화합물 임베딩 공간을 올바르게 형성하지 못합니다. &lt;strong&gt;두번째로, 그래프 형태의 화합물 표현이 SMILES 화합물 표현보다 분자의 화학적 특성을 더 잘 담아냅니다.&lt;/strong&gt; 이러한 이유로, 본 논문은 그래프적 표현(molecular graph)을 직접적으로 사용하는 것이 유효한 구조를 만들어 내는 것을 향상시킬 것이라 가정을 하고 있습니다.&lt;/p&gt;

&lt;h2&gt; Junction Tree Variational Auto-Encoder &lt;/h2&gt;

&lt;p&gt;Molecular graph를 만든다는 건 일반적으로 원자를 하나씩 순차적으로 생성하는 것으로 생각할 수 있습니다(Li et al., 2018). 그러나, 본 논문에서는 이러한 접근법은 유효하지 않은 구조(chemically invalid)를 만들어 낼 가능성이 높다고 합니다. 원자를 하나씩 붙여나가면서 생성하면 중간 단계 구조들은 invalid하며, 완전한 구조가 나올때 까지 딜레이가 길기 때문입니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;400&quot; src=&quot;https://user-images.githubusercontent.com/37501153/86246329-c7126d80-bbe5-11ea-8d0a-d1f909528ea0.png&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 2&lt;/figcaption&gt;
&lt;/p&gt;

&lt;p&gt;따라서, 본 논문은 원자 단위로 molecular graph를 만들어 나가는 것이 아니라 &lt;strong&gt;유효한 분자 단위들의 집합을 미리 정해놓고&lt;/strong&gt; 이 단위들을 붙여 나가면서 화합물을 구축합니다. 마치 자연어처리에서 문장 생성 문제를 풀 때, 사전을 미리 구축해 놓고 그 속에 존재하는 단어들로 문장을 구축해 나가는 것과 같이 생각하면 될 것 같습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;junction tree 라는 이름이 붙게 된 이유는 다음과 같습니다. 유효한 분자 단위는 molecular graph내의 sub-graph로 생각할 수 있고, 이 sub-graph는 이 그래프 자체로도 유효한 화학 분자 구성 요소를 이룹니다. 즉, 마치 junction tree의 node가 complete graph인 clique과 유사합니다. 이 부분에서 junction tree 아이디어를 착안한 것입니다&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;하나의 분자에 대해서 생성되는 방식은 다음과 같습니다. 어떤 유한한 개수의 유효한 화합물 단위(valid components)들로 구성된 집합에서 해당 분자를 구성할 것 같은 요소들을 선택한 후, 그 요소들을 가지고 제일 그럴듯한 구조가 나오도록 조합하는 것입니다. 이런 식의 접근의 장점은 Li et al.(2018)와 다르게 valid한 화합물을 생성하는 것을 보장할 수 있습니다. 또한 구성요소 간의 상호작용 관계도 고려되기 때문에 더 실용적인 방법입니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;유효한 화합물 단위는 마치 building block과 같은 역할입니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p align=&quot;center&quot;&gt;
 &lt;img width=&quot;400&quot; src=&quot;https://user-images.githubusercontent.com/37501153/86303866-15f0ef00-bc48-11ea-99c6-a537a9e5f0e8.png&quot; /&gt;
 &lt;figcaption align=&quot;center&quot;&gt;그림 3&lt;/figcaption&gt;
 &lt;/p&gt;

&lt;p&gt;그림 3.은 Junction Tree VAE(JT-VAE) 모식도입니다. 첫번째 단계로 한 분자가 입력으로 들어오면, 화합물 단위 사전을 이용하여 Tree Decomposition을 수행합니다. 수행 결과, Junction Tree $\tau$ 가 나옵니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;하나의 분자가 주어졌을 때, 한 분자는 2가지 종류의 표현을 가지게 됩니다. - Molecular graph 와 Junction Tree 표현&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;2가지 표현을 가지고 있는 것과 같이 Graph Encoder/Decoder와 Tree Encoder/Decoder로 구성됩니다. Molecular Graph는 Graph Encoder에 의해 $z_{G}$ 로 인코딩됩니다. 마찬가지로, Tree Encoder에 의해 Junction Tree $\tau$ 는 $z_{\tau}$ 으로 인코딩됩니다. 그런 후 Tree Decoder와 화합물 사전을 이용하여 제일 가능성이 높은 화합물 단위를 조합하여 Junction Tree $\hat{\tau}$ 를 생성합니다. 이 때, Junction Tree $\hat{\tau}$ 에서 화합물 단위간 연결(edge)는 화합물 간 결합 방향 및 결합 종류(단일결합, 이중결합 등등)에 관한 정보를 포함하고 있지 않고, 상대적인 배열에 관한 정보만을 담고 있습니다. 그 다음, graph decoder에 의해, Junction Tree $\hat{\tau}$ 와 $z_{G}$ 는 Molecular graph 표현으로 나타내지게 됩니다. 이 과정에서 화합물 단위 사이의 결합이 정해지게 됩니다. 화합물 단위 간 결합될 수 있는 후보들을 나열 한 후, 각 후보 군에 대한 결합점수를 매기고, 점수가 가장 높은 결합이 화합물 단위 사이의 결합으로 결정됩니다.&lt;/p&gt;

&lt;h3&gt; Junction Tree Algorithm&lt;/h3&gt;
&lt;p&gt;junction tree algorithm은 probabilistic graphical model에서 inference problem를 효율적으로 풀기 위한 알고리즘입니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;inference 문제 2가지 종류&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt; 

	&lt;li&gt;Marginal inference : what is the probability of a given variable in our model after we sum everything else out?&lt;/li&gt;
	
	 $$p(y=1) = \sum_{x_1}\sum_{x_2}\sum_{x_3}\dots\sum_{x_n}p(y=1,x_1,x_2,x_3,\dots,x_n)$$

	&lt;li&gt;Maximum a posteriori(MAP) inference : what is the most likely assignment to the variables in the model(possibly conditioned on evidence)?&lt;/li&gt;
	
$$\max_{x1,\dots,x_n} p(y=1,x_1,x_2,x_3,\dots,x_n)$$
&lt;/ol&gt;

&lt;p&gt;변수 간 dependence가 표현된 directed acyclic graph $G$ 를 undirected graph로 변환한 뒤, 정해진 변수 간 order에 의해 변수들의 cluster를 하나의 single node로 구성하고, 특정 규칙 아래(변수 간 dependence가 잘 반영될 수 있도록), cluster간 edge를 연결하여 Junction Tree $\tau_{G}$=($\nu$, $\varepsilon$), $\nu$ : nodes, $\varepsilon$ : edges 를 구축합니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;변수간 order에 의해 cluster를 구성해 나가면서 tree를 구축하는 건 variable elimination과 관련이 있습니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;구축된 tree는 cycle-free이고, 변수들의 cluster는 graph 상에서 complete graph(clique)를 이루고 있어야 합니다. 또한 서로 이웃된 cluster 간에 공통되는 변수들이 있을 때, cluster 간 연결된 path 위에 해당 변수들로 구성된 cluster가 있어야 합니다. Junction Tree는 아래 그림에서와 같이 세가지 특성을 가져야 합니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;400&quot; src=&quot;https://user-images.githubusercontent.com/37501153/86310382-e696ae00-bc58-11ea-9a09-d303222bcda0.png&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림. 4. Junction Tree Properties&lt;/figcaption&gt;
&lt;/p&gt;

&lt;h4&gt; belief propagation as message passing &lt;/h4&gt;
&lt;p&gt;Junction tree $\tau_{G}$ 를 가지고, inference problem을 푸는 방법 중 하나가 message-passing algorithm을 이용한 belief propagation입니다. 아래 그림과 같이 variable elimination을 통해 marginal inference 문제를 해결해 나갈 수 있습니다. 이 때, 정해진 변수 순서에 따라 summing out 되면서 변수가 순차적으로 제거됩니다(marginalized out).&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/37501153/86317080-7f352a00-bc69-11ea-85c5-e29e3da427de.jpg&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 5&lt;/figcaption&gt;

아래 그림과 같이 variable elimination이 되는 과정이 마치 tree 상에서, 한 node가 marginalization이 되면서 연결되어 있는 다른 노드로 message를 전달하는 과정으로 볼 수 있습니다. 아래 그림에서, $x_1$ 을 summing out에서 제거하기 위해선 우선적으로 $x_2$ 가 summing out되어 $x_1$ 으로 message인 $m_{21}(x_1)$ 이 전달되어야 합니다. 마찬가지로, 우리가 구하고 싶은 marginalized distribution인 $p(x_3)$ 를 구하기 위해선 $x_1, x_4, x_5$ 에서 오는 message가 모두 올 때까지 기다렸다가 계산을 할 수 있습니다.

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/37501153/86316985-39786180-bc69-11ea-8c3f-b76f0c70c62a.jpg&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 6&lt;/figcaption&gt;
&lt;/p&gt;

$i$ node 에서 $j$ node로 가는 message $m_{i \rightarrow j}$ 는 아래와 같이 정의할 수 있습니다.

$$m_{i \rightarrow j} = \sum_{x_i}\phi(x_i)\phi(x_i,x_j)\prod_{l \in N(I) \setminus j}m_{l \rightarrow j}(x_i)$$
&lt;p align=&quot;center&quot;&gt; 수식. belief propagation &lt;/p&gt;

$i$ node 에서 $j$ node로 가는 message $m_{i \rightarrow j}$ 는 **j node를 제외하고 i로 가는 모든 node의 메세지를 기다렸다가** i node와 연관된 distribution function을 다 계산한 후 summing out하는 것입니다. 

특정 node와 연결된 message가 모두 올 때까지 기다렸다가 계산하는 방식을 belief propagation이라 합니다. Loopy belief propagation은 이를 기다리지 않고 계산하고, 모든 노드에서 수렴할 때까지 반복하여 inference 문제를 해결하는 방식입니다. 

$$m_{i \rightarrow  j}^{t+1} = \sum_{x_i}\phi(x_i)\phi(x_i,x_j)\prod_{l \in N(I) \setminus j}m_{l \rightarrow j}^{t}(x_i)$$
&lt;p align=&quot;center&quot;&gt; 수식. Loopy belief propagation &lt;/p&gt;

&lt;h3&gt;message passing network&lt;/h3&gt;
[Neural Message Passing for Quantum Chemistry](https://arxiv.org/abs/1704.01212)은 기존에 존재하는 그래프 모델들을 message passing algorithm을 학습하는 모델로 다시 해석하였습니다. 아래와 같이 세가지 함수를 정의하여 그래프 모델들을 분자의 화학적 특성을 예측하는 등 Quantum Chemistry에 적용하는 연구를 하였습니다. 
- A Message Passing function : $m_{v}^{t+1}=\sum_{w \in N(v)}M_t(h_v^t,h_u^t,e_{uv})$
- A Node Update function : $h_{v}^{l+1}=U_t(h_v^t,m_v^{t+1})$ 
- A Readout function(ex. classification) : $\hat y = R({h_v^T|v \in G})$

즉, 한 원자의 특성을 결정짓는 건 원자와 연결된 다른 원자로 부터 오는 정보와 자기 자신에 의해 결정됨을 의미합니다. 본 논문에서도 위와 같은 아이디어를 사용하였습니다. 

&lt;h3&gt; Tree Decomposition of Molecules &lt;/h3&gt;
Molecule Junction Tree 는 junction tree $\tau_{G} = (\nu, \varepsilon)$ 에서 $\chi$ 가 추가된 $\tau_{G} = (\nu, \varepsilon, \chi)$ 입니다. $\chi$ 는 junction tree의 node 후보가 될 수 있는 화합물 구성 단위들의 집합 사전을 나타냅니다. 화합물 단위 사전은 ring결합으로 이뤄진 화합물(ex. aromatic compound), 일반 결합(?)(a single edges ex. single bond, double bond, triple bond..)으로만 구성됩니다. 여기서 사용된 집합 사전의 크기 |$\chi$|=$780$ 입니다.

여기서, 집합 사전의 크기가 한정적이기 때문에, 다양한 종류의 분자를 표현하는 것이 가능한 것에 대해 의문이 들 수 있습니다. 본 논문에서는 training set에 존재하는 화합물들 기반으로 분자 집합 사전을 구축했으며, test set에 있는 분자들을 대부분 커버했기 때문에 크게 문제 삼지 않고 넘어갔습니다.
 
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;600&quot; alt=&quot;tree-decomp&quot; src=&quot;https://user-images.githubusercontent.com/37501153/86321270-a0027d00-bc73-11ea-891e-60ed31070b37.png&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 7. Tree Decomposition of Molecules&lt;/figcaption&gt;
&lt;/p&gt;

위 그림은 tree decomposition을 나타낸 그림입니다. 집합 사전 $\chi$ 를 가지고, cycle구조와 edge 구조로 분해합니다. cycle은 고리형 화합물이고, edge는 단순 결합으로 이뤄진 화합물입니다. 위 그림에서 색칠된 노드가 화합물 단위로 분해된 것을 가르킵니다.

&lt;h3&gt;Graph Encoder&lt;/h3&gt;
먼저, molecular graph 표현을 graph message passing network을 통해 인코딩합니다. Graph에서 원자에 해당하는 vertex는 특징 벡터 $\mathrm x_{v}$(원자 종류, 공유가 등과 같은 특성을 나타내는 벡터), 원자 간 결합을 나타내는 edge $(u,v) \in E$ 는 특징 벡터 $\mathrm x_{uv}$(결합 종류) 로 표현합니다. 또한 message passing algorithm에 의해 두 원자 간 주고받는 message hidden vector $\nu_{uv}$(u에서 v로 가는 message) 와 $\nu_{vu}$(v에서 u로 가는 message)로 표현합니다. Graph encoder에서 message가 전달되는 방식은 loopy belief propagation을 따릅니다. 아래 식에 의하면, 한 원자의 hidden vector는 결국 자신의 특징과 더불어 자신과 결합하고 있는 원자들로부터 오는 정보로 표현되는 것입니다.

$$\nu_{uv}^{(t)} = \tau(W_1^g\mathrm x_u + W_2^g\mathrm x_{uv} + W_3^g\sum_{w \in N(u) \setminus v} \nu_{wu}^{(t-1)})$$

$$\mathbf {h}_u=\tau(\mathrm U_1^g\mathrm x_u + \sum_{v \in N(u)}\mathrm U_2^g\nu_{vu}^{(T)})$$

한 분자에 대하 최종적인 표현은 $\mathbf h_G = \sum_i \mathbf h_i/|V|$ 로, 분자에 포함된 모든 원자들의 hidden vector들을 평균낸 것입니다. 그런 다음, VAE처럼 평균 $\mu_G$ 와 분산 $\sigma_G$ 를 계산하는 레이어가 각각 연결되고, 잠재 벡터 $z_G$ 는 $\mathcal N(\mu_G, \sigma_G)$ 에서 샘플링됩니다. 

 
&lt;h3&gt;Tree Encoder&lt;/h3&gt;

Tree Encoder도 message passing network방식으로 분자의 junction tree 표현을 인코딩합니다. 각 클러스터 $C_i$ 는 해당 라벨에 대한 정보를 담고 있는 원핫 벡터 $\mathrm x_i$ 로 표현되고, 클러스터 $(C_i,C_j)$ 간 주고받는 메세지 정보는 $\mathbf m_{ij}, \mathbf m_{ji}$ 로 표현합니다. Tree에서,임의로 root node 를 정하고 메세지는 GRU unit을 이용해 $\mathbf m_{ij} = \mathrm GRU(\mathbf x_i, {\mathbf m_{k \in N(i) \setminus j}})$ 와 같이 업데이트됩니다. 

$$\mathbf s_{ij} = \sum_{k \in N(I) \setminus j}\mathbf m_{kj}$$

$$\mathbf z_{ij} = \sigma(\mathbf W^z \mathbf x_i +\mathbf U^z\mathbf s_{ij}+\mathbf b^z)$$

$$\mathbf r_{kj} = \sigma(\mathbf W^r \mathbf x_i +\mathbf U^r \mathbf m_{ki}+\mathbf b^r)$$

$$\mathbf {\tilde m_{ij}} = tanh(\mathbf W \mathbf x_i+\mathbf U \sum_{k \in N(I) \setminus j}\mathbf r_{ki} \odot \mathbf m_{ki})$$

$$\mathbf m_{ij} = (1-\mathbf z_{ij}) \odot \mathbf s_{ij}+\mathbf z_{ij} \odot \mathbf {\tilde m_{ij}}$$

Graph Encoder와 다르게 Tree Encoder는 loopy belief propagation이 아니라, 특정 노드에서 다른 노드로 메세지를 전달하기 전에, 특정 노드와 연결된 노드들의 메세지가 다 올 때까지 기다리다가 전달하는 belief propagation 방식을 따릅니다. 특정 노드에서 message passing이 완료된 후, 해당 노드의 hidden feature $\mathrm h_i =\tau(\mathbf W^o \mathbf x_i+\sum_{k \in N(i)}\mathbf U^o \mathbf m_{kj})$ 로 계산됩니다. 즉 노드 i로 오는 모든 메세지와 노드 i의 label feature $x_i$ 를 이용하여 hidden feature vector i 를 계산합니다. 

tree의 최종적인 표현 $\mathbf h_{\mathcal T}=\mathbf h_{root}$ 입니다. Graph encoder에서 평균값으로 계산한 것과는 달리, root node의 hidden feature를 최종 표현으로 둡니다. 그 이유는 tree decoder에서 tree를 생성할 때, 어느 노드에서 시작할 지에 대한 정보가 있어야 하기 때문입니다. 다음으로 graph encoder와 마찬가지로 $\mu_\mathcal T$ 와 $\sigma_\mathcal T$ 를 출력하는 레이어가 각각 연결되고, $\mathcal N(\mu_\mathcal T, \sigma_\mathcal T)$ 에서 latent vector $z_\mathcal T$ 를 샘플링합니다. 
  
&lt;h3&gt;Tree Decoder&lt;/h3&gt;

Tree Decoder를 이용해 $z_\mathcal T$ 를 통해 junction tree $\hat {\mathcal T}$ 를 생성합니다. root node부터 시작해서 top-down 방식과 깊이 우선 탐색(depth-first order) 순서로 나머지 node들을 생성해 나갑니다. 깊이 우선 탐색이란, 루트 노드에서 시작해서 다음 분기로 넘어가기 전에 해당 분기를 완벽하게 탐색하는 방법입니다. 노드를 방문할 때마다 두가지 일을 수행합니다. 

&lt;ol&gt;
&lt;li&gt;topological prediction : 해당 노드가 자식 노드를 가지고 있는지에 대한 여부&lt;/li&gt; 
&lt;li&gt; label prediction : 해당 클러스터의 라벨 예측(화합물 단위 집합 사전에 있는 라벨 예측)&lt;/li&gt; 
&lt;/ol&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/37501153/86680037-2256bc80-c039-11ea-9a90-ceb83dd09fa7.jpeg&quot; /&gt;
&lt;/p&gt;

만약 자식 노드가 더 이상 없다면 해당 노드를 탐색하기 직전 노드로 거슬러 올라갑니다. 각 노드를 방문하여 두가지 일을 수행하기 위해선 연결된 다른 노드로부터 정보를 받아야 합니다. Tree decoder에서 전달되는 정보는 message vector $\mathbf h_{ij}$ 를 이용합니다. tree의 node별로 하나씩 순서대로 생성해 나가면서 방문하는 edge들마다 번호를 매기면 최대 번호는 엣지갯수 x 2 가 됩니다.

Molecular junction tree $\mathcal T=(\mathcal V, \mathcal E)$ 에 대해, 처음 시작해서 t step이 될 때까지 방문한 엣지들을 $\tilde {\mathcal E}$ = { $(i_1, j_1), \dots,(i_t,j_t)$ } 라 하고, t step일 때 방문한 노드를 $i_t$ 라 한다면, 노드 i에서 노드j로 가는 메세지 $\mathbf h_{i_t, j_t}$ 는 i 노드로 향하는 메세지와 t step에서의 노드 특징 벡터(여기서는 라벨 벡터)에 의해 GRU unit을 이용해 업데이트 됩니다. 

$$\mathbf h_{i_t, j_t} = \mathrm {GRU}(\mathbf x_{i_t}, \{\mathbf h_{k,i_t}\}_{(k,i_t) \in \mathcal {\tilde E}_t, k \neq j_t})$$

&lt;h4&gt;Topological Prediction &amp;amp; Label Prediction&lt;/h4&gt;

노드 i에 방문했을 때, 자식 노드 j 존재 여부는 아래와 같이 확률을 계산하여 판단합니다.

$$p_t = \sigma (\mathbf u^d \bullet \tau(\mathbf W_1^d\mathbf x_{i_t}+\mathbf W_2^d\mathbf z_{\tau}+\mathbf W_3^d\sum_{(k,i_t) \in \mathcal {\tilde E_t}}\mathbf h_{k,i_t}))$$

자식 노드 j가 있다면, 자식 노드 j의 라벨 예측은 아래와 같습니다. $\mathbf q_j$ 는 화합물 단위 집합 사전에 대한 분포를 나타냅니다. 라벨 예측 후, 자식 노드 j의 특징벡터 $\mathbf x_j$ 는 분포 $\mathbf q_j$ 에서 샘플링됩니다. &lt;b&gt;샘플링 시, 부모노드와 연결되는 자식노드가 유효하지 않은 화합물 단위가 오면 안되기 때문에 미리 invalid한 화합물 단위들은 분포에서 masking을 하고 샘플링을 진행합니다.&lt;/b&gt;
  
$$\mathbf q_j =\mathrm {softmax(\mathbf U_\tau^l(\mathbf 
W_1^l\mathbf z_{\tau}+ \mathbf W_2^l\mathbf h_{ij}))}$$

Tree decoder가 작동하는 알고리즘과 자세한 설명은 아래와 같습니다.
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;700&quot; src=&quot;https://user-images.githubusercontent.com/37501153/86676892-0bfb3180-c036-11ea-99f5-98371c540158.png&quot; /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/37501153/86562978-8d06ea00-bf9e-11ea-84f3-5c8137bb3140.jpg&quot; /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/37501153/86563121-c9d2e100-bf9e-11ea-93e6-ddac38a0ac13.jpg&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 8&lt;/figcaption&gt;
&lt;/p&gt;

Tree decoder의 목표는 우도 $p(\mathcal T|\mathbf z_{\mathcal T})$ 를 최대화하는 것입니다. 따라서 아래와 같이 크로스 엔트로피 손실함수를 최소화하는 방향으로 학습이 이뤄집니다.

$$L_c(\mathcal T) = \sum_tL^d(p_t, \hat p_t) + \sum_jL^l(\mathbf q_j, \mathbf {\hat q_j})$$

또한 teacher forcing을 이용하여 학습합니다. Teacher forcing이란 매 스텝마다 prediction한 후, 추 후 해당 스텝의 값을 이용할 때 prediction 값이 아니라 ground truth을 이용하는 것입니다.

&lt;h3&gt;Graph Decoder&lt;/h3&gt;
JT-VAE는 마지막으로 graph decoder를 통해 molecular graph를 생성합니다. 그러나 하나의 molecular junction tree는 하나의 molecular graph에 대응하는 것이 아니라 화합물 단위인 subgraph를 어떻게 조합하느냐에 따라 여러 개의 molecular graph를 나타낼 수 있습니다. &lt;del&gt;junction tree의 edge는 단순히 subgraph들의 상대적인 배열만을 나타낸다고 했습니다.&lt;/del&gt; 이렇기 때문에 Graph Decoder의 역할은 올바른 molecular graph를 만들기 위해 subgraph를 잘 조합하는 것입니다. 

$$\hat G = argmax_{G' \in \mathcal g(\mathcal {\hat T})}f^a(G')$$

Tree Decoder에서 root node에서 하나씩 node를 붙여나가듯이, 마찬가지로 subgraph를 하나씩 붙여나가는 것입니다. 그러나 이 때, subgraph를 붙여나갈 때 여러가지 경우의 수가 나오기 때문에 scoring function $f^a$ 을 이용해서 각 경우의 수에 대해 점수를 매깁니다. 가장 높은 점수가 나온 subgraph 간 조합을 두 subgraph간 조합으로 보고 다음 subgraph를 붙여나갑니다. subgraph를 붙여나가는 순서는 tree decoder에서 디코딩된 노드 순을 따릅니다. 그림 8의 예를 보면, 생성되는 tree node 순서에 따라, subgraph를 1-&amp;gt;2-&amp;gt;3-&amp;gt;4-&amp;gt;5 순으로 붙여나가는 것입니다.

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/37501153/86570554-11f80080-bfab-11ea-91a7-8974539e566b.png&quot; /&gt;
&lt;/p&gt;

$G_i$ 를 특정 sub graph cluster인 $C_i$ 와 그것의 neighbor clusters $C_j, j \in N_{\mathcal {\hat T}}(i)$ 을 조합해서 나온 그래프라 한다면, $G_i$ 에 대한 점수는 $f^a_i(G_i) =\mathbf h_{G_i}\bullet\mathbf z_G$ 입니다. $\mathbf h_{G_i}$ 는 subgraph $G_i$ 에 대한 hidden vector representation 입니다. 즉, Graph decoder의 역할은 조합해 가면서 나오는 각 subgraph에 대하여 hidden vector representation을 message passing algorithm을 통해 구하는 것입니다. Junction tree message passing network는 아래와 같습니다.

$$\mu_{uv}^t = \tau(\mathbf W_1^a\mathbf x_u + \mathbf W_2^a\mathbf x_{uv} + \mathbf W^a_3 \tilde \mu_{uv}^{(t-1)})$$

$$\tilde \mu_{uv}^{(t-1)} = 
	\begin{cases}
		\sum_{w \in N(u) \setminus v}\mu_{wu}^{(t-1)} &amp;amp; \quad \alpha_u = \alpha_v \\
		\mathbf {\hat m}_{\alpha_u,\alpha_v} + \sum_{w \in N(u) \setminus v}\mu_{wu}^{(t-1)} &amp;amp; \quad \alpha_u \neq \alpha_v
		\end{cases}$$
		
위 수식을 보면, message 계산 과정이 graph encoder와 비슷합니다. 하나 차이점은 u 원자와 v 원자가 다른 cluster라면 즉, 다른 subgraph라면 전달되는 메세지에 다른 subgraph에서 온 것을 추가적으로 알려준다는 점입니다(provides a tree dependent positional context for bond (u, v)). 이 때 메세지 $\mathbf {\hat m}_{\alpha_u,\alpha_v}$ 는 sub-graph $G_i$ 를 graph encoder를 통과시켜 계산된 메세지입니다. 

Graph decoder 학습은 $\mathcal {L_g}(G) = \sum_i [f^a(G_i) - log \sum_{G' \in \mathcal g_i} exp(f^a(G'_i))]$ 을 최대화하는 과정입니다. Tree decoder와 마찬가지로 teacher forcing을 이용해 학습합니다. 

&lt;h5&gt; Complexity &lt;/h5&gt;
Tree decomposition에 의해, 두 클러스터 간에 공유되는 원자 갯수가 최대 4개이며 또한 tree decoder 과정에서 invalid한 화합물 단위가 나오지 않도록 masking을 통해 sampling 하기 때문에 복잡도는 그리 높지 않습니다. 따라서 JT-VAE의 계산복잡도는 molecular graph의 sub-graph 수의 비례합니다.




		


&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Grammar Variational Autoencoder 논문 리뷰</title>
   <link href="http://localhost:4000/deep%20learning/2020/06/19/gvae/"/>
   <updated>2020-06-19T00:00:00+09:00</updated>
   <id>http://localhost:4000/deep%20learning/2020/06/19/gvae</id>
   <content type="html">&lt;p&gt;Grammar Variational Auto-Encoder(GVAE)는 Gomez-Bomb barelli et al.(2016)&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;의 VAE기반으로 생성된 신약 후보 물질들이 대부분 유효하지 않는 것에 문제를 제기하여, SMILES string 생성 문법을 직접적으로 제약 조건으로 걸어 유효한 신약 후보 물질을 생성하는 모델입니다.&lt;/p&gt;
&lt;h2 id=&quot;problem&quot;&gt;Problem&lt;/h2&gt;
&lt;p&gt;신약 후보 물질의 SMILES string 생성 모델들(RNN, VAE)의 단점은 유효하지 않은 string을 생성하는 경우가 많이 발생한다는 것입니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Valid String : c1ccccc1 (benzene)&lt;/li&gt;
  &lt;li&gt;Invalid String : c1ccccc2 (??)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Gomez-Bomb barelli et al.(2016)가 제안한 VAE는 decoder를 통해 연속적인 latent space에서 discrete한 SMILES string을 생성합니다. 그러나, 유효하지 않는 string에 대한 확률이 높아지도록 학습이 된 경우, 학습이 완료가 된 후에도 계속 올바르지 않은 SMILES string이 생성되는 문제가 발생합니다.&lt;/p&gt;

&lt;p&gt;따라서 본 논문에서는 이러한 이슈를 완화하기 위해 SMILES string을 생성하는 문법에 관한 정보를 모델에게 직접적으로 알려줌으로써 유효한 SMILES string을 생성하도록 하는 모델(Grammar VAE)을 제안하였습니다.&lt;/p&gt;

&lt;h2 id=&quot;context-free-grammars&quot;&gt;Context-free grammars&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/h2&gt;
&lt;p&gt;SMILES string을 생성하는 문법은 문맥 자유 문법(Context-free grammar, CFG)을 따릅니다. 다른 나라 언어를 이해하거나 그 나라 언어로 대화나 글을 쓰기 위해선 문법을 이해하고 있어야 합니다. 마찬가지로 프로그래밍 언어를 이해하기 위해선 그 언어를 정의한 문법을 이해하고 있어야 합니다. 대다수 프로그래밍 언어들이 CFG기반입니다. CFG은 &lt;a href=&quot;https://en.wikipedia.org/wiki/Chomsky_hierarchy&quot;&gt;촘스키 위계&lt;/a&gt;의 type-2에 해당하는 문법입니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;300&quot; src=&quot;https://user-images.githubusercontent.com/37501153/86242024-f2458e80-bbde-11ea-81ba-397850c09b5d.png&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림1. 촘스키 위계&lt;/figcaption&gt;    
&lt;/p&gt;
&lt;p&gt;문맥 자유 문법은 $G=(V, \Sigma, R, S)$ 4개의 순서쌍으로 구성됩니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;V : non-terminal 심볼들의 유한집합&lt;/li&gt;
  &lt;li&gt;$\Sigma$ : terminal 심볼들의 유한집합&lt;/li&gt;
  &lt;li&gt;R : 생성규칙(production rules)의 유한집합&lt;/li&gt;
  &lt;li&gt;S : 시작(start) 심볼&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;예를 들어, 문법 G=({A}, {a,b,c}, P, A), P : A $\rightarrow$ aA, A $\rightarrow$ abc가 있다면, 문법아래 생성될 수 있는 string은 aabc입니다. 위의 예는 단순하지만 생성 규칙에 따라 나올 수 있는 string의 경우의 수는 매우 많습니다. 이렇게 생성된 string을 tree구조로도 표현할 수 있습니다.&lt;/p&gt;

&lt;p&gt;문법 G=({S}, {a,b}, P, S), P : S $\rightarrow$ SS | aSb | $\epsilon$ 이라면, 생성규칙에 따라 생성된 string중 하나는 $S \rightarrow SS \rightarrow aSbS \rightarrow abS \rightarrow abaSb \rightarrow abaaSbb \rightarrow abaabb$ 입니다. 이를 tree구조로 나타내면 아래 그림과 같습니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;https://www.tutorialspoint.com/automata_theory/images/yield_of_a_tree.jpg&quot; width=&quot;500px&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림2. CFG grammar 예시&lt;/figcaption&gt;      
&lt;/p&gt;
&lt;p&gt;GVAE는 CFG의 아이디어를 이용한 모델입니다. CFG기반의 SMILES grammar가 있으며, encoder의 입력값은 SMILES string이 아니라 각 화합물 SMILES string을 생성하기 위해 사용된 생성규칙들입니다. 마찬가지로, decoding 결과는 SMILES string이 아니라, SMILES string에 관한 생성 규칙입니다. 시퀀스 별로 그 다음으로 나올 가능성이 높은 생성 규칙이 결과로 나옵니다. 세부적인 모델 설명은 아래와 같습니다.&lt;/p&gt;

&lt;h2 id=&quot;methods&quot;&gt;Methods&lt;/h2&gt;
&lt;p&gt;본 논문에서 사용된 VAE의 encoder와 decoder는 Gomez-Bomb barelli et al.(2016)와 동일한 구조를 사용하였습니다.&lt;/p&gt;
&lt;h3 id=&quot;encoding&quot;&gt;encoding&lt;/h3&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;1151&quot; alt=&quot;gvae&quot; src=&quot;https://user-images.githubusercontent.com/37501153/85167983-afadb900-b2a4-11ea-9ce0-48aab485b3f5.png&quot; /&gt;  
&lt;figcaption align=&quot;center&quot;&gt;그림3. GVAE encoder&lt;/figcaption&gt;
&lt;/p&gt;
&lt;p&gt;위 그림은 모델의 encoder가 SMILES grammar와 함께 구현되는 과정에 관한 것입니다. 그림3의 1번은 SMILES grammar의 일부입니다. 전체 SMILES grammars는 논문 참고하시기 바랍니다. 예를 들어 벤젠 SMILES string인 c1ccccc1을 encoding 한다고 했을 때(2번), SMILES grammar에 따라 벤젠 SMILEs string의 parse tree를 구축합니다. 그런 뒤, 이 parse tree를 위에서부터 아래, 왼쪽에서 오른쪽 방향으로 생성 규칙들로 다시 분해된 후(3번), 분해된 각 규칙들은 원핫벡터로 변환됩니다(4번 그림). 이 때, 원핫벡터의 차원 $K$ 은 SMILES grammar 생성 규칙의 개수입니다. $T(X)$ 를 분해된 생성규칙들의 개수라 할 때, 벤젠의 생성규칙을 인코딩한 행렬의 차원은 $T(X)\times K$ 가 됩니다. 그 후, Deep CNN을 거쳐서, 벤젠에 대한 생성규칙을 latent space 상에 $z$ 로 맵핑합니다.&lt;/p&gt;

&lt;h3 id=&quot;decoding&quot;&gt;decoding&lt;/h3&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;1142&quot; alt=&quot;gvae-decoding&quot; src=&quot;https://user-images.githubusercontent.com/37501153/85188683-852f2080-b2e3-11ea-9dc1-d8d3714d695e.png&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림4. GVAE decoder&lt;/figcaption&gt;
&lt;/p&gt;

&lt;p&gt;다음은 latent space상에 $z$ 로 맵핑된 벤젠 생성규칙이 어떻게 다시 discrete한 시퀀스를 가진 생성규칙들로 이뤄진 string으로 변환되는지에 관한 과정에 대한 설명입니다. GVAE에서 Decoder의 핵심은 항상 유효(valid)한 string이 나오도록 생성규칙들을 선택하는 것입니다. 먼저, 잠재 벡터 $z$ 를 RNN layer를 통과하여 시퀀스 별로 logit이 출력됩니다(그림4의 2번). logit 벡터의 각 차원은 하나의 SMILES grammar 생성규칙에 대응됩니다. 타임 시퀀스의 최대 길이는 $T_{max}$ 이며 따라서 최대로 나올 수 있는 logit 벡터의 갯수도 $T_{max}$ 입니다.&lt;/p&gt;

&lt;h5 id=&quot;masking-vector&quot;&gt;Masking Vector&lt;/h5&gt;
&lt;p&gt;decoding 결과로 출력된 일련의 생성규칙 시퀀스들이 유효하기 위해서  last-in first-out(LIFO) stack과 masking vector가 등장합니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img alt=&quot;decoding process&quot; src=&quot;https://user-images.githubusercontent.com/37501153/85480797-8f129580-b5fb-11ea-893b-2f25053851e3.jpeg&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림5. decoding process&lt;/figcaption&gt;
&lt;/p&gt;
&lt;p&gt;그림 5.는 stack과 masking vector를 이용한 decoding과정을 나타낸 그림입니다. 제일 첫 심볼은 항상 smiles가 나와야 하므로, (1)처럼 smiles를 stack합니다. 그 다음, smiles를 뽑은 후, smiles으로 시작하는 생성규칙은 1 그 외 나머지는 0으로 구성된 masking vector를 구성한 뒤 첫번째 시퀀스 logit가 element-wise 곱을 합니다((3)). 그런 다음, 아래 mask된 분포에 따라 sampling을 하면 (4)와 같이, smiles $\rightarrow$ chain 생성규칙이 출력됩니다.&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;
$$p(\text{x}_t = k|\alpha,\text{z}) = \frac{m_{\alpha, \,k}exp(f_{tk})}{\sum_{j=1}^Km_{\alpha, \,k}exp(f_{tj})}$$
&lt;figcaption align=&quot;center&quot;&gt;수식1. masked distribution at timestep t&lt;/figcaption&gt;
&lt;/p&gt;
&lt;p&gt;위와 같은 방법으로 $t \rightarrow T_{max}$ 가 될 때까지, sampling을 하면 한 화합물을 구성하는 생성규칙들을 출력하였고, 결국 문법적으로 유효한 화합물을 생성한 것입니다.&lt;/p&gt;

&lt;h3 id=&quot;bayesian-optimization&quot;&gt;Bayesian Optimization&lt;/h3&gt;
&lt;p&gt;Gomez-Bomb barelli et al.(2016)&lt;sup id=&quot;fnref:1:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;를 보면, 약물 특성을 포함한 latent space를 구축하기 위해, latent layer에 약물 특성을 예측하는 MLP layer를 추가하여 학습을 진행합니다. 마찬가지로, 약물 특성이 담긴 latent space를 구축하기 위해 VAE 학습 완료 후, Sparse Gaussian Process(SGP)를 이용하여 예측모델을 학습합니다. 여기서 사용된 약물 특성은 penalized logP 입니다.&lt;/p&gt;

&lt;h2 id=&quot;experiment-result&quot;&gt;Experiment Result&lt;/h2&gt;
&lt;p&gt;GVAE 모델의 성능은 Gomez-Bomb barelli et. al.(2016)&lt;sup id=&quot;fnref:1:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;와 유사한 VAE인 Character VAE(CVAE) 성능과 비교하였습니다. 정성적인 평가를 위해, 임의의 수학표현식을 생성하여 유효한 수학표현식을 만드는지를 확인하였습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;500&quot; alt=&quot;gvae-result1&quot; src=&quot;https://user-images.githubusercontent.com/37501153/85487426-82943a00-b607-11ea-95d6-ffe2cc26c922.jpeg&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림6. 결과 1&lt;/figcaption&gt;
&lt;/p&gt;

&lt;p&gt;&amp;lt;그림 6.&amp;gt;의 Table 1.은두 모델의 embedding 공간의 smoothness를 보여주는 결과입니다. 각각 두 개의 식(볼드체)을 encoding 한 뒤, latent space 상의 두 점을 선형보간법(linear interpolation)을 한 것입니다. 보시면, GVAE는 100% 유효한 수학식이 공간 위에 있지만 CVAE는 그렇지 않은 것을 확인하실 수 있습니다. 이처럼, &lt;strong&gt;GVAE가 유효한 string으로 구성된 latent space를 더 잘 구축한다는 것입니다.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Table 2.은  각 latent space에서 z를 여러 번 샘플링 한 후, decoding 결과 유효한 수학표현식 또는 분자 string의 비율을 나타낸 것입니다. GVAE가 CVAE보다 문법적으로 의미있는 string을 더 잘 출력함을 확인할 수 있습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;500&quot; alt=&quot;gvae-result2&quot; src=&quot;https://user-images.githubusercontent.com/37501153/85489804-888c1a00-b60b-11ea-8ae0-95f8367bb94c.jpeg&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 7. 결과 2&lt;/figcaption&gt;
&lt;/p&gt;

&lt;p&gt;그림 7의 결과는 penalized logP와 latent vector를 가지고 Bayesian Optimization한 결과, penalized logP score가 높은 순으로 3개를 뽑은 결과입니다. CVAE와 GVAE 모두 유효한 string을 내놨을 때, GVAE의 약물 특성에 관한 score가 더 높습니다. 즉, &lt;strong&gt;GVAE를 Bayesian Optimization까지 완료 후 형성된 latent space는 valid한 string을 내뱉는 공간을 형성했을 뿐만 아니라 약물 특성을 잘 포함하는 공간을 구축했음을 의미합니다.&lt;/strong&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;500&quot; alt=&quot;gvae-result2&quot; src=&quot;https://user-images.githubusercontent.com/37501153/85491367-5c25cd00-b60e-11ea-93b5-6c92003622ee.jpeg&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;그림 8. 결과 3&lt;/figcaption&gt;
&lt;/p&gt;

&lt;p&gt;그림 8의 결과는 약물 특성(penalized logP)에 관한 예측 성능에 관한 표입니다. Loss function을 Log-Likelihood와 RMSE를 모두 사용했을 때, GVAE가 CVAE보다 성능이 더 낫습니다. &lt;del&gt;다만, 개인적인 의견으로는 두개의 성능 차이는 거의 나지 않는 것으로 보입니다.&lt;/del&gt;&lt;/p&gt;

&lt;h2 id=&quot;논문-한줄-리뷰평&quot;&gt;논문 한줄 리뷰평&lt;/h2&gt;

&lt;p&gt;GVAE는 이제까지 여러 Generative Model이 상당 수가 SMILES 문법에 어긋나는 string을 출력한다라는 단점을 보완하는 논문입니다. 하지만 valid한 string이 얼마나 신약개발에 적합한 string인지에 대한 결과는 논문에 실리지 않았습니다(&lt;del&gt;이 부분은 대부분의 신약개발 논문들이 나와있지 않는 것으로 보입니다.&lt;/del&gt;). 하지만 valid한 string을 내놓는 것에 있어서 제일 실용적인(practical)하지도 않을까 생각이 듭니다.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://pubs.acs.org/doi/abs/10.1021/acscentsci.7b00572&quot;&gt;Gómez-Bombarelli, Rafael, et al. “Automatic chemical design using a data-driven continuous representation of molecules.” ACS central science 4.2 (2018): 268-276.&lt;/a&gt; &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:1:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:1:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://upcount.tistory.com/99&quot;&gt;Context-free grammar, https://upcount.tistory.com/99&lt;/a&gt; &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</content>
 </entry>
 

</feed>
