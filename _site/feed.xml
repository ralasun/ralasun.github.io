<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.7">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-03-04T17:01:37+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Ralasun Resarch Blog</title><subtitle></subtitle><author><name>Seonhwa Lee</name></author><entry><title type="html">Graph Convolutional Network에 대하여 - Spectral Graph Convolution</title><link href="http://localhost:4000/deep%20learning/2021/02/15/gcn/" rel="alternate" type="text/html" title="Graph Convolutional Network에 대하여 - Spectral Graph Convolution" /><published>2021-02-15T00:00:00+09:00</published><updated>2021-02-15T00:00:00+09:00</updated><id>http://localhost:4000/deep%20learning/2021/02/15/gcn</id><content type="html" xml:base="http://localhost:4000/deep%20learning/2021/02/15/gcn/">&lt;p&gt;지난 GNN 포스팅&amp;lt;&lt;a href=&quot;https://ralasun.github.io/deep%20learning/2021/02/11/gcn/&quot;&gt;Introduction to Graph Neural Network - GNN 소개 및 개념&lt;/a&gt;&amp;gt;에서 graph neural network의 전반적인 개념에 대해 소개하였습니다. 이번 포스팅은 graph neural network가 더욱 유명해진 계기가 된 &lt;a href=&quot;https://arxiv.org/abs/1609.02907&quot;&gt;Kipf. et, al.의 Graph Convolutional Neural Network&lt;/a&gt;에 대해 다루도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;Kipf. et al.의 GCN을 이해하기 위해서는 먼저, spectral graph convolution에서부터 시작해야 합니다. 그러나 :spectral” 이라는 부분이 생소하신 분들이 많을 거라 생각됩니다. 반면에 일반적인 CNN 동작 방식은 많이 알려져 있습니다. 일반적인 CNN 동작은 spatial convolution입니다. 따라서 이를 유사하게 graph에 적용하는 방식을 spatial graph convolution입니다. 따라서, 이번 포스팅에서는 spectral 방식과 spatial 방식을 비교하고, spectral graph convolution에 대해 자세히 설명한 뒤에 Kipf. et al의 Graph Convolutional Network에 대해 다루도록 하겠습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1&gt;Spatial Graph Convolution vs. &lt;br /&gt;Spectral Graph Convolution&lt;/h1&gt;

&lt;p&gt;Graph convolution은 크게 2가지 방법이 있습니다. Spatial graph convolution과 Spectral graph convolution입니다. Spatial graph convolution은 convolution 연산을 graph위에서 직접 수행하는 방식으로, 각 노드와 가깝게 연결된 이웃 노드들에 한해서 convolution 연산을 수행합니다. 즉, 노드와 이웃노드들을 특정 grid form으로 재배열하여 convolution 연산을 수행하는 것입니다. 그러나, 우리가 일반적으로 아는 CNN의 filter는 고정된 사이즈를 가집니다(그림 1.).&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/S5B1k.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 1. CNN operation with fixed-size filter(3x3)&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;따라서, &lt;b&gt;&lt;i&gt;spatial graph convolution 방식의 관건은 고정된 크기의 이웃 노드를 선택하는 것입니다.&lt;/i&gt;&lt;/b&gt; 뿐만 아니라, CNN의 특징 중 하나는 “local invariance” 입니다. 입력의 위치가 바뀌어도 출력은 동일함을 의미합니다. 즉, 이미지 내의 강아지 위치가 달라도 CNN은 강아지라는 아웃풋을 출력함을 의미합니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://miro.medium.com/max/1400/1*HUJ3-xs3nUv-wY_GTBVUMg.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 2. Local invariance&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;따라서, &lt;b&gt;&lt;i&gt;Spatial graph convolution의 또다른 관건은 바로 “local invariance”를 유지를 해야한다는 것입니다.&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;앞에서 언급한 spatial graph convolution이 다뤄야 할 문제점과 별개로 또다른 문제점이 존재합니다. &lt;b&gt;Spatial graph convolution은 고정된 이웃 노드에서만 정보는 받아서 노드의 정보를 업데이트를 한다는 점입니다.&lt;/b&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img width=&quot;300&quot; src=&quot;https://imgur.com/KWmqbgk.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 3. Select neighborhood of red node&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;그러나, 그래프에서의 한 노드의 정보는 시간에 따라 여러 노드들의 정보의 혼합으로 표현될 수 있습니다. &amp;lt;그림 4.&amp;gt;를 살펴보도록 하겠습니다. 1번노드의 t=0일 때 정보는 [1,-1] 이지만 시간에 따라 여러 노드들의 정보(노드들의 signal)들이 밀려 들어오게 됩니다. 즉, 고정된 이웃노드 말고도 멀리 연결되어 있는 노드의 정보도 시간이 흐르면서 밀려 들어올 수 있는 것입니다. 이를 노드 간 message passing이라 합니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/Fv2FJbC.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 4. Message Passing in graph&lt;/figcaption&gt;
&lt;/p&gt;

&lt;p&gt;즉, 한 노드의 정보는 여러 노드의 signal이 혼재해 있는 것으로, 이를 time domain이 아닌 frequency 도메인으로 분석한다면, 한 노드 내에 혼재된 signal들을 여러 signal의 요소로 나눠서 node의 특징을 더 잘 추출할 수 있습니다. 이것에 관한 것이 바로 “Spectral Graph Convolution”입니다. Spectral graph convolution은 spectral 영역에서 convolution을 수행하는 것입니다. 이에 대해 자세히 살펴보도록 하겠습니다.&lt;/p&gt;

&lt;h1&gt;Dive into Spectral Graph Convolution&lt;/h1&gt;

&lt;p&gt;Signal Processing 분야에서 “spectral analysis”라는 것은 이미지/음성/그래프 신호(signal)을 time/spatial domain이 아니라 frequency domain으로 바꿔서 분석을 진행하는 것입니다. 즉, &lt;em&gt;어떤 특정 신호를 단순한 요소의 합으로 분해하는 것&lt;/em&gt;을 의미합니다. 대표적으로 이를 수행할 수 있는 방법이 푸리에 변환(Fourier Transform)입니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;spectral analysis에서 입력 신호가 전파/음성신호면 time domain을 frequency domain으로 변환하는 것이고, 컴퓨터 비전/그래프/영상처리 분야이면 spatial domain을 frequency domain으로 변환하는 것입니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;푸리에 변환이란, &lt;span style=&quot;text-decoration: underline&quot;&gt;&lt;b&gt;임의의 입력 신호를 다양한 주파수를 갖는 주기함수들의 합으로 분해하여 표현&lt;/b&gt;&lt;/span&gt;하는 것입니다. 아래 그림처럼 빨간색 신호를 파란색의 주기함수들의 성분으로 나누는 작업이 바로 푸리에 변환입니다. 즉, 파란색 주기함수들을 합하면 결국 빨간색 신호가 되는 것입니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/9967FA3359B63D8122&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 5. 푸리에 변환&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;그렇다면 graph signal에서의 푸리에 변환은 어떤 걸까요 ?&lt;/p&gt;

&lt;p&gt;결론부터 얘기하면, &lt;span style=&quot;text-decoration: underline&quot;&gt;&lt;b&gt;graph signal의 푸리에 변환은 graph의 Laplacian matrix를 eigen-decomposition하는 것&lt;/b&gt;&lt;/span&gt;입니다. 아래에서 수식과 함께 자세히 살펴보도록 하겠습니다.&lt;/p&gt;

&lt;h3&gt;Fourier transform&lt;/h3&gt;

&lt;p&gt;먼저, 푸리에 변환 식에 대해서 살펴봅시다. &lt;span style=&quot;color:gray&quot;&gt;&lt;del&gt;저도 푸리에 변환에 대한 이해가 아직 한없이 부족합니다. 최대한 공부하고 이해한 내용을 풀어볼려고 노력하였습니다.&lt;/del&gt;&lt;/span&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{f}(\xi) = \int_{\mathbf{R}^d} f(x)e^{2\pi ix\xi} \,dx \tag{1}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x) = \int_{\mathbf{R}^d} \hat{f}(\xi) e^{-2\pi ix\xi} \,d\xi \tag{2}&lt;/script&gt;

&lt;p&gt;(1)은 f의 푸리에 변환이고, (2)는 푸리에 역변환입니다. 푸리에 변환은 위에서 설명드린 것처럼, time domain을 frequency domain으로 변환한 것으로, 다양한 주파수를 갖는 주기함수의 합입니다. 그렇다면, 푸리에 역변환은 frequency domain의 함수를 다시 time domain으로 변환하는 것입니다. 푸리에 변환을 바라보는 관점은 여러가지가 존재하지만 그 중 하나는 ‘내적’입니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&quot;임의의 주파수 $f(x)$ 에 대하여, $\hat{f}(\xi)$ 는 $f(x)$ 와 $e^{-2\pi ix\xi}$ 의 내적&quot;&lt;/p&gt;

&lt;p&gt;‘내적’이 내포하고 있는 의미는 유사도입니다. 즉, “a와 b의 내적은 a와 b가 얼마나 닮았는가”를 뜻합니다. 결국 푸리에 변환은 다시 풀어쓰면 아래와 같은 의미를 가지고 있습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&quot;임의의 주파수 $f(x)$ 에 대하여, $\hat{f}(\xi)$ 는 $f(x)$ 와 $e^{-2\pi ix\xi}$ 가 얼마나 닮았는가&quot;&lt;/p&gt;

&lt;p&gt;그렇다면, $e^{-2\pi ix\xi}$ 의 의미는 무엇일까요 ? 이를 이해하기 위해선 ‘오일러 공식’이 필요합니다. 오일러 공식은 복소지수함수(complext exponential function)를 삼각함수(trigonometric function)로 표현하는 유명한 식입니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;e^{ix} = cost + isinx \tag{3}&lt;/script&gt;

&lt;p&gt;따라서, 오일러 공식에 의해 (1)식의 $e^{2\pi ix\xi}$ 부분을 cos요소와 sin요소의 합으로 표현할 수 있습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;e^{2\pi ix\xi} = cos(2\pi x\xi) + i sin(2\pi x\xi) \tag{4}&lt;/script&gt;

&lt;p&gt;즉, 주어진 주파수 f(x)에 대해 cosine에서 유사한 정도와 sine과 유사한 정도의 합이 푸리에 변환이라고 생각할 수 있습니다.&lt;/p&gt;

&lt;p&gt;이번엔 푸리에 변환의 선형대수(linear algebra)적인 의미를 살펴보도록 하겠습니다. 선형 대수에서, 벡터 $a \in R^d$ 를 d차원의 orthonormal basis를 찾을 수 있다면, 벡터 $a$ 를 orhonormal basis의 선형결합으로 표현할 수 있습니다. 이 orthonormal basis를 찾는 방법 중 하나가 바로 Eigen-value decomposition 입니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;orthonormal이란 서로 직교하면서 길이가 1인 벡터들을 의미합니다. 또한, 모든 matrix에 대해서 eigen-value decomposition 결과로 찾은 basis가 orthonormal은 아닙니다. 하지만 real-symmetric matrix에 대하여 구한 eigenvector들은 orthgonal한 관계입니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;다시 돌아와서, 푸리에 변환에서 주기함수 요소인 sine과 cosine에 대해 살펴봅시다. 아래와 같이 sine과 sine, sine과 cosine, cosine과 cosine을 내적하면 모두 다 0이 나옵니다. 이는 즉 삼각함수는 직교함을 알 수 있습니다(삼각함수의 직교성).&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/Bdo17jG.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 6. 삼각함수의 직교성&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;그렇다면, 선형대수 관점에서, &lt;span style=&quot;text-decoration: underline&quot;&gt;sine과 cosine 기저들의 선형결합이 즉 푸리에 변환이 되는 것&lt;/span&gt;입니다. 즉, &lt;span style=&quot;text-decoration:underline; color:red&quot;&gt;어떤 특정 signal이 real symmetric matrices이고 이들의 eigenvectors를 구할 수 있다면, eigenvector의 선형결합이 즉 해당 signal의 푸리에 변환&lt;/span&gt;임을 의미하는 것입니다.&lt;/p&gt;

&lt;h3&gt;Laplacian(Laplace Operator)&lt;/h3&gt;

&lt;p&gt;Graph laplacian을 보기 전에 Laplace Operator에 대해 살펴보도록 하겠습니다. Laplace operator는 differential operator로, 벡터 기울기의 발산(Divergence)을 의미합니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\triangle f= \triangledown \cdot \triangledown f = \triangledown^2 f&lt;/script&gt;

&lt;p&gt;$\triangledown f$ 는 $f$ 의 기울기를 의미하는 것으로 1차함수의 기울기처럼, 한 점에서의 변화하는 정도를 의미합니다. 이를 그림으로 나타나면 아래와 같습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2019-08-28_laplacian/noname01.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 7. Scalar 함수&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;&amp;lt;그림 7.&amp;gt;의 scalar 함수의 gradient는 아래와 같습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2019-08-28_laplacian/noname03.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 8. Scalar 함수의 gradient&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;&amp;lt;그림 7.&amp;gt;과 &amp;lt;그림 8.&amp;gt;을 보시면, (x,y)=(0,2) 부근에는 수렴하는 형태의 gradient가 형성되어 있고, (x,y)=(0,-2) 부근에는 발산하는 형태의 gradient가 형성되어 있습니다. Laplace 연산자를 이용해서 이 기울기의 발산 $\triangle f$ 을 구해주면, 아래와 같습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2019-08-28_laplacian/noname04.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 9. Divergence&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;divergence가 나타내는 의미, 즉, Laplace operator가 나타내는 의미는 무엇일까요 ? 이 함수의 높고 낮음을 표시하는 것입니다. 고등학교 때, 배운 2차 편미분과 비슷합니다. 이차 편미분 값이 양수면 아래로 볼록이고, 이차 편미분 값이 음수면 위로 볼록인 것과 유사합니다. 즉, 노란 부분일수록 양수 이기때문에 위로볼록인 모양이고, 파란부분일수록 음수값이기 때문에 아래로 볼록입니다.&lt;/p&gt;

&lt;p&gt;그렇다면, graph signal 영역에서 Laplace operator가 갖는 의미가 무엇일까요 ? 여기서 부터는 제 생각이지만, graph signal 영역에서 Laplace operator를 적용한다는 건, 한 노드에서의 signal의 흩어짐 정도, 즉, 흐르는 정도를 알 수 있습니다. 어떤 노드에서 signal이 들어왔을 때 그 signal이 어떤 방향으로 얼마만큼 빠르게 흩어지는지를 알 수 있고 이는 즉 그 노드의 특징이 될 수 있는 것입니다. 위의 그림을 예를 들어서 설명한다면, 만약에 한 signal이 그림 7.의 가장 높은 부분(노란색 부분)에서 시작된다면 가장 낮은 부분(파란색 부분)까지 빠른 속도로 흘러갈 것입니다.&lt;/p&gt;

&lt;p&gt;아래는 이와 관련된 gif이미지입니다. Grid 격자를 어떤 graph라고 생각한다면, 어떤 노드에서 signal이 들어왔을 때 흩어지는 양상을 보실 수 있습니다. 이 흩어지는 양상을 자세히 알기 위해서는 laplcian operator를 이용하여 계산하면 됩니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://miro.medium.com/max/1120/1*gz2hyrcSSJG9MtDzmQLe3w.gif&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 9. Diffusion of some signal in a regular grid graph based on the graph Laplacian&lt;/figcaption&gt;&lt;/p&gt;

&lt;h3&gt;Graph Laplacian&lt;/h3&gt;

&lt;p&gt;그러나, 이제까지 설명한 laplacian operator는 지난 포스팅에서 언급한 Laplacian matrix랑 무슨 관련이 있는 걸까요? 이름이 비슷한 걸 보니, laplacian matrix도 어떤 differential operator, 즉 ‘변화’에 관한 행렬임을 짐작할 수 있습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1609.02907&quot;&gt;Kipf, Thomas N., and Max Welling. “Semi-supervised classification with graph convolutional networks.” arXiv preprint arXiv:1609.02907 (2016).&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1812.08434&quot;&gt;Zhou, Jie, et al. “Graph neural networks: A review of methods and applications.” arXiv preprint arXiv:1812.08434 (2018).&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://dsba.korea.ac.kr/seminar/?mod=document&amp;amp;pageid=1&amp;amp;keyword=spectral&amp;amp;uid=1330&quot;&gt;DSBA 연구실 세미나 자료, [Paper Review] MultiSAGE - Spatial GCN with Contextual Embedding&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;푸리에 변환 참고 페이지
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://darkpgmr.tistory.com/171&quot;&gt;https://darkpgmr.tistory.com/171&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.math.ucla.edu/~tao/preprints/fourier.pdf&quot;&gt;https://www.math.ucla.edu/~tao/preprints/fourier.pdf&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://angeloyeo.github.io/2019/10/11/Fourier_Phase.html&quot;&gt;https://angeloyeo.github.io/2019/10/11/Fourier_Phase.html&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Laplacian Operator
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://angeloyeo.github.io/2019/08/25/laplacian.html&quot;&gt;https://angeloyeo.github.io/2019/08/28/laplacian.html&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/spectral-graph-convolution-explained-and-implemented-step-by-step-2e495b57f801&quot;&gt;https://towardsdatascience.com/spectral-graph-convolution-explained-and-implemented-step-by-step-2e495b57f801&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name>Seonhwa Lee</name></author><category term="Deep Learning" /><category term="graph-neural-network" /><summary type="html">지난 GNN 포스팅&amp;lt;Introduction to Graph Neural Network - GNN 소개 및 개념&amp;gt;에서 graph neural network의 전반적인 개념에 대해 소개하였습니다. 이번 포스팅은 graph neural network가 더욱 유명해진 계기가 된 Kipf. et, al.의 Graph Convolutional Neural Network에 대해 다루도록 하겠습니다. Kipf. et al.의 GCN을 이해하기 위해서는 먼저, spectral graph convolution에서부터 시작해야 합니다. 그러나 :spectral” 이라는 부분이 생소하신 분들이 많을 거라 생각됩니다. 반면에 일반적인 CNN 동작 방식은 많이 알려져 있습니다. 일반적인 CNN 동작은 spatial convolution입니다. 따라서 이를 유사하게 graph에 적용하는 방식을 spatial graph convolution입니다. 따라서, 이번 포스팅에서는 spectral 방식과 spatial 방식을 비교하고, spectral graph convolution에 대해 자세히 설명한 뒤에 Kipf. et al의 Graph Convolutional Network에 대해 다루도록 하겠습니다. Spatial Graph Convolution vs. Spectral Graph Convolution Graph convolution은 크게 2가지 방법이 있습니다. Spatial graph convolution과 Spectral graph convolution입니다. Spatial graph convolution은 convolution 연산을 graph위에서 직접 수행하는 방식으로, 각 노드와 가깝게 연결된 이웃 노드들에 한해서 convolution 연산을 수행합니다. 즉, 노드와 이웃노드들을 특정 grid form으로 재배열하여 convolution 연산을 수행하는 것입니다. 그러나, 우리가 일반적으로 아는 CNN의 filter는 고정된 사이즈를 가집니다(그림 1.). 그림 1. CNN operation with fixed-size filter(3x3) 따라서, spatial graph convolution 방식의 관건은 고정된 크기의 이웃 노드를 선택하는 것입니다. 뿐만 아니라, CNN의 특징 중 하나는 “local invariance” 입니다. 입력의 위치가 바뀌어도 출력은 동일함을 의미합니다. 즉, 이미지 내의 강아지 위치가 달라도 CNN은 강아지라는 아웃풋을 출력함을 의미합니다. 그림 2. Local invariance 따라서, Spatial graph convolution의 또다른 관건은 바로 “local invariance”를 유지를 해야한다는 것입니다. 앞에서 언급한 spatial graph convolution이 다뤄야 할 문제점과 별개로 또다른 문제점이 존재합니다. Spatial graph convolution은 고정된 이웃 노드에서만 정보는 받아서 노드의 정보를 업데이트를 한다는 점입니다. 그림 3. Select neighborhood of red node 그러나, 그래프에서의 한 노드의 정보는 시간에 따라 여러 노드들의 정보의 혼합으로 표현될 수 있습니다. &amp;lt;그림 4.&amp;gt;를 살펴보도록 하겠습니다. 1번노드의 t=0일 때 정보는 [1,-1] 이지만 시간에 따라 여러 노드들의 정보(노드들의 signal)들이 밀려 들어오게 됩니다. 즉, 고정된 이웃노드 말고도 멀리 연결되어 있는 노드의 정보도 시간이 흐르면서 밀려 들어올 수 있는 것입니다. 이를 노드 간 message passing이라 합니다. 그림 4. Message Passing in graph 즉, 한 노드의 정보는 여러 노드의 signal이 혼재해 있는 것으로, 이를 time domain이 아닌 frequency 도메인으로 분석한다면, 한 노드 내에 혼재된 signal들을 여러 signal의 요소로 나눠서 node의 특징을 더 잘 추출할 수 있습니다. 이것에 관한 것이 바로 “Spectral Graph Convolution”입니다. Spectral graph convolution은 spectral 영역에서 convolution을 수행하는 것입니다. 이에 대해 자세히 살펴보도록 하겠습니다. Dive into Spectral Graph Convolution Signal Processing 분야에서 “spectral analysis”라는 것은 이미지/음성/그래프 신호(signal)을 time/spatial domain이 아니라 frequency domain으로 바꿔서 분석을 진행하는 것입니다. 즉, 어떤 특정 신호를 단순한 요소의 합으로 분해하는 것을 의미합니다. 대표적으로 이를 수행할 수 있는 방법이 푸리에 변환(Fourier Transform)입니다. spectral analysis에서 입력 신호가 전파/음성신호면 time domain을 frequency domain으로 변환하는 것이고, 컴퓨터 비전/그래프/영상처리 분야이면 spatial domain을 frequency domain으로 변환하는 것입니다. 푸리에 변환이란, 임의의 입력 신호를 다양한 주파수를 갖는 주기함수들의 합으로 분해하여 표현하는 것입니다. 아래 그림처럼 빨간색 신호를 파란색의 주기함수들의 성분으로 나누는 작업이 바로 푸리에 변환입니다. 즉, 파란색 주기함수들을 합하면 결국 빨간색 신호가 되는 것입니다. 그림 5. 푸리에 변환 그렇다면 graph signal에서의 푸리에 변환은 어떤 걸까요 ? 결론부터 얘기하면, graph signal의 푸리에 변환은 graph의 Laplacian matrix를 eigen-decomposition하는 것입니다. 아래에서 수식과 함께 자세히 살펴보도록 하겠습니다. Fourier transform 먼저, 푸리에 변환 식에 대해서 살펴봅시다. 저도 푸리에 변환에 대한 이해가 아직 한없이 부족합니다. 최대한 공부하고 이해한 내용을 풀어볼려고 노력하였습니다. (1)은 f의 푸리에 변환이고, (2)는 푸리에 역변환입니다. 푸리에 변환은 위에서 설명드린 것처럼, time domain을 frequency domain으로 변환한 것으로, 다양한 주파수를 갖는 주기함수의 합입니다. 그렇다면, 푸리에 역변환은 frequency domain의 함수를 다시 time domain으로 변환하는 것입니다. 푸리에 변환을 바라보는 관점은 여러가지가 존재하지만 그 중 하나는 ‘내적’입니다. &quot;임의의 주파수 $f(x)$ 에 대하여, $\hat{f}(\xi)$ 는 $f(x)$ 와 $e^{-2\pi ix\xi}$ 의 내적&quot; ‘내적’이 내포하고 있는 의미는 유사도입니다. 즉, “a와 b의 내적은 a와 b가 얼마나 닮았는가”를 뜻합니다. 결국 푸리에 변환은 다시 풀어쓰면 아래와 같은 의미를 가지고 있습니다. &quot;임의의 주파수 $f(x)$ 에 대하여, $\hat{f}(\xi)$ 는 $f(x)$ 와 $e^{-2\pi ix\xi}$ 가 얼마나 닮았는가&quot; 그렇다면, $e^{-2\pi ix\xi}$ 의 의미는 무엇일까요 ? 이를 이해하기 위해선 ‘오일러 공식’이 필요합니다. 오일러 공식은 복소지수함수(complext exponential function)를 삼각함수(trigonometric function)로 표현하는 유명한 식입니다. 따라서, 오일러 공식에 의해 (1)식의 $e^{2\pi ix\xi}$ 부분을 cos요소와 sin요소의 합으로 표현할 수 있습니다. 즉, 주어진 주파수 f(x)에 대해 cosine에서 유사한 정도와 sine과 유사한 정도의 합이 푸리에 변환이라고 생각할 수 있습니다. 이번엔 푸리에 변환의 선형대수(linear algebra)적인 의미를 살펴보도록 하겠습니다. 선형 대수에서, 벡터 $a \in R^d$ 를 d차원의 orthonormal basis를 찾을 수 있다면, 벡터 $a$ 를 orhonormal basis의 선형결합으로 표현할 수 있습니다. 이 orthonormal basis를 찾는 방법 중 하나가 바로 Eigen-value decomposition 입니다. orthonormal이란 서로 직교하면서 길이가 1인 벡터들을 의미합니다. 또한, 모든 matrix에 대해서 eigen-value decomposition 결과로 찾은 basis가 orthonormal은 아닙니다. 하지만 real-symmetric matrix에 대하여 구한 eigenvector들은 orthgonal한 관계입니다. 다시 돌아와서, 푸리에 변환에서 주기함수 요소인 sine과 cosine에 대해 살펴봅시다. 아래와 같이 sine과 sine, sine과 cosine, cosine과 cosine을 내적하면 모두 다 0이 나옵니다. 이는 즉 삼각함수는 직교함을 알 수 있습니다(삼각함수의 직교성). 그림 6. 삼각함수의 직교성 그렇다면, 선형대수 관점에서, sine과 cosine 기저들의 선형결합이 즉 푸리에 변환이 되는 것입니다. 즉, 어떤 특정 signal이 real symmetric matrices이고 이들의 eigenvectors를 구할 수 있다면, eigenvector의 선형결합이 즉 해당 signal의 푸리에 변환임을 의미하는 것입니다. Laplacian(Laplace Operator) Graph laplacian을 보기 전에 Laplace Operator에 대해 살펴보도록 하겠습니다. Laplace operator는 differential operator로, 벡터 기울기의 발산(Divergence)을 의미합니다. $\triangledown f$ 는 $f$ 의 기울기를 의미하는 것으로 1차함수의 기울기처럼, 한 점에서의 변화하는 정도를 의미합니다. 이를 그림으로 나타나면 아래와 같습니다. 그림 7. Scalar 함수 &amp;lt;그림 7.&amp;gt;의 scalar 함수의 gradient는 아래와 같습니다. 그림 8. Scalar 함수의 gradient &amp;lt;그림 7.&amp;gt;과 &amp;lt;그림 8.&amp;gt;을 보시면, (x,y)=(0,2) 부근에는 수렴하는 형태의 gradient가 형성되어 있고, (x,y)=(0,-2) 부근에는 발산하는 형태의 gradient가 형성되어 있습니다. Laplace 연산자를 이용해서 이 기울기의 발산 $\triangle f$ 을 구해주면, 아래와 같습니다. 그림 9. Divergence divergence가 나타내는 의미, 즉, Laplace operator가 나타내는 의미는 무엇일까요 ? 이 함수의 높고 낮음을 표시하는 것입니다. 고등학교 때, 배운 2차 편미분과 비슷합니다. 이차 편미분 값이 양수면 아래로 볼록이고, 이차 편미분 값이 음수면 위로 볼록인 것과 유사합니다. 즉, 노란 부분일수록 양수 이기때문에 위로볼록인 모양이고, 파란부분일수록 음수값이기 때문에 아래로 볼록입니다. 그렇다면, graph signal 영역에서 Laplace operator가 갖는 의미가 무엇일까요 ? 여기서 부터는 제 생각이지만, graph signal 영역에서 Laplace operator를 적용한다는 건, 한 노드에서의 signal의 흩어짐 정도, 즉, 흐르는 정도를 알 수 있습니다. 어떤 노드에서 signal이 들어왔을 때 그 signal이 어떤 방향으로 얼마만큼 빠르게 흩어지는지를 알 수 있고 이는 즉 그 노드의 특징이 될 수 있는 것입니다. 위의 그림을 예를 들어서 설명한다면, 만약에 한 signal이 그림 7.의 가장 높은 부분(노란색 부분)에서 시작된다면 가장 낮은 부분(파란색 부분)까지 빠른 속도로 흘러갈 것입니다. 아래는 이와 관련된 gif이미지입니다. Grid 격자를 어떤 graph라고 생각한다면, 어떤 노드에서 signal이 들어왔을 때 흩어지는 양상을 보실 수 있습니다. 이 흩어지는 양상을 자세히 알기 위해서는 laplcian operator를 이용하여 계산하면 됩니다. 그림 9. Diffusion of some signal in a regular grid graph based on the graph Laplacian Graph Laplacian 그러나, 이제까지 설명한 laplacian operator는 지난 포스팅에서 언급한 Laplacian matrix랑 무슨 관련이 있는 걸까요? 이름이 비슷한 걸 보니, laplacian matrix도 어떤 differential operator, 즉 ‘변화’에 관한 행렬임을 짐작할 수 있습니다. Kipf, Thomas N., and Max Welling. “Semi-supervised classification with graph convolutional networks.” arXiv preprint arXiv:1609.02907 (2016). Zhou, Jie, et al. “Graph neural networks: A review of methods and applications.” arXiv preprint arXiv:1812.08434 (2018). DSBA 연구실 세미나 자료, [Paper Review] MultiSAGE - Spatial GCN with Contextual Embedding 푸리에 변환 참고 페이지 https://darkpgmr.tistory.com/171 https://www.math.ucla.edu/~tao/preprints/fourier.pdf https://angeloyeo.github.io/2019/10/11/Fourier_Phase.html Laplacian Operator https://angeloyeo.github.io/2019/08/28/laplacian.html https://towardsdatascience.com/spectral-graph-convolution-explained-and-implemented-step-by-step-2e495b57f801</summary></entry><entry><title type="html">Introduction to Graph Neural Network - GNN 소개 및 개념</title><link href="http://localhost:4000/deep%20learning/2021/02/11/gcn/" rel="alternate" type="text/html" title="Introduction to Graph Neural Network - GNN 소개 및 개념" /><published>2021-02-11T00:00:00+09:00</published><updated>2021-02-11T00:00:00+09:00</updated><id>http://localhost:4000/deep%20learning/2021/02/11/gcn</id><content type="html" xml:base="http://localhost:4000/deep%20learning/2021/02/11/gcn/">&lt;p&gt;이번 포스팅을 시작으로, Graph Neural Network(GNN)에 대해 본격적으로 다루도록 하겠습니다. 이번 포스팅은 Graph Neural Network가 나온 배경 및 기본적인 구조와 개념에 대해 다루도록 하겠습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;우리가 흔히 많이 보는 데이터의 종류로는 이미지, 정형 데이터, 텍스트가 있습니다. 이미지는 2-D grid 형식인 격자 형식을 가지며, 정형 테이터는 테이블 형태를 띕니다. 또한 텍스트는 1-D sequence로 생각할 수 있습니다. 즉, 이들 데이터는 ‘격자’의 모양으로 표현할 수 있으며 이는 Euclidean space 상에 있는 것을 뜻합니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/0bBI5DP.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 1. Euclidean space vs. Non-Euclidean space&lt;/figcaption&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img width=&quot;300&quot; src=&quot;https://imgur.com/wsEg0pl.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 2. 3D mesh 이미지&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;그러나, social network 데이터, molecular 데이터, 3D mesh 이미지 데이터(그림 2.)는 ‘비’ Eculidean space 데이터입니다. 그렇다면 기존 CNN과 RNN계열의 모델과 다르게 이런 형태의 데이터를 처리할 수 있는 새로운 모델이 필요합니다. 그것이 바로 Graph Neural Network 입니다.&lt;/p&gt;

&lt;h1&gt;What is graph?&lt;/h1&gt;

&lt;p&gt;GNN을 본격적으로 시작하기 전에 그래프에 대해서 알아보도록 하겠습니다. 그래프란 $G = (N, E)$ 로 구성된 일종의 자료 구조입니다. V는 노드들의 집합이고, E는 노드 사이를 연결하는 엣지들의 집합입니다. 노드에는 일반적으로 데이터의 정보가 담겨있고, 엣지는 데이터 간의 관계 정보가 포함되어 있습니다. 또한, 아래와 같은 그래프 형태를 ‘undirected graph’ 라고도 합니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/rRWSycm.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 3. graph&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;directed graph&lt;/b&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img width=&quot;300&quot; src=&quot;https://imgur.com/HO2ho4k.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 4. directed graph&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;방향 그래프란 엣지가 방향성을 가지는 그래프입니다. 아래 그림에서 $V_2$ 에서 $V_1$ 으로 향하는 엣지 $e_1$ 이 있다면, $V_2$ 를 predecessor, $V_1$ 을 sucessor 라고 부릅니다. 그리고 $e_1$ 을 $V_2$ 의 outgoing edge, $V_1$ 의 incoming edge 라고 합니다.&lt;/p&gt;

&lt;p&gt;그렇다면, 이러한 그래프를 네트워크의 인풋으로 넣기 위해선 행렬 형태로 표현해야 합니다. 따라서 그래프를 표현하기 위한 방법으로는 adjacency matrix, degree matrix, laplacian matrix가 있습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/bYiaa4S.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 5. degree vs. adjacency vs. laplacian&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Adjacency matrix&lt;/b&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;adjacency 행렬은 그래프 노드의 개수가 N개라면, NxN 정사각 행렬입니다. i노드와 j노드가 연결되어 있으면 $A_{ij} = 1$ 아니면 $A_{ij} = 0$ 의 성분을 가집니다.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Degree matrix&lt;/b&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Degree 행렬은 그래프 노드의 개수가 N개라면 NxN 크기를 가지는 대각행렬입니다. 각 꼭짓점의 차수에 대한 정보를 포함하고 있는 행렬로, 꼭짓점의 차수란 꼭짓점와 연결된 엣지의 갯수를 말합니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;D_{i,j} = \begin{cases} deg(v_i) \quad if \,\, i=j \\
		                      0 \quad otherwise \end{cases}&lt;/script&gt;

&lt;p&gt;&lt;b&gt;Laplacian matrix&lt;/b&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;adjacency 행렬은 노드 자신에 대한 정보가 없습니다. 그에 반해 laplacian 행렬은 노드와 연결된 이웃노드와 자기 자신에 대한 정보가 모두 포함된 행렬입니다. laplacian 행렬은 degree 행렬에서 adjacency 행렬을 빼준 것입니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L = D - A&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L_{i,j} = \begin{cases}
		deg(v_i) \quad if \,\, i=j \\
		 -1 \quad if \,\, i \neq j \\
		  0 \quad otherwise \end{cases}&lt;/script&gt;

&lt;h1&gt;Motivation : GNN $\approx$ CNN&lt;/h1&gt;

&lt;p&gt;다시 GNN으로 돌아오겠습니다. GNN의 아이디어는 Convolutional Neural Network(CNN)에서 시작되었습니다. CNN은 아래와 같은 특징을 가지고 있습니다.&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;local connectivity&lt;/li&gt;
&lt;li&gt;shared weights&lt;/li&gt;
&lt;li&gt;use of Multi-layer&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;위와 같은 특징 때문에, CNN은 spatial feature를 계속해서 layer마다 계속해서 추출해 나가면서 고차원적인 특징을 표현할 수 있습니다. 위와 같은 특징은 마찬가지로 graph 영역에도 적용할 수 있습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/qa04Jf2.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 6. GNN $\approx$ CNN&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Local Connectivity&lt;/b&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&amp;lt;그림 3.&amp;gt; 을 보면, CNN과 GNN의 유사한 점을 확인할 수 있습니다. 먼저, graph도 한 노드와 이웃노드 간의 관계를 local connectivity라 볼 수 있기 때문에, 한 노드의 특징을 뽑기 위해서 local connection에 있는 이웃노드들의 정보만 받아서 특징을 추출할 수 있습니다. 즉, CNN의 filter의 역할과 유사합니다.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Shared Weights&lt;/b&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;또한 이렇게 graph 노드의 특징을 추출하는 weight은 다른 노드의 특징을 추출하는데도 동일한 가중치를 사용할 수 있어(shared weight), computational cost를 줄일 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Use of Multi-layer&lt;/b&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;CNN에서 multi layer 구조로 여러 레이어를 쌓게 되면 초반에는 low-level feature위주로 뽑고, 네트워크가 깊어질수록 high level feature를 뽑습니다. &lt;span style=&quot;color:red&quot;&gt;graph같은 경우에 multi-layer구조로 쌓게되면 초반 layer는 단순히 이웃노드 간의 관계에 대해서만 특징을 추출하지만, 네트워크가 깊어질수록 나와 간접적으로 연결된 노드의 영향력까지 고려된 특징을 추출할 수 있게 됩니다.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;그렇다면, 위와 같은 특성을 가지려면 GNN은 어떻게 인풋 그래프에 대하여 연산을 해야하는지 알아보도록 하겠습니다.&lt;/p&gt;

&lt;h1&gt;Original Graph Neural Network&lt;/h1&gt;

&lt;p&gt;graph neural network는 &lt;a href=&quot;https://www.infona.pl/resource/bwmeta1.element.ieee-art-000004700287&quot;&gt;Scarselli et al.의 The Graph Neural Network Model&lt;/a&gt;에서 처음 등장했습니다. GNN의 목적은 결국 이웃노드들 간의 정보를 이용해서 해당 노드를 잘 표현할 수 있는 특징 벡터를 잘 찾아내는 것입니다. 이렇게 찾아낸 특징 벡터를 통해 task를 수행할 수 있습니다(graph classification, node classification 등).&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img width=&quot;400&quot; src=&quot;https://imgur.com/eDqPQFW.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 7. GNN&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;GNN의 동작은 따라서 크게 두가지로 생각할 수 있습니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;propagation step - 이웃노드들의 정보를 받아서 현재 자신 노드의 상태를 업데이트 함&lt;/li&gt;
  &lt;li&gt;output step - task 수행을 위해 노드 벡터에서 task output를 출력함&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;이를 수식으로 표현하면 아래와 같습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_n = f_w(l_n, l_{co[n]}, x_{ne[n]}, l_{ne[n]})&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;o_n = g_w(x_n, l_n)&lt;/script&gt;

&lt;p&gt;이때, $l_n, l_{co[n]}, x_{ne[n]}, l_{ne[n]}$ 은 각각 n 노드의 라벨, n노드와 연결된 엣지들의 라벨, 이웃노드들의 states, 이웃노드들의 라벨입니다. 또한 $f_w$ 와 $o_w$ 는 각각 propagation function(논문에선 transition function 이라 표현함)와 output function입니다.&lt;/p&gt;

&lt;p&gt;propagation function(transition function)은 이웃 노드들의 정보와 노드와 연결된 엣지정보들을 토대로 현재 자신의 노드를 표현합니다. 즉, d-차원의 공간에 이러한 인풋들을 받아서 맵핑하는 과정이라 생각할 수 있습니다. output function은 task 수행을 위해 학습을 통해 얻은 node feature을 입력으로 하여 output을 얻습니다. 예를 들어, node label classification 이라면 node label이 아웃풋이 될 것입니다.&lt;/p&gt;

&lt;h3&gt;Learning algorithm : Banach fixed point theorem&lt;/h3&gt;

&lt;p&gt;그렇다면 어떻게 학습이 이뤄질까요 ? 위에서 Motivation : GNN $\approx CNN$ 에서 Multi-layer를 GNN에 사용하면 얻는 이점은 layer가 깊어질수록 직접적으로 연결된 이웃 노드 이외에 멀리 있는 노드들의 영향력을 고려하여 현재 노드의 feature를 구성할 수 있다고 하였습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;이렇게 멀리있는 노드에서부터 현재 노드까지 정보가 전달되는 과정을 message passing이라고 합니다. message passing이란 개념은 GNN이 등장하고 난 이후에, Gilmer et al.의 “Neural message passing for quantumchemistry” 에서 등장하였습니다. 해당 논문은 여러 종류의 GNN 구조를 일반화하는 프레임워크를 message passing 이라는 것으로 제안한 논문입니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;하지만, 초기 GNN은 multi-layer 구조가 아니기 때문에 불가능합니다. 따라서, Banach fixed point theorem에 따라, iterative method로 고정된 node feature를 찾습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img width=&quot;400&quot; src=&quot;https://imgur.com/UIfPnoL.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 8. Network obtained by unfolding the encoding network&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;$x_n$ 와 $o_n$ 이 어떤 수렴된 값을 가지려면, Banach fixed point theorem에 의하면 propagation function이 contraction map이어야 합니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;$F_w$ is a &lt;em&gt;contraction map&lt;/em&gt; with respect to the state, i.e., there exists $\mu$ , $0 \leq \mu \le 1$ , such that $|F_w(x, l) - F_w(y, l)| \leq \mu |x-y|$ holds for any x, y where $| \cdot |$ denotes a vectorial norm.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;b&gt;Contraction Map에 대한 개인적인 생각&lt;/b&gt;&lt;br /&gt;
사실 contraction map의 수학적인 이해가 완벽하게 되진 않았습니다. 그러나, 제가 생각하는 contraction map은 다음과 같습니다. 선형대수학에서 선형변환을 진행하면, m차원의 벡터가 n차원의 공간으로 맵핑이 됩니다. 이 때, 서로 다른 두 m차원의 벡터가 n차원의 공간으로 맵핑이 되었을 때, 두 벡터 사이의 거리가 줄어드는 방향이라면 이 맵핑 function은 contraction map입니다.&lt;/p&gt;

&lt;p&gt;그렇다면 fixed point가 되려면, 즉 수렴된 node feature들은 contraction map에 의해 정의된 공간 안에서 존재하는 것이고, 어떻게 보면 node feature를 서치하는 범위가 작다라고 생각할 수 있습니다.&lt;/p&gt;

&lt;p&gt;이러한 문제 때문에 추후 다양한 버전의 GNN은 이러한 제한된 가정을 두지 않고 우리가 딥러닝 네트워크 학습시 사용하는 방식으로 node feature 값을 찾습니다. 즉, node feature의 search space가 훨씬 넓어지는 것입니다.&lt;/p&gt;

&lt;p&gt;다시 돌아와서, 그렇다면 iterative method식으로 수식을 전개하면 아래와 같이 전개할 수 있습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_n(t+1) = f_w(l_n, l_{co[n]}, x_{ne[n]}(t), l_{ne[n]})&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;o_n(t) = g_w(x_n(t), l_n), \quad n \in N&lt;/script&gt;

&lt;p&gt;fixed 된 $x_n, o_n$ 을 얻으면 아래와 같은 loss를 계산할 수 있고, gradient 계산을 통해 weight을 업데이트합니다. 여기서 weight은 $F_w$ 의 파라미터 입니다. neural network 라면 network의 가중치가 됩니다.&lt;/p&gt;

&lt;h1&gt;Variants of GNNs&lt;/h1&gt;

&lt;p&gt;Scarselli의 GNN 이후로 여러 변형된 GNN이 많이 등장하였습니다. 초기 GNN은 학습 방식의 단점에 의해 수렴이 잘 되지 않는다는 문제가 있습니다. 이러한 문제를 해결하기 위해 초기 GNN 이후에 다양한 GNN이 등장하였습니다. 대표적으로 Graph Convolutional Network와 Gated Graph Neural Network 등이 있습니다.&lt;/p&gt;

&lt;p&gt;다음 포스팅부터는 GNN이 더욱 더 유명해진 계기가 된 Graph Convolutional Network에 대해 다루도록 하겠습니다. 읽어주셔서 감사합니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1812.08434&quot;&gt;Zhou, Jie, et al. “Graph neural networks: A review of methods and applications.” arXiv preprint arXiv:1812.08434 (2018).&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;What is graph?, &lt;a href=&quot;https://ratsgo.github.io/data%20structure&amp;amp;algorithm/2017/11/18/graph/&quot;&gt;https://ratsgo.github.io/data%20structure&amp;amp;algorithm/2017/11/18/graph/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.infona.pl/resource/bwmeta1.element.ieee-art-000004700287&quot;&gt;Scarselli, F., et al. “The Graph Neural Network Model.” IEEE Transactions on Neural Networks 1.20 (2009): 61-80.&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Seonhwa Lee</name></author><category term="Deep Learning" /><category term="graph-neural-network" /><summary type="html">이번 포스팅을 시작으로, Graph Neural Network(GNN)에 대해 본격적으로 다루도록 하겠습니다. 이번 포스팅은 Graph Neural Network가 나온 배경 및 기본적인 구조와 개념에 대해 다루도록 하겠습니다. 우리가 흔히 많이 보는 데이터의 종류로는 이미지, 정형 데이터, 텍스트가 있습니다. 이미지는 2-D grid 형식인 격자 형식을 가지며, 정형 테이터는 테이블 형태를 띕니다. 또한 텍스트는 1-D sequence로 생각할 수 있습니다. 즉, 이들 데이터는 ‘격자’의 모양으로 표현할 수 있으며 이는 Euclidean space 상에 있는 것을 뜻합니다. 그림 1. Euclidean space vs. Non-Euclidean space 그림 2. 3D mesh 이미지 그러나, social network 데이터, molecular 데이터, 3D mesh 이미지 데이터(그림 2.)는 ‘비’ Eculidean space 데이터입니다. 그렇다면 기존 CNN과 RNN계열의 모델과 다르게 이런 형태의 데이터를 처리할 수 있는 새로운 모델이 필요합니다. 그것이 바로 Graph Neural Network 입니다. What is graph? GNN을 본격적으로 시작하기 전에 그래프에 대해서 알아보도록 하겠습니다. 그래프란 $G = (N, E)$ 로 구성된 일종의 자료 구조입니다. V는 노드들의 집합이고, E는 노드 사이를 연결하는 엣지들의 집합입니다. 노드에는 일반적으로 데이터의 정보가 담겨있고, 엣지는 데이터 간의 관계 정보가 포함되어 있습니다. 또한, 아래와 같은 그래프 형태를 ‘undirected graph’ 라고도 합니다. 그림 3. graph directed graph 그림 4. directed graph 방향 그래프란 엣지가 방향성을 가지는 그래프입니다. 아래 그림에서 $V_2$ 에서 $V_1$ 으로 향하는 엣지 $e_1$ 이 있다면, $V_2$ 를 predecessor, $V_1$ 을 sucessor 라고 부릅니다. 그리고 $e_1$ 을 $V_2$ 의 outgoing edge, $V_1$ 의 incoming edge 라고 합니다. 그렇다면, 이러한 그래프를 네트워크의 인풋으로 넣기 위해선 행렬 형태로 표현해야 합니다. 따라서 그래프를 표현하기 위한 방법으로는 adjacency matrix, degree matrix, laplacian matrix가 있습니다. 그림 5. degree vs. adjacency vs. laplacian Adjacency matrix adjacency 행렬은 그래프 노드의 개수가 N개라면, NxN 정사각 행렬입니다. i노드와 j노드가 연결되어 있으면 $A_{ij} = 1$ 아니면 $A_{ij} = 0$ 의 성분을 가집니다. Degree matrix Degree 행렬은 그래프 노드의 개수가 N개라면 NxN 크기를 가지는 대각행렬입니다. 각 꼭짓점의 차수에 대한 정보를 포함하고 있는 행렬로, 꼭짓점의 차수란 꼭짓점와 연결된 엣지의 갯수를 말합니다. Laplacian matrix adjacency 행렬은 노드 자신에 대한 정보가 없습니다. 그에 반해 laplacian 행렬은 노드와 연결된 이웃노드와 자기 자신에 대한 정보가 모두 포함된 행렬입니다. laplacian 행렬은 degree 행렬에서 adjacency 행렬을 빼준 것입니다. Motivation : GNN $\approx$ CNN 다시 GNN으로 돌아오겠습니다. GNN의 아이디어는 Convolutional Neural Network(CNN)에서 시작되었습니다. CNN은 아래와 같은 특징을 가지고 있습니다. local connectivity shared weights use of Multi-layer 위와 같은 특징 때문에, CNN은 spatial feature를 계속해서 layer마다 계속해서 추출해 나가면서 고차원적인 특징을 표현할 수 있습니다. 위와 같은 특징은 마찬가지로 graph 영역에도 적용할 수 있습니다. 그림 6. GNN $\approx$ CNN Local Connectivity &amp;lt;그림 3.&amp;gt; 을 보면, CNN과 GNN의 유사한 점을 확인할 수 있습니다. 먼저, graph도 한 노드와 이웃노드 간의 관계를 local connectivity라 볼 수 있기 때문에, 한 노드의 특징을 뽑기 위해서 local connection에 있는 이웃노드들의 정보만 받아서 특징을 추출할 수 있습니다. 즉, CNN의 filter의 역할과 유사합니다. Shared Weights 또한 이렇게 graph 노드의 특징을 추출하는 weight은 다른 노드의 특징을 추출하는데도 동일한 가중치를 사용할 수 있어(shared weight), computational cost를 줄일 수 있습니다. Use of Multi-layer CNN에서 multi layer 구조로 여러 레이어를 쌓게 되면 초반에는 low-level feature위주로 뽑고, 네트워크가 깊어질수록 high level feature를 뽑습니다. graph같은 경우에 multi-layer구조로 쌓게되면 초반 layer는 단순히 이웃노드 간의 관계에 대해서만 특징을 추출하지만, 네트워크가 깊어질수록 나와 간접적으로 연결된 노드의 영향력까지 고려된 특징을 추출할 수 있게 됩니다. 그렇다면, 위와 같은 특성을 가지려면 GNN은 어떻게 인풋 그래프에 대하여 연산을 해야하는지 알아보도록 하겠습니다. Original Graph Neural Network graph neural network는 Scarselli et al.의 The Graph Neural Network Model에서 처음 등장했습니다. GNN의 목적은 결국 이웃노드들 간의 정보를 이용해서 해당 노드를 잘 표현할 수 있는 특징 벡터를 잘 찾아내는 것입니다. 이렇게 찾아낸 특징 벡터를 통해 task를 수행할 수 있습니다(graph classification, node classification 등). 그림 7. GNN GNN의 동작은 따라서 크게 두가지로 생각할 수 있습니다. propagation step - 이웃노드들의 정보를 받아서 현재 자신 노드의 상태를 업데이트 함 output step - task 수행을 위해 노드 벡터에서 task output를 출력함 이를 수식으로 표현하면 아래와 같습니다. 이때, $l_n, l_{co[n]}, x_{ne[n]}, l_{ne[n]}$ 은 각각 n 노드의 라벨, n노드와 연결된 엣지들의 라벨, 이웃노드들의 states, 이웃노드들의 라벨입니다. 또한 $f_w$ 와 $o_w$ 는 각각 propagation function(논문에선 transition function 이라 표현함)와 output function입니다. propagation function(transition function)은 이웃 노드들의 정보와 노드와 연결된 엣지정보들을 토대로 현재 자신의 노드를 표현합니다. 즉, d-차원의 공간에 이러한 인풋들을 받아서 맵핑하는 과정이라 생각할 수 있습니다. output function은 task 수행을 위해 학습을 통해 얻은 node feature을 입력으로 하여 output을 얻습니다. 예를 들어, node label classification 이라면 node label이 아웃풋이 될 것입니다. Learning algorithm : Banach fixed point theorem 그렇다면 어떻게 학습이 이뤄질까요 ? 위에서 Motivation : GNN $\approx CNN$ 에서 Multi-layer를 GNN에 사용하면 얻는 이점은 layer가 깊어질수록 직접적으로 연결된 이웃 노드 이외에 멀리 있는 노드들의 영향력을 고려하여 현재 노드의 feature를 구성할 수 있다고 하였습니다. 이렇게 멀리있는 노드에서부터 현재 노드까지 정보가 전달되는 과정을 message passing이라고 합니다. message passing이란 개념은 GNN이 등장하고 난 이후에, Gilmer et al.의 “Neural message passing for quantumchemistry” 에서 등장하였습니다. 해당 논문은 여러 종류의 GNN 구조를 일반화하는 프레임워크를 message passing 이라는 것으로 제안한 논문입니다. 하지만, 초기 GNN은 multi-layer 구조가 아니기 때문에 불가능합니다. 따라서, Banach fixed point theorem에 따라, iterative method로 고정된 node feature를 찾습니다. 그림 8. Network obtained by unfolding the encoding network $x_n$ 와 $o_n$ 이 어떤 수렴된 값을 가지려면, Banach fixed point theorem에 의하면 propagation function이 contraction map이어야 합니다. $F_w$ is a contraction map with respect to the state, i.e., there exists $\mu$ , $0 \leq \mu \le 1$ , such that $|F_w(x, l) - F_w(y, l)| \leq \mu |x-y|$ holds for any x, y where $| \cdot |$ denotes a vectorial norm. Contraction Map에 대한 개인적인 생각 사실 contraction map의 수학적인 이해가 완벽하게 되진 않았습니다. 그러나, 제가 생각하는 contraction map은 다음과 같습니다. 선형대수학에서 선형변환을 진행하면, m차원의 벡터가 n차원의 공간으로 맵핑이 됩니다. 이 때, 서로 다른 두 m차원의 벡터가 n차원의 공간으로 맵핑이 되었을 때, 두 벡터 사이의 거리가 줄어드는 방향이라면 이 맵핑 function은 contraction map입니다. 그렇다면 fixed point가 되려면, 즉 수렴된 node feature들은 contraction map에 의해 정의된 공간 안에서 존재하는 것이고, 어떻게 보면 node feature를 서치하는 범위가 작다라고 생각할 수 있습니다. 이러한 문제 때문에 추후 다양한 버전의 GNN은 이러한 제한된 가정을 두지 않고 우리가 딥러닝 네트워크 학습시 사용하는 방식으로 node feature 값을 찾습니다. 즉, node feature의 search space가 훨씬 넓어지는 것입니다. 다시 돌아와서, 그렇다면 iterative method식으로 수식을 전개하면 아래와 같이 전개할 수 있습니다. fixed 된 $x_n, o_n$ 을 얻으면 아래와 같은 loss를 계산할 수 있고, gradient 계산을 통해 weight을 업데이트합니다. 여기서 weight은 $F_w$ 의 파라미터 입니다. neural network 라면 network의 가중치가 됩니다. Variants of GNNs Scarselli의 GNN 이후로 여러 변형된 GNN이 많이 등장하였습니다. 초기 GNN은 학습 방식의 단점에 의해 수렴이 잘 되지 않는다는 문제가 있습니다. 이러한 문제를 해결하기 위해 초기 GNN 이후에 다양한 GNN이 등장하였습니다. 대표적으로 Graph Convolutional Network와 Gated Graph Neural Network 등이 있습니다. 다음 포스팅부터는 GNN이 더욱 더 유명해진 계기가 된 Graph Convolutional Network에 대해 다루도록 하겠습니다. 읽어주셔서 감사합니다. Zhou, Jie, et al. “Graph neural networks: A review of methods and applications.” arXiv preprint arXiv:1812.08434 (2018). What is graph?, https://ratsgo.github.io/data%20structure&amp;amp;algorithm/2017/11/18/graph/ Scarselli, F., et al. “The Graph Neural Network Model.” IEEE Transactions on Neural Networks 1.20 (2009): 61-80.</summary></entry><entry><title type="html">A Dual-Stage Attention-Based Recurrent Neural Network for Time-Series Prediction 논문 리뷰</title><link href="http://localhost:4000/deep%20learning/2021/01/18/dual-stage-attention/" rel="alternate" type="text/html" title="A Dual-Stage Attention-Based Recurrent Neural Network for Time-Series Prediction 논문 리뷰" /><published>2021-01-18T00:00:00+09:00</published><updated>2021-01-18T00:00:00+09:00</updated><id>http://localhost:4000/deep%20learning/2021/01/18/dual-stage-attention</id><content type="html" xml:base="http://localhost:4000/deep%20learning/2021/01/18/dual-stage-attention/">&lt;p&gt;A Dual-Stage Attention-Based Recurrent Neural Network는 다변량 시계열 예측 모델입니다(Multi-Variate Time Series Prediction). Bahdanau et al.의 &lt;a href=&quot;https://arxiv.org/abs/1409.0473&quot;&gt;Attention network 기반 시퀀스 모델&lt;/a&gt; 을 베이스로, 인코더 뿐만 아니라 디코더에도 Attention netowork를 이용해 예측을 위한 다변량 시계열 변수 간 상대적인 중요도와 타임 스텝 간 상대적인 중요도를 모두 고려한 모델입니다.&lt;/p&gt;

&lt;h2&gt;Problem&lt;/h2&gt;</content><author><name>Seonhwa Lee</name></author><category term="Deep Learning" /><category term="time-series-analysis" /><summary type="html">A Dual-Stage Attention-Based Recurrent Neural Network는 다변량 시계열 예측 모델입니다(Multi-Variate Time Series Prediction). Bahdanau et al.의 Attention network 기반 시퀀스 모델 을 베이스로, 인코더 뿐만 아니라 디코더에도 Attention netowork를 이용해 예측을 위한 다변량 시계열 변수 간 상대적인 중요도와 타임 스텝 간 상대적인 중요도를 모두 고려한 모델입니다. Problem</summary></entry><entry><title type="html">TADA, Trend Alignment with Dual-Attention Multi-Task Recurrent Neural Networks for Sales Prediction 논문 리뷰</title><link href="http://localhost:4000/deep%20learning/2021/01/18/tada/" rel="alternate" type="text/html" title="TADA, Trend Alignment with Dual-Attention Multi-Task Recurrent Neural Networks for Sales Prediction 논문 리뷰" /><published>2021-01-18T00:00:00+09:00</published><updated>2021-01-18T00:00:00+09:00</updated><id>http://localhost:4000/deep%20learning/2021/01/18/tada</id><content type="html" xml:base="http://localhost:4000/deep%20learning/2021/01/18/tada/">&lt;hr /&gt;

&lt;p&gt;다변량 시계열 예측 모델에 관한 논문으로, 다변량 시계열 데이터를 가지고 encoder-decoder RNN 모델 기반으로 dual-attention과 multi-task RNN으로 구성된 모델입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2&gt;Problem&lt;/h2&gt;
&lt;p&gt;다변량 시계열 예측을 위한 여러 통계 기반 모델링이 있으나, 판매량에 영향을 주는 변수들 간의 관계를 파악하기 어렵고, 이 변수들로 부터 의미있는 정보(contextual information)을 추출하는 건 더욱 어렵습니다. 예를 들어, 겨울의복은 날씨에 영향에 두드러지게 받지만, 일반적인 셔츠는 사계절내내 잘 입는 옷이기 때문에 겨울의복보단 계절의 영향을 덜 받습니다. 또한 소비자의 주관적인 선호도(브랜드 선호도, 상품 선호도 등)에 따라 상품 판매는 크게 또한 달라지게 됩니다. 따라서, 본 논문에서 주목하는 다변량 시계열 예측에서의 문제는 아래와 같이 크게 세가지입니다.&lt;/p&gt;

&lt;ol&gt;&lt;li&gt;how to fully capture the dynamic dependencies among multiple influential factors?&lt;br /&gt;
판매에 영향을 주는 여러 변수들 간의 관계는 시간에 따라 변할 가능성이 높습니다. 그렇다면 매 스텝마다 변수들 간의 관계를 어떻게 포착할 수 있을까요 ?&lt;/li&gt;&lt;li&gt;how can we possibly glean wisdoms form the past to compensate for the unpredictability of influential factors?&lt;br /&gt;이 변수들이 미래에 어떻게 변할지는 아무도 모릅니다. 그렇다면 과거 이 변수들의 정보만을 가지고 어떻게 미래를 눈여겨 볼 수 있는 정보를 추출할지는 생각해 봐야 합니다.&lt;/li&gt;
&lt;li&gt;how to align the upcoming trend with historical sales trends?&lt;br /&gt;현실 시계에서의 판매 트랜드는 전혀 규칙적이지 않습니다. 그렇다면 과거 판매 트렌드를 어떻게 하면 현실 트렌드와 연관지을 수 있을까요 ?&lt;/li&gt;&lt;/ol&gt;

&lt;h2&gt;TADA : Trend Alignment with Dual-Attention Multi-Task RNN&lt;/h2&gt;

&lt;h3&gt;Problem Formulation&lt;/h3&gt;

&lt;p&gt;본 논문에서 풀고자 하는 다변량 시계열 예측 문제는 아래와 같이 수학적으로 정의됩니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\{\hat{y_t\}}^{T+\triangle}_{t=T+1} = F({\{\mathbf x_t\}}^{T}_{t=1}, {\{y_t\}}^{T}_{t=1})&lt;/script&gt;

&lt;p&gt;$\mathbf x_t$ 는 influential factors로 판매량 이외의 변수(ex. 날씨, 브랜드, 상품인덱스 등)이고, $y_t$ 는 판매량 입니다.&lt;/p&gt;

&lt;h3&gt;TADA 모델 개요&lt;/h3&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/w09ZSHF.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 1. 모델 개요&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;위의 그림은 본 논문의 모델 개요입니다. 크게 아래와 같이 구성되어 있습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Multi-task based Encoder Structures&lt;/li&gt;
  &lt;li&gt;Dual-Attention based Decoder Structures
    &lt;ul&gt;
      &lt;li&gt;Attention got weighted decoder input mapping&lt;/li&gt;
      &lt;li&gt;attention for trend alignment&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;Multi-task based Encoder Structures&lt;/h4&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/leH0yfV.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 2. Multi-task based Encoder&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;influential factor의 semantic한 특징을 잘 추출한다면 분명 예측에 도움이 될 것입니다. 그러나 매 타임 스텝마다 어떻게 하면 판매량 예측에 도움될 semantic한 특징을 추출할 수 있을까요 ? 본 논문에서는 influential factor를 크게 intrinsic한 속성과 objective한 속성으로 나누어 LSTM을 이용한 인코딩을 각각 따로하였습니다. 이를 통해 각각 두 개의 LSTM(intrinsic LSTM, external LSTM)을 통해 각기 다른 semantic한 특징을 추출할 수 있습니다. 따라서, 위의 문제 정의는 아래와 같이 다시 정의될 수 있습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\{\hat{y_t\}}^{T+\triangle}_{t=T+1} = F({\{\mathbf x_t^{int}\}}^{T}_{t=1}, {\{\mathbf x_t^{ext}\}}^{T}_{t=1}, {\{y_t\}}^{T}_{t=1})&lt;/script&gt;

&lt;p&gt;intrinsic한 속성이란 브랜드, 카테고리, 가격등 상품과 관련된 것이고, objective한 속성은 날씨, 휴일유무, 할인등과 관련된 속성입니다. 아래 표는 논문에서 실험한 데이터의 intrinsic/objective 속성입니다.&lt;/p&gt;

&lt;p&gt;하지만 우리가 구하고 싶은 건 두 가지의 다른 semantic한 feature를 적절하게 결합하여 의미있는 &lt;strong&gt;contextual vector&lt;/strong&gt;를 만드는 것입니다. 따라서 또다른 LSTM 네트워크인 Synergic LSTM을 구축하여 joint representation을 학습합니다. 이때, Synergic LSTM에 입력으로 들어가는 건 각 타임스텝에 해당되는 $h_t^{int}$ 와 $h_t^{ext}$ 뿐만 아니라 판매량 $y_t$ 도 같이 joint space가 구축되도록 학습됩니다.&lt;/p&gt;

&lt;p&gt;먼저, 두 타입스텝 t에서의 두 개의 hidden state을 $h_t^{int}$ 와 $h_t^{ext}$ 이용하여 Synergic LSTM의 인풋인 $\mathbf X_t^{syn}$ 을 아래와 같이 계산합니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf X_t^{syn} = \mathbf W_{syn}[\mathbf h_t^{int};\mathbf h_t^{ext};y_t]&lt;/script&gt;

&lt;p&gt;그런 다음, intrinsic LSTM/external LSTM과 동일하게 각 타임스텝마다 두 정보가 결합되어 인코딩된 hidden stated인 $\mathbf h^{con}_t$ 를 계산합니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/ijwajF4.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 3. Multi-task based Encoder(2)&lt;/figcaption&gt;&lt;/p&gt;

&lt;h4&gt;Dual-Attention based Decoder Structures&lt;/h4&gt;
&lt;p&gt;Multi-task Encoder를 통해 과거 판매량 시계열 데이터를 인코딩하면 contextual vectors인 ${\mathbf h_t^{con}}^T_{t=1}$ 이 계산되어 나옵니다. $h_t^{con}$ 은 타임스텝 t까지의 시계열 데이터에 대한 contextual 정보를 품고 있습니다.&lt;/p&gt;

&lt;p&gt;LSTM decoder도 encoder와 유사하게 예측에 필요한 contextual vector $\mathbf d_t^{con}$ 을 생성합니다. 따라서, $T &amp;lt; t \leq T + \Delta$ 에 대해 decoder 수학식은 아래와 같습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf d_t^{con} = LSTM^{dec}(\mathbf x_t^{dec}, \mathbf d^{con}_{t-1})&lt;/script&gt;

&lt;p&gt;위 식에서 $\mathbf x_t^{dec}$ 는 attention weighted input입니다. 그러면 contextual vector가 어떻게 만들어지는지 보기 전에 attention weighted input 계산 과정을 살펴봅시다.&lt;/p&gt;

&lt;h5&gt;Attention for Weighted Decoder Input Mapping&lt;/h5&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/NvCkYMs.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 4. Attention for Weighted Decoder Input&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;Decoder에 입력될 Input은 encoder contextual vector들에서 각 디코터 타임 스텝에 필요한 정보를 적절하게 취하도록 하기 위해 attention 메카니즘을 통해 생성합니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf x_t^{dec} = \mathbf W_{dec}\left[\sum_{t'=1}^T \alpha_{tt'}^{int}\mathbf h_{t'}^{int};\sum_{t'=1}^T \alpha_{tt'}^{ext}\mathbf h_{t'}^{ext}\right] + \mathbf b_{dec}&lt;/script&gt;

&lt;p&gt;$\alpha_{tt’}^{int}$ 와 $\alpha_{tt’}^{ext}$ 는 어텐션 가중치를 의미합니다. 어텐션 가중치는 아래 과정을 통해 계산됩니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;e^{int}_{tt'} = \mathbf v^{\mathrm T}_{int}tanh(\mathbf M_{int}\mathbf d_{t-1}^{con} + \mathbf H_{int}\mathbf h_{t'}^{int})&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;e^{ext}_{tt'} = \mathbf v^{\mathrm T}_{ext}tanh(\mathbf M_{int}\mathbf d_{t-1}^{con} + \mathbf H_{ext}\mathbf h_{t'}^{ext})&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\alpha_{tt'}^{int} = \frac{exp(e_{tt'}^{int})}{\sum_{s=1}^{T}exp(e_{ts}^{int})}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\alpha_{tt'}^{ext} = \frac{exp(e_{tt'}^{ext})}{\sum_{s=1}^{T}exp(e_{ts}^{ext})}&lt;/script&gt;

&lt;p&gt;이때, $\sum_{t’=1}^{T}\alpha_{tt’}^{int} = \sum_{t’=1}^{T}\alpha_{tt’}^{ext} = 1$ 이 여야 합니다.&lt;/p&gt;

&lt;h5&gt;Attention for Trend Alignment&lt;/h5&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/riUczJ9.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 5. Attention for Trend Alignment&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;미래를 예측하기 위해선 과거의 trend 패턴을 안다면 좀 더 수월할 수 있습니다. 따라서, 미래에 예상되는 패턴과 유사한 패턴을 과거에서 찾는 작업을 attention을 통해 진행하는 과정을 본 논문에서 제안하였습니다. 그러나, 
일반적으로 attention 메카니즘은 현 타임스텝에서 아웃풋을 출력하기 위해 이전 hidden state들중에서 가장 align되는 정보를 선택합니다. 과거 정보들 중에서 &lt;strong&gt;미래의 트렌드와 유사한 트렌드 정보&lt;/strong&gt;를 선택적으로 이용하고 싶다면 전통적인 attention 메카니즘을 그대로 사용하기는 어렵습니다. 왜냐하면, 일반적인 데이터에선 trend외에 노이즈도 많이 포함하고 있기 때문입니다. 즉, 전체 데이터에 trend + noise라서 이전 모든 과거들에서 유사한 trend 패턴만을 집중하는 건 힘듭니다. 따라서 논문 저자는 아래와 같은 방법을 고안하였습니다.&lt;/p&gt;

&lt;p&gt;먼저, ${\mathbf h_t^{con}}_{t=1}^T$ 를 $\triangle$ 타임 스텝 크기에 해당되는  contextual vector를 이어붙여서 $\triangle$ -step trend vector를 생성합니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/yUmHRP0.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$\mathbf p_i$ 는 과거 시계열 데이터에서 $\triangle$ 간격에 해당되는 구간의 트렌드를 나타냅니다. $i$ 가 1씩 증가하므로, 마치 슬라이딩 윈도우 1씩 움직이면서 트렌드를 포착하는 것과 유사합니다.&lt;/p&gt;

&lt;p&gt;마찬가지 방식으로 decoder hidden state들을 이어 붙여 미래에 예상될 트렌드 정보를 생성합니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/I9C1wnQ.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;따라서, 그림5 처럼 과거에 생성된 트렌드 벡터과 미래 트렌드 벡터를 각각 내적하여 가장 큰 값에 해당되는 인덱스 i를 반환합니다. 내적값이 가장 크다는 것은 가장 유사함을 의미합니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;e_i^{trd} = \mathbf p_i^{\mathrm T} \tilde{\mathbf p}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;i' = argmax(e_i^{trd} , e_{i+1}^{trd},\dots, e_{T+\triangle -1}^{trd})&lt;/script&gt;

&lt;p&gt;그 다음 $\mathbf p_{i’}$ 내의 각 ${\mathbf d_t^{con}}$ 와 $\mathbf h_t^{con}$ 을 아래와 같은 계산과정을 거쳐서 $\tilde {\mathbf d}^{con}$ 을 생성합니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/haL8Udd.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$\tilde {\mathbf d}^{con}$ 은 타임스텝 t에서의 과거 유사한 트렌드 정보에 집중하여 생성된 aligned contextual vector 입니다.&lt;/p&gt;

&lt;h4&gt;Sales Prediction and Model Learning&lt;/h4&gt;

&lt;p&gt;위에서 생성된 aligned contextual vector ${\widetilde {\mathbf d_t}^{con}}$ 를 가지고 판매량을 예측합니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat y_t = \mathbf v_y^{\mathrm T} \mathbf {\widetilde d}^{con} + b_y&lt;/script&gt;

&lt;p&gt;$\hat y_t , \,\,(T+1 \leq t \leq T+\Delta)$ 는 타임스텝 T에서의 예측된 판매량입니다.&lt;/p&gt;

&lt;p&gt;본 논문에서 학습은 L2 regularization과 함께 Mean Squared Error를 minimize하였습니다.&lt;/p&gt;

&lt;h2&gt;Experiment and result.&lt;/h2&gt;

&lt;p&gt;전체적인 결과에 관한 건 논문을 참고 바랍니다. trend alignment 부분에 대한 결과를 살펴보면 과거 유사하다고 찾은 trend와 예측된 trend는 아래 그래프와 같이 나왔습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/F5w1W2o.png&quot; /&gt;&lt;/p&gt;
&lt;p&gt;보면 어느정도 과거 매치된 트렌드와 유사한 트렌드를 따르는 것을 확인할 수 있었습니다.&lt;/p&gt;

&lt;h2&gt;Lessons Learned&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;결과를 보면 어느정도 lag가 발생하는 것으로 보입니다.&lt;/li&gt;
  &lt;li&gt;trend에 대한 파악을 먼저하고 판매량 데이터 입력을 나중에 하면 어떨까?&lt;/li&gt;
  &lt;li&gt;dual-stage attention에서의 input attention 모듈과 multi-tasked encoder를 결합하는 건 어떨까 ??&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;이상으로 본 논문 리뷰를 마치겠습니다.&lt;/p&gt;

&lt;hr /&gt;
&lt;ol&gt;
  &lt;li&gt;Chen, Tong, et al. “Tada: trend alignment with dual-attention multi-task recurrent neural networks for sales prediction.” 2018 IEEE International Conference on Data Mining (ICDM). IEEE, 2018.&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Seonhwa Lee</name></author><category term="Deep Learning" /><category term="time-series-analysis" /><summary type="html">다변량 시계열 예측 모델에 관한 논문으로, 다변량 시계열 데이터를 가지고 encoder-decoder RNN 모델 기반으로 dual-attention과 multi-task RNN으로 구성된 모델입니다. Problem 다변량 시계열 예측을 위한 여러 통계 기반 모델링이 있으나, 판매량에 영향을 주는 변수들 간의 관계를 파악하기 어렵고, 이 변수들로 부터 의미있는 정보(contextual information)을 추출하는 건 더욱 어렵습니다. 예를 들어, 겨울의복은 날씨에 영향에 두드러지게 받지만, 일반적인 셔츠는 사계절내내 잘 입는 옷이기 때문에 겨울의복보단 계절의 영향을 덜 받습니다. 또한 소비자의 주관적인 선호도(브랜드 선호도, 상품 선호도 등)에 따라 상품 판매는 크게 또한 달라지게 됩니다. 따라서, 본 논문에서 주목하는 다변량 시계열 예측에서의 문제는 아래와 같이 크게 세가지입니다. how to fully capture the dynamic dependencies among multiple influential factors? 판매에 영향을 주는 여러 변수들 간의 관계는 시간에 따라 변할 가능성이 높습니다. 그렇다면 매 스텝마다 변수들 간의 관계를 어떻게 포착할 수 있을까요 ?how can we possibly glean wisdoms form the past to compensate for the unpredictability of influential factors?이 변수들이 미래에 어떻게 변할지는 아무도 모릅니다. 그렇다면 과거 이 변수들의 정보만을 가지고 어떻게 미래를 눈여겨 볼 수 있는 정보를 추출할지는 생각해 봐야 합니다. how to align the upcoming trend with historical sales trends?현실 시계에서의 판매 트랜드는 전혀 규칙적이지 않습니다. 그렇다면 과거 판매 트렌드를 어떻게 하면 현실 트렌드와 연관지을 수 있을까요 ? TADA : Trend Alignment with Dual-Attention Multi-Task RNN Problem Formulation 본 논문에서 풀고자 하는 다변량 시계열 예측 문제는 아래와 같이 수학적으로 정의됩니다. $\mathbf x_t$ 는 influential factors로 판매량 이외의 변수(ex. 날씨, 브랜드, 상품인덱스 등)이고, $y_t$ 는 판매량 입니다. TADA 모델 개요 그림 1. 모델 개요 위의 그림은 본 논문의 모델 개요입니다. 크게 아래와 같이 구성되어 있습니다. Multi-task based Encoder Structures Dual-Attention based Decoder Structures Attention got weighted decoder input mapping attention for trend alignment Multi-task based Encoder Structures 그림 2. Multi-task based Encoder influential factor의 semantic한 특징을 잘 추출한다면 분명 예측에 도움이 될 것입니다. 그러나 매 타임 스텝마다 어떻게 하면 판매량 예측에 도움될 semantic한 특징을 추출할 수 있을까요 ? 본 논문에서는 influential factor를 크게 intrinsic한 속성과 objective한 속성으로 나누어 LSTM을 이용한 인코딩을 각각 따로하였습니다. 이를 통해 각각 두 개의 LSTM(intrinsic LSTM, external LSTM)을 통해 각기 다른 semantic한 특징을 추출할 수 있습니다. 따라서, 위의 문제 정의는 아래와 같이 다시 정의될 수 있습니다. intrinsic한 속성이란 브랜드, 카테고리, 가격등 상품과 관련된 것이고, objective한 속성은 날씨, 휴일유무, 할인등과 관련된 속성입니다. 아래 표는 논문에서 실험한 데이터의 intrinsic/objective 속성입니다. 하지만 우리가 구하고 싶은 건 두 가지의 다른 semantic한 feature를 적절하게 결합하여 의미있는 contextual vector를 만드는 것입니다. 따라서 또다른 LSTM 네트워크인 Synergic LSTM을 구축하여 joint representation을 학습합니다. 이때, Synergic LSTM에 입력으로 들어가는 건 각 타임스텝에 해당되는 $h_t^{int}$ 와 $h_t^{ext}$ 뿐만 아니라 판매량 $y_t$ 도 같이 joint space가 구축되도록 학습됩니다. 먼저, 두 타입스텝 t에서의 두 개의 hidden state을 $h_t^{int}$ 와 $h_t^{ext}$ 이용하여 Synergic LSTM의 인풋인 $\mathbf X_t^{syn}$ 을 아래와 같이 계산합니다. 그런 다음, intrinsic LSTM/external LSTM과 동일하게 각 타임스텝마다 두 정보가 결합되어 인코딩된 hidden stated인 $\mathbf h^{con}_t$ 를 계산합니다. 그림 3. Multi-task based Encoder(2) Dual-Attention based Decoder Structures Multi-task Encoder를 통해 과거 판매량 시계열 데이터를 인코딩하면 contextual vectors인 ${\mathbf h_t^{con}}^T_{t=1}$ 이 계산되어 나옵니다. $h_t^{con}$ 은 타임스텝 t까지의 시계열 데이터에 대한 contextual 정보를 품고 있습니다. LSTM decoder도 encoder와 유사하게 예측에 필요한 contextual vector $\mathbf d_t^{con}$ 을 생성합니다. 따라서, $T &amp;lt; t \leq T + \Delta$ 에 대해 decoder 수학식은 아래와 같습니다. 위 식에서 $\mathbf x_t^{dec}$ 는 attention weighted input입니다. 그러면 contextual vector가 어떻게 만들어지는지 보기 전에 attention weighted input 계산 과정을 살펴봅시다. Attention for Weighted Decoder Input Mapping 그림 4. Attention for Weighted Decoder Input Decoder에 입력될 Input은 encoder contextual vector들에서 각 디코터 타임 스텝에 필요한 정보를 적절하게 취하도록 하기 위해 attention 메카니즘을 통해 생성합니다. $\alpha_{tt’}^{int}$ 와 $\alpha_{tt’}^{ext}$ 는 어텐션 가중치를 의미합니다. 어텐션 가중치는 아래 과정을 통해 계산됩니다. 이때, $\sum_{t’=1}^{T}\alpha_{tt’}^{int} = \sum_{t’=1}^{T}\alpha_{tt’}^{ext} = 1$ 이 여야 합니다. Attention for Trend Alignment 그림 5. Attention for Trend Alignment 미래를 예측하기 위해선 과거의 trend 패턴을 안다면 좀 더 수월할 수 있습니다. 따라서, 미래에 예상되는 패턴과 유사한 패턴을 과거에서 찾는 작업을 attention을 통해 진행하는 과정을 본 논문에서 제안하였습니다. 그러나, 일반적으로 attention 메카니즘은 현 타임스텝에서 아웃풋을 출력하기 위해 이전 hidden state들중에서 가장 align되는 정보를 선택합니다. 과거 정보들 중에서 미래의 트렌드와 유사한 트렌드 정보를 선택적으로 이용하고 싶다면 전통적인 attention 메카니즘을 그대로 사용하기는 어렵습니다. 왜냐하면, 일반적인 데이터에선 trend외에 노이즈도 많이 포함하고 있기 때문입니다. 즉, 전체 데이터에 trend + noise라서 이전 모든 과거들에서 유사한 trend 패턴만을 집중하는 건 힘듭니다. 따라서 논문 저자는 아래와 같은 방법을 고안하였습니다. 먼저, ${\mathbf h_t^{con}}_{t=1}^T$ 를 $\triangle$ 타임 스텝 크기에 해당되는 contextual vector를 이어붙여서 $\triangle$ -step trend vector를 생성합니다. $\mathbf p_i$ 는 과거 시계열 데이터에서 $\triangle$ 간격에 해당되는 구간의 트렌드를 나타냅니다. $i$ 가 1씩 증가하므로, 마치 슬라이딩 윈도우 1씩 움직이면서 트렌드를 포착하는 것과 유사합니다. 마찬가지 방식으로 decoder hidden state들을 이어 붙여 미래에 예상될 트렌드 정보를 생성합니다. 따라서, 그림5 처럼 과거에 생성된 트렌드 벡터과 미래 트렌드 벡터를 각각 내적하여 가장 큰 값에 해당되는 인덱스 i를 반환합니다. 내적값이 가장 크다는 것은 가장 유사함을 의미합니다. 그 다음 $\mathbf p_{i’}$ 내의 각 ${\mathbf d_t^{con}}$ 와 $\mathbf h_t^{con}$ 을 아래와 같은 계산과정을 거쳐서 $\tilde {\mathbf d}^{con}$ 을 생성합니다. $\tilde {\mathbf d}^{con}$ 은 타임스텝 t에서의 과거 유사한 트렌드 정보에 집중하여 생성된 aligned contextual vector 입니다. Sales Prediction and Model Learning 위에서 생성된 aligned contextual vector ${\widetilde {\mathbf d_t}^{con}}$ 를 가지고 판매량을 예측합니다. $\hat y_t , \,\,(T+1 \leq t \leq T+\Delta)$ 는 타임스텝 T에서의 예측된 판매량입니다. 본 논문에서 학습은 L2 regularization과 함께 Mean Squared Error를 minimize하였습니다. Experiment and result. 전체적인 결과에 관한 건 논문을 참고 바랍니다. trend alignment 부분에 대한 결과를 살펴보면 과거 유사하다고 찾은 trend와 예측된 trend는 아래 그래프와 같이 나왔습니다. 보면 어느정도 과거 매치된 트렌드와 유사한 트렌드를 따르는 것을 확인할 수 있었습니다. Lessons Learned 결과를 보면 어느정도 lag가 발생하는 것으로 보입니다. trend에 대한 파악을 먼저하고 판매량 데이터 입력을 나중에 하면 어떨까? dual-stage attention에서의 input attention 모듈과 multi-tasked encoder를 결합하는 건 어떨까 ?? 이상으로 본 논문 리뷰를 마치겠습니다. Chen, Tong, et al. “Tada: trend alignment with dual-attention multi-task recurrent neural networks for sales prediction.” 2018 IEEE International Conference on Data Mining (ICDM). IEEE, 2018.</summary></entry><entry><title type="html">RDD, Resilient Distributed Dataset에 대하여[3] - RDD액션, RDD 데이터 불러오기와 저장하기, 공유변수</title><link href="http://localhost:4000/spark%20programming/2021/01/07/rdd(3)/" rel="alternate" type="text/html" title="RDD, Resilient Distributed Dataset에 대하여[3] - RDD액션, RDD 데이터 불러오기와 저장하기, 공유변수" /><published>2021-01-07T00:00:00+09:00</published><updated>2021-01-07T00:00:00+09:00</updated><id>http://localhost:4000/spark%20programming/2021/01/07/rdd(3)</id><content type="html" xml:base="http://localhost:4000/spark%20programming/2021/01/07/rdd(3)/">&lt;p&gt;이번 포스팅은 지난 포스팅 &amp;lt;RDD, Resilient Distributed DataSet에 대하여[2] - RDD기본액션, RDD트랜스포메이션&amp;gt; 에 이어서 진행하도록 하겠습니다. 교재는 빅데이터 분석을 위한 스파크2 프로그래밍을 참고하였습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2&gt;2.1.6 RDD 액션&lt;/h2&gt;
&lt;p&gt;RDD트랜스포메이션 연산은 느긋한 평가(lazy evaluation) 또는 지연 계산 방식을 따릅니다. 이는 계산에 필요한 정보를 누적하다가 계산이 필요한 시점이 돼서야 계산을 수행하는 방식을 뜻합니다. 여기서 계산이 필요한 시점은 RDD 액션 메서드가 호출된 시점입니다. RDD 액션 메서드가 호출이 되어야 비로소 RDD 트랜스포메이션 연산이 수행되게 됩니다.&lt;/p&gt;

&lt;h3&gt;1. 출력 연산&lt;/h3&gt;
&lt;h4&gt;1.1. first&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;RDD 요소 중 ,첫번째 요소를 돌려줌&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize(range(50))
&amp;gt;&amp;gt;&amp;gt; result = rdd.first()
&amp;gt;&amp;gt;&amp;gt; print(result)
0
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;1.2. take&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;RDD 요소중, n번째까지 요소를 돌려줌&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize(range(50))
&amp;gt;&amp;gt;&amp;gt; result = rdd.take(5)
&amp;gt;&amp;gt;&amp;gt; print(result)
[0, 1, 2, 3, 4]
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;1.3. takeSample&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;지정된 크기의 sample을 추출해서 리스트, 배열 타입등으로 반환함&lt;/li&gt;
  &lt;li&gt;sample 메서드와의 차이점
    &lt;ul&gt;
      &lt;li&gt;sample 메서드는 RDD 트랜스포메이션 메서드이고, 크기를 지정할 수 없음.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;takeSample(withReplacement, num, seed=None)&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize(range(100))
&amp;gt;&amp;gt;&amp;gt; result = rdd.takeSample(False, 3)
&amp;gt;&amp;gt;&amp;gt; result
[55, 23, 45]
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;1.5. countByValue&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;RDD의 요소들이 나타낸 횟수를 맵 형태로 돌려주는 메서드&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize([1,1,3,2,1,2,2,1,1,4,5,3,2,3])
&amp;gt;&amp;gt;&amp;gt; result = rdd.countByValue()
&amp;gt;&amp;gt;&amp;gt; print(result)
defaultdict(&amp;lt;class 'int'&amp;gt;, {1: 5, 3: 3, 2: 4, 4: 1, 5: 1})
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;1.6. reduce&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;reduce 메서드 인자는 함수가 들어감.&lt;/li&gt;
  &lt;li&gt;그 함수는 교환법칙과 결합법칙이 성립하는 함수여야 함.&lt;/li&gt;
  &lt;li&gt;따라서, 메서드 인자로 받은 함수를 이용해서 하나의 요소로 합치는 메서드임.&lt;/li&gt;
  &lt;li&gt;def reduce(f: (T,T)=&amp;gt;T):T
    &lt;ul&gt;
      &lt;li&gt;동일한 타입 2개를 입력으로 받아, 같은 타입으로 반환해주는 메서드임&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;실제 구현은 파티션단위로 나눠져서 처리됨. 분산 프로그램이기 때문임.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; from operator import add
&amp;gt;&amp;gt;&amp;gt; add(1,2)
3
&amp;gt;&amp;gt;&amp;gt; sc.parallelize([1,2,3,4,5]).reduce(add)
15
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;1.7. fold&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;reduce와 동일하나, 초기값을 설정할 수 있음&lt;/li&gt;
  &lt;li&gt;def fold(zeroValue: T)(op: (T,T)=&amp;gt;T):T&lt;/li&gt;
  &lt;li&gt;그런데 유의할 점은 파티션단위로 나뉘어서 처리하기 때문에, 파티션단위로 처리할 때마다 초깃값을 이용하여 연산이 수행됨. 따라서, 더하기 연산을 할 땐 항등원인 0을, 곱셈 연산을 할 땐 항등원인 1을 초깃값으로 설정하는 것이 좋음&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize(range(1,11), 3)
&amp;gt;&amp;gt;&amp;gt; rdd.fold(1, add)
59 #값이 55가 아니라 59가 나오는 것을 확인할 수 있음. 
&lt;/code&gt;&lt;/pre&gt;

&lt;h5&gt;reduce와 fold차이&lt;/h5&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;#product.py
class Product:
    def __init__(self, price):
        self.price = price
        self.count = 1
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;def addPriceandCount(p1, p2):
    p1.price += p2.price
    p1.count += 1
    return p1 #return을 p1인 이유 --&amp;gt; 입력값과 출력값의 타입이 동일해야 함.

if __name__ =='__main__':
    conf = SparkConf()
    conf.set(&quot;spark.driver.host&quot;, &quot;127.0.0.1&quot;)
    sc = SparkContext(master=&quot;local[*]&quot;, appName=&quot;RDDOpSample&quot;, conf=conf)

    rdd = sc.parallelize([Product(300), Product(200), Product(100)], 10)

    #reduce
    result = rdd.reduce(addPriceandCount)
    print(result.price, result.count)

    #fold
    result = rdd.fold(Product(0), addPriceandCount)
    print(result.price, result.count)
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;fold의 count합을 보면 11인 것을 알 수 있음. 그 이유는 위에서 파티션 개수를 10으로 지정하였고, 파티션 단위로 연산을 초기값을 이용하여 연산을 수행하기 때문임&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;1.8. aggregate&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;입력와 출력의 타입이 다른 경우 사용 가능&lt;/li&gt;
  &lt;li&gt;def aggregate&lt;a href=&quot;zeroValue: U&quot;&gt;U&lt;/a&gt;(seqOp:(U,T)=&amp;gt;U, combOp:(U,U)=&amp;gt;U):U
    &lt;ul&gt;
      &lt;li&gt;크게 세가지 인자를 받음. 첫번째는 초깃값으로 fold와 동일&lt;/li&gt;
      &lt;li&gt;aggregate은 병합을 크게 2단계로 구성되는데, 1단계는 seqOp에 의해, 2단계는 combOp에 의해 진행됨&lt;/li&gt;
      &lt;li&gt;seqOp는 초깃값과 동일한 타입(U)과 RDD요소 타입(T)가 입력되어 병합 결과 초깃값과 동일한 타입인 U가 반환됨&lt;/li&gt;
      &lt;li&gt;combOp는 최종병합에서 사용됨&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;#rdd에 속한 요소들의 평균을 aggregate을 이용하여 구하는 예제
#record.py
class Record:

    def __init__(self, amount, number=1):
        self.amount = amount
        self.number = number

    def addAmt(self, amount):
        return Record(self.amount + amount, self.number + 1)

    def __add__(self, other):
        amount = self.amount + other.amount
        number = self.number + other.number
        return Record(amount, number)

    def __str__(self):
        return &quot;avg:&quot; + str(self.amount / self.number)

    def __repr__(self):
        return 'Record(%r, %r)' % (self.amount, self.number)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;def seqop(r,v):
    return r.addAmt(v)

if __name__ =='__main__':
    conf = SparkConf()
    conf.set(&quot;spark.driver.host&quot;, &quot;127.0.0.1&quot;)
    sc = SparkContext(master=&quot;local[*]&quot;, appName=&quot;RDDOpSample&quot;, conf=conf)

    rdd = sc.parallelize([100,80,75,90,95], 3)

    #aggregate
    result = rdd.aggregate(Record(0,0), seqop, lambda r1, r2:r1+r2)
    print(result) # avg:88.0
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;1.9. sum&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;모든 요소의 합을 구해주며, Double, Long등 숫자타입인 경우에만 사용가능&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize(range(1,11))
&amp;gt;&amp;gt;&amp;gt; rdd.sum()
55
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;1.10. foreach, foreachPartition&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;foreach는 RDD의 개별요소에 전달받은 함수를 적용하는 메서드이고, foreachPartition은 파티션 단위로 적용됨&lt;/li&gt;
  &lt;li&gt;이때 인자로 받는 함수는 한개의 입력값을 가지는 함수임&lt;/li&gt;
  &lt;li&gt;이 메서드를 사용할 때 유의할 점은 &lt;strong&gt;드라이버 프로그램(메인 함수를 포함하고 있는 프로그램)이 작동하고 있는 서버위가 아니라 클러스터의 각 개별 서버에서 실행된다는 것&lt;/strong&gt;임&lt;/li&gt;
  &lt;li&gt;따라서 foreach() 인자로 print함수를 전달한다는 것은 각 서버의 콘솔에 출력하라는 의미가 됨.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;def sideEffect(values):
    print(&quot;partition side effect&quot;)
    for v in values:
        print(&quot;value side effect : %s&quot; %v)

if __name__ =='__main__':
    conf = SparkConf()
    conf.set(&quot;spark.driver.host&quot;, &quot;127.0.0.1&quot;)
    sc = SparkContext(master=&quot;local[*]&quot;, appName=&quot;RDDOpSample&quot;, conf=conf)

    rdd = sc.parallelize(range(1,11),3)
    result = rdd.foreach(lambda v:print(&quot;value side effect: %s&quot; %v))
    result2 = rdd.foreachPartition(sideEffect)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;###
value side effect: 2
value side effect: 3
value side effect: 4
value side effect: 5
value side effect: 6
value side effect: 7
value side effect: 8
value side effect: 9
value side effect: 10
partition side effect
value side effect : 7
value side effect : 8
value side effect : 9
value side effect : 10
partition side effect
value side effect : 4
value side effect : 5
value side effect : 6
partition side effect
value side effect : 1
value side effect : 2
value side effect : 3
###
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;1.11. toDebugString&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;디버깅을 위한 메서드. RDD파티션 개수나 의존성 정보 등 세부 정보 알고 싶을 때 사용&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize(range(1,100), 10).persist().map(lambda v:(v,1)).coalesce(2)
&amp;gt;&amp;gt;&amp;gt; rdd.toDebugString()
b'(2) CoalescedRDD[65] at coalesce at NativeMethodAccessorImpl.java:0 []\n |  PythonRDD[64] at RDD at PythonRDD.scala:53 []\n |  PythonRDD[63] at RDD at PythonRDD.scala:53 []\n |  ParallelCollectionRDD[62] at parallelize at PythonRDD.scala:195 []'
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;1.12. cache, persist, unpersist&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;rdd액션 연산이 수행될때마다 RDD 생성 히스토리를 이용해 복구하는 단계를 수행하지만 너무나 번거로움&lt;/li&gt;
  &lt;li&gt;따라서 반복적으로 사용되는 RDD인 경우 메모리에 저장해서 사용함&lt;/li&gt;
  &lt;li&gt;cache와 persist는 rdd정보를 메모리 또는 디스크에 저장해서 다음 액션을 수행 시 다시 rdd를 생성하는 단계를 거치지 않음&lt;/li&gt;
  &lt;li&gt;unpersist는 저장된 메모리가 더이상 필요없을 시 취소할 때 사용&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;RDD 데이터 불러오기와 저장하기&lt;/h2&gt;
&lt;p&gt;스파크는 하둡 API기반이라서 다양한 데이터 포맷과 파일을 지원합니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;파일 포맷 : 텍스트파일, JSON, 하둡의 시퀀스파일, csv&lt;/li&gt;
  &lt;li&gt;파일 시스템 : 로컬 파일 시스템, 하둡파일시스템(HDFS), AWS의 S3, 오픈스택의 Swift등
    &lt;ul&gt;
      &lt;li&gt;파일시스템이란 ? 컴퓨터에서 파일이나 자료를 쉽게 발견할 수 있도록 유지 관리하는 방법임. 즉, 저장매체에는 많은 파일이 있으므로, 이러한 파일을 관리하는 방법을 말함. 파일을 빠르게 읽기, 쓰기, 삭제 등 기본적인 기능을 원활히 수행하기 위한 목적임&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;1. 텍스트 파일&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;rdd = sc.textFile(&quot;file:////Users/ralasun/Desktop/ralasun.github.io/_posts/2020-07-11-introRL(1).md&quot;)
&amp;gt;&amp;gt;&amp;gt; rdd.collect()
['---', 'layout : post', 'title: Reinforcement Learning 소개[1]', 'category: Reinforcement Learning', 'tags: cs234 reinforcement-learning david-silver sutton', '---', '', '이번 포스팅은 강화학습이 기존에 알려진 여러 방법론들과의 비교를 통한 강화학습 특성과 구성요소를 다룹니다. ...```
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;“file:///”처럼 ///를 세개 작성해야 함. HDFS와 구별하기 위해서임&lt;/li&gt;
  &lt;li&gt;또한 클러스터내 각 서버에서 동일한 경로를 통해 지정한 파일에 접근이 가능해야 함&lt;/li&gt;
  &lt;li&gt;sc.textFile(path, n)에서, n을 통해 파티션 개수 정할 수 있음&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;#save
rdd.saveAsTextFile(&quot;&amp;lt;path_to_save&amp;gt;/sub1&quot;)

#save(gzip)
rdd.saveAsTextFile(&quot;&amp;lt;path_to_save&amp;gt;/sub1&quot;, codec)
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;위와 같이 rdd를 text파일로도 저장이 가능함. 두번째는 압축을 사용하는 방법임&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;2. 오브젝트 파일&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;오브젝트 직렬화 방법으로 RDD를 저장함. python의 경우, pickle형태로 저장함&lt;/li&gt;
  &lt;li&gt;텍스트파일도 가능함&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize(range(1,1000),3)
&amp;gt;&amp;gt;&amp;gt; rdd.saveAsPickleFile(&quot;/Users/ralasun/Desktop/pythonpickle.pkl&quot;)
&amp;gt;&amp;gt;&amp;gt; rdd2 = sc.pickleFile(&quot;/Users/ralasun/Desktop/pythonpickle.pkl&quot;)       
&amp;gt;&amp;gt;&amp;gt; rdd2.take(2)
[667, 668]
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;3. 시퀀스 파일&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;시퀀스파일이란, 키와 값으로 구성된 데이터를 저장하는 이진 파일 포맷으로, 하둡에서 자주 사용됨&lt;/li&gt;
  &lt;li&gt;오브젝트 파일과의 차이점은 오브젝트 파일은 RDD에 포함된 각 데이터가 serializable 인터페이스를 구현하고 있어야 하는 것처럼 시퀀스 파일로 만들고 싶은 RDD가 하둡의 writable 인터페이스를 구현하고 있어야 함.&lt;/li&gt;
  &lt;li&gt;saveAsNewAPIHadoopFile(path, outputFormatClass, keyClass=None, valueClass=None, keyConverter=None, valueConverter=None, conf=None)
    &lt;ul&gt;
      &lt;li&gt;sequence파일로 저장하기 위해선 outputFormatClass에 문자열의 형태로 하둡내 시퀀스포맷의 풀네임을 작성해야 함. keyclass와 valueclass도 마찬가지임. 이렇게 하는 이유는 하둡의 writable 인터페이스를 구현해야 할 객체가 필요하기 때문임.&lt;/li&gt;
      &lt;li&gt;따라서 내부에서는 keyclass와 valueclass 인자에 전달한 포맷으로 rdd를 변환한 뒤 sequencefile포맷으로 저장하는 작업을 거치는 것임&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt; path = &quot;/Users/ralasun/Desktop/ppkl&quot;
&amp;gt;&amp;gt;&amp;gt; outputFormatClass = &quot;org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat&quot;
&amp;gt;&amp;gt;&amp;gt; inputformatClass = &quot;org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat&quot;
&amp;gt;&amp;gt;&amp;gt; keyClass = &quot;org.apache.hadoop.io.Text&quot;
&amp;gt;&amp;gt;&amp;gt; valueClass = &quot;org.apache.hadoop.io.IntWritable&quot;
&amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize([&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;b&quot;,&quot;c&quot;])
&amp;gt;&amp;gt;&amp;gt; rdd2 = rdd.map(lambda x:(x,1))
&amp;gt;&amp;gt;&amp;gt; rdd2.collect()
[('a', 1), ('b', 1), ('c', 1), ('b', 1), ('c', 1)]
&amp;gt;&amp;gt;&amp;gt; rdd2.saveAsNewAPIHadoopFile(path, outputFormatClass, keyClass, valueClass)
rdd3 = sc.newAPIHadoopFile(path, inputformatClass, keyClass, valueClass)
&amp;gt;&amp;gt;&amp;gt; for k, v in rdd3.collect():
...     print(k,v)
... 
a 1
b 1
b 1
c 1
c 1
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;클러스터 환경에서의 공유 변수&lt;/h2&gt;

&lt;p&gt;클러스터 환경에서 하나의 잡을 수행하기 위해 다수의 서버가 여러 개의 프로세스를 실행합니다. 따라서, 여러 프로세스가 공유할 수 있는 자원을 관리(읽기/쓰기 자원)할 수 있도록 스파크는 지원하는데, 브로드캐스트 변수와 어큐뮬레이터라 합니다.&lt;/p&gt;

&lt;h5&gt;브로드캐스트 변수(Broadcast Variables)&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;스파크 잡이 실행되는 동안 클러스터 내의 모든 서버에서 공유할 수 있는 읽기전용 자원을 설정할 수 있는 변수임&lt;/li&gt;
  &lt;li&gt;예를 들어, 온라인 쇼핑몰에서 사용자 ID와 구매 정보가 담긴 10TB짜리 로그를 분석할 때, 우리가 찾고자 하는 사용자 ID목록이 담긴 세트 컬렉션 타입의 데이터를 공유 변수로 설정해 각 서버에서 로그를 처리하면서 현재 처리하려는 로그가 우리가 찾고자 하는 로그가 맞는지 확인하는 용도로 사용 가능함&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; bu = sc.broadcast([&quot;u1&quot;,&quot;u2&quot;])
#1. sparkcontext의 broadcast인자를 이용해서 broadcast변수 생성

&amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize([&quot;u1&quot;,&quot;u2&quot;,&quot;u3&quot;,&quot;u4&quot;,&quot;u5&quot;,&quot;u6&quot;],3)
&amp;gt;&amp;gt;&amp;gt; result = rdd.filter(lambda v: v in bu.value)
#2. broadcast변수 요소 접근시 value매서드를 이용

&amp;gt;&amp;gt;&amp;gt; result.collect()
['u1', 'u2']
&lt;/code&gt;&lt;/pre&gt;

&lt;h5&gt;어큐뮬레이터(Accumulators)&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;어큐뮬레이터는 쓰기 동작을 위한 것임&lt;/li&gt;
  &lt;li&gt;예를 들어, 온라인 쇼핑몰에서 사용자 접속 로그파일을 각 서버에서 취합해서 분석하는 경우임&lt;/li&gt;
  &lt;li&gt;또한 다수의 서버로 구성된 클러스터 환경에서 오류가 발생 했을 시, 어느 프로세스에서 오류가 난건지 확인이 필요함. 그러기 위해선 에러 정보를 한곳에 모아서 볼 수 있는 방법이 필요함.&lt;/li&gt;
  &lt;li&gt;어큐뮬레이터는 이렇게 클러스터내의 모든 서버가 공유하는 쓰기 공간을 제공해서, 각 서버에서 발생하는 이벤트나 정보를 모아두는 용도로 사용함.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;#accumulator 기본 예제
def accumulate(v, acc):
    if(len(v.split(&quot;:&quot;)) !=2):
        acc.add(1)

if __name__ =='__main__':
    conf = SparkConf()
    conf.set(&quot;spark.driver.host&quot;, &quot;127.0.0.1&quot;)
    sc = SparkContext(master=&quot;local[*]&quot;, appName=&quot;RDDOpSample&quot;, conf=conf)

    acc1 = sc.accumulator(0)
    data = [&quot;U1:Addr1&quot;, &quot;U2:Addr2&quot;, &quot;U3&quot;, &quot;U4:Addr4&quot;, &quot;U5:Addr5&quot;,&quot;U6:Addr6&quot;, &quot;U7&quot;]
    rdd = sc.parallelize(data)
    rdd.foreach(lambda v : accumulate(v, acc1))
    print(acc1.value)
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;파이썬의 경우 어큐뮬레이터의 이름 지정 불가능함&lt;/li&gt;
  &lt;li&gt;기본 제공하는 어큐뮬레이터는 sparkcontext의 accumulator 메서드를 이용하는데, 초깃값으로 정수, 실수, 복소수 타입중 하나여야 함. 따라서, 사용자 정의 데이터 타입에 대한 어큐뮬레이터는 아래와 같이 사용해야 함.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;from pyspark import AccumulatorParam
from record import Record
from builtins import isinstance

class RecordAccumulatorParam(AccumulatorParam):

    def zero(self, initialValue):
        return Record(0)

    def addInPlace(self, v1, v2):
        if(isinstance(v2, Record)):
            return v1+v2
        else:
            return v1.addAmt(v2)

def accumulate(v, acc):
    if(len(v.split(&quot;:&quot;))!=2):
        acc.add(1)

if __name__ =='__main__':
    conf = SparkConf()
    conf.set(&quot;spark.driver.host&quot;, &quot;127.0.0.1&quot;)
    sc = SparkContext(master=&quot;local[*]&quot;, appName=&quot;RDDOpSample&quot;, conf=conf)

    acc = sc.accumulator(Record(0), RecordAccumulatorParam())
    data = [&quot;U1:Addr1&quot;, &quot;U2:Addr2&quot;, &quot;U3&quot;, &quot;U4:Addr4&quot;, &quot;U5:Addr5&quot;,&quot;U6:Addr6&quot;, &quot;U7&quot;]
    rdd = sc.parallelize(data)
    rdd.foreach(lambda v: accumulate(v, acc))
    print(acc.value.amount) #&amp;gt;&amp;gt; 2
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;#AccumulatorParam에 대한 pyspark Document

class pyspark.AccumulatorParam
# Helper object that defines how to accumulate values of a given type.

	addInPlace(value1, value2)
	# Add two values of the accumulator’s data type, returning a new value; for efficiency, can also update value1 in place and return it.

	zero(value)
	# Provide a “zero value” for the type, compatible in dimensions with the provided value (e.g., a zero vector)
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Recordclass타입에 대한 accumulator를 작성한 것임.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;어큐뮬레이터 사용시 주의할 점 두 가지&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;
        &lt;ol&gt;
          &lt;li&gt;어큐뮬레이터를 증가시키는 동작은 클러스터 내 모든 서버에서 가능하나, 어큐뮬레이터 내 데이터를 읽는 동작은 드라이버 프로그램 내에서만 가능
            &lt;ul&gt;
              &lt;li&gt;transformation 또는 action 연산 내부에서는 어큐뮬레이터를 증가시킬 수 있으나, 그 값을 참조해서 사용은 불가능하다는 것을 뜻함.&lt;/li&gt;
              &lt;li&gt;어큐뮬레이터는 액션 메서드 내에서만 수행하는 것이 좋음. 트렌스포메이션은 여러번 수행될 수 있기 때문에 집계가 잘못될 수 있음&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;이상으로 본 포스팅을 마치겠습니다.&lt;/p&gt;</content><author><name>Seonhwa Lee</name></author><category term="Spark Programming" /><category term="data-engineering" /><summary type="html">이번 포스팅은 지난 포스팅 &amp;lt;RDD, Resilient Distributed DataSet에 대하여[2] - RDD기본액션, RDD트랜스포메이션&amp;gt; 에 이어서 진행하도록 하겠습니다. 교재는 빅데이터 분석을 위한 스파크2 프로그래밍을 참고하였습니다. 2.1.6 RDD 액션 RDD트랜스포메이션 연산은 느긋한 평가(lazy evaluation) 또는 지연 계산 방식을 따릅니다. 이는 계산에 필요한 정보를 누적하다가 계산이 필요한 시점이 돼서야 계산을 수행하는 방식을 뜻합니다. 여기서 계산이 필요한 시점은 RDD 액션 메서드가 호출된 시점입니다. RDD 액션 메서드가 호출이 되어야 비로소 RDD 트랜스포메이션 연산이 수행되게 됩니다. 1. 출력 연산 1.1. first RDD 요소 중 ,첫번째 요소를 돌려줌 &amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize(range(50)) &amp;gt;&amp;gt;&amp;gt; result = rdd.first() &amp;gt;&amp;gt;&amp;gt; print(result) 0 1.2. take RDD 요소중, n번째까지 요소를 돌려줌 &amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize(range(50)) &amp;gt;&amp;gt;&amp;gt; result = rdd.take(5) &amp;gt;&amp;gt;&amp;gt; print(result) [0, 1, 2, 3, 4] 1.3. takeSample 지정된 크기의 sample을 추출해서 리스트, 배열 타입등으로 반환함 sample 메서드와의 차이점 sample 메서드는 RDD 트랜스포메이션 메서드이고, 크기를 지정할 수 없음. takeSample(withReplacement, num, seed=None) &amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize(range(100)) &amp;gt;&amp;gt;&amp;gt; result = rdd.takeSample(False, 3) &amp;gt;&amp;gt;&amp;gt; result [55, 23, 45] 1.5. countByValue RDD의 요소들이 나타낸 횟수를 맵 형태로 돌려주는 메서드 &amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize([1,1,3,2,1,2,2,1,1,4,5,3,2,3]) &amp;gt;&amp;gt;&amp;gt; result = rdd.countByValue() &amp;gt;&amp;gt;&amp;gt; print(result) defaultdict(&amp;lt;class 'int'&amp;gt;, {1: 5, 3: 3, 2: 4, 4: 1, 5: 1}) 1.6. reduce reduce 메서드 인자는 함수가 들어감. 그 함수는 교환법칙과 결합법칙이 성립하는 함수여야 함. 따라서, 메서드 인자로 받은 함수를 이용해서 하나의 요소로 합치는 메서드임. def reduce(f: (T,T)=&amp;gt;T):T 동일한 타입 2개를 입력으로 받아, 같은 타입으로 반환해주는 메서드임 실제 구현은 파티션단위로 나눠져서 처리됨. 분산 프로그램이기 때문임. &amp;gt;&amp;gt;&amp;gt; from operator import add &amp;gt;&amp;gt;&amp;gt; add(1,2) 3 &amp;gt;&amp;gt;&amp;gt; sc.parallelize([1,2,3,4,5]).reduce(add) 15 1.7. fold reduce와 동일하나, 초기값을 설정할 수 있음 def fold(zeroValue: T)(op: (T,T)=&amp;gt;T):T 그런데 유의할 점은 파티션단위로 나뉘어서 처리하기 때문에, 파티션단위로 처리할 때마다 초깃값을 이용하여 연산이 수행됨. 따라서, 더하기 연산을 할 땐 항등원인 0을, 곱셈 연산을 할 땐 항등원인 1을 초깃값으로 설정하는 것이 좋음 &amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize(range(1,11), 3) &amp;gt;&amp;gt;&amp;gt; rdd.fold(1, add) 59 #값이 55가 아니라 59가 나오는 것을 확인할 수 있음. reduce와 fold차이 #product.py class Product: def __init__(self, price): self.price = price self.count = 1 def addPriceandCount(p1, p2): p1.price += p2.price p1.count += 1 return p1 #return을 p1인 이유 --&amp;gt; 입력값과 출력값의 타입이 동일해야 함. if __name__ =='__main__': conf = SparkConf() conf.set(&quot;spark.driver.host&quot;, &quot;127.0.0.1&quot;) sc = SparkContext(master=&quot;local[*]&quot;, appName=&quot;RDDOpSample&quot;, conf=conf) rdd = sc.parallelize([Product(300), Product(200), Product(100)], 10) #reduce result = rdd.reduce(addPriceandCount) print(result.price, result.count) #fold result = rdd.fold(Product(0), addPriceandCount) print(result.price, result.count) fold의 count합을 보면 11인 것을 알 수 있음. 그 이유는 위에서 파티션 개수를 10으로 지정하였고, 파티션 단위로 연산을 초기값을 이용하여 연산을 수행하기 때문임 1.8. aggregate 입력와 출력의 타입이 다른 경우 사용 가능 def aggregateU(seqOp:(U,T)=&amp;gt;U, combOp:(U,U)=&amp;gt;U):U 크게 세가지 인자를 받음. 첫번째는 초깃값으로 fold와 동일 aggregate은 병합을 크게 2단계로 구성되는데, 1단계는 seqOp에 의해, 2단계는 combOp에 의해 진행됨 seqOp는 초깃값과 동일한 타입(U)과 RDD요소 타입(T)가 입력되어 병합 결과 초깃값과 동일한 타입인 U가 반환됨 combOp는 최종병합에서 사용됨 #rdd에 속한 요소들의 평균을 aggregate을 이용하여 구하는 예제 #record.py class Record: def __init__(self, amount, number=1): self.amount = amount self.number = number def addAmt(self, amount): return Record(self.amount + amount, self.number + 1) def __add__(self, other): amount = self.amount + other.amount number = self.number + other.number return Record(amount, number) def __str__(self): return &quot;avg:&quot; + str(self.amount / self.number) def __repr__(self): return 'Record(%r, %r)' % (self.amount, self.number) def seqop(r,v): return r.addAmt(v) if __name__ =='__main__': conf = SparkConf() conf.set(&quot;spark.driver.host&quot;, &quot;127.0.0.1&quot;) sc = SparkContext(master=&quot;local[*]&quot;, appName=&quot;RDDOpSample&quot;, conf=conf) rdd = sc.parallelize([100,80,75,90,95], 3) #aggregate result = rdd.aggregate(Record(0,0), seqop, lambda r1, r2:r1+r2) print(result) # avg:88.0 1.9. sum 모든 요소의 합을 구해주며, Double, Long등 숫자타입인 경우에만 사용가능 &amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize(range(1,11)) &amp;gt;&amp;gt;&amp;gt; rdd.sum() 55 1.10. foreach, foreachPartition foreach는 RDD의 개별요소에 전달받은 함수를 적용하는 메서드이고, foreachPartition은 파티션 단위로 적용됨 이때 인자로 받는 함수는 한개의 입력값을 가지는 함수임 이 메서드를 사용할 때 유의할 점은 드라이버 프로그램(메인 함수를 포함하고 있는 프로그램)이 작동하고 있는 서버위가 아니라 클러스터의 각 개별 서버에서 실행된다는 것임 따라서 foreach() 인자로 print함수를 전달한다는 것은 각 서버의 콘솔에 출력하라는 의미가 됨. def sideEffect(values): print(&quot;partition side effect&quot;) for v in values: print(&quot;value side effect : %s&quot; %v) if __name__ =='__main__': conf = SparkConf() conf.set(&quot;spark.driver.host&quot;, &quot;127.0.0.1&quot;) sc = SparkContext(master=&quot;local[*]&quot;, appName=&quot;RDDOpSample&quot;, conf=conf) rdd = sc.parallelize(range(1,11),3) result = rdd.foreach(lambda v:print(&quot;value side effect: %s&quot; %v)) result2 = rdd.foreachPartition(sideEffect) ### value side effect: 2 value side effect: 3 value side effect: 4 value side effect: 5 value side effect: 6 value side effect: 7 value side effect: 8 value side effect: 9 value side effect: 10 partition side effect value side effect : 7 value side effect : 8 value side effect : 9 value side effect : 10 partition side effect value side effect : 4 value side effect : 5 value side effect : 6 partition side effect value side effect : 1 value side effect : 2 value side effect : 3 ### 1.11. toDebugString 디버깅을 위한 메서드. RDD파티션 개수나 의존성 정보 등 세부 정보 알고 싶을 때 사용 &amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize(range(1,100), 10).persist().map(lambda v:(v,1)).coalesce(2) &amp;gt;&amp;gt;&amp;gt; rdd.toDebugString() b'(2) CoalescedRDD[65] at coalesce at NativeMethodAccessorImpl.java:0 []\n | PythonRDD[64] at RDD at PythonRDD.scala:53 []\n | PythonRDD[63] at RDD at PythonRDD.scala:53 []\n | ParallelCollectionRDD[62] at parallelize at PythonRDD.scala:195 []' 1.12. cache, persist, unpersist rdd액션 연산이 수행될때마다 RDD 생성 히스토리를 이용해 복구하는 단계를 수행하지만 너무나 번거로움 따라서 반복적으로 사용되는 RDD인 경우 메모리에 저장해서 사용함 cache와 persist는 rdd정보를 메모리 또는 디스크에 저장해서 다음 액션을 수행 시 다시 rdd를 생성하는 단계를 거치지 않음 unpersist는 저장된 메모리가 더이상 필요없을 시 취소할 때 사용 RDD 데이터 불러오기와 저장하기 스파크는 하둡 API기반이라서 다양한 데이터 포맷과 파일을 지원합니다. 파일 포맷 : 텍스트파일, JSON, 하둡의 시퀀스파일, csv 파일 시스템 : 로컬 파일 시스템, 하둡파일시스템(HDFS), AWS의 S3, 오픈스택의 Swift등 파일시스템이란 ? 컴퓨터에서 파일이나 자료를 쉽게 발견할 수 있도록 유지 관리하는 방법임. 즉, 저장매체에는 많은 파일이 있으므로, 이러한 파일을 관리하는 방법을 말함. 파일을 빠르게 읽기, 쓰기, 삭제 등 기본적인 기능을 원활히 수행하기 위한 목적임 1. 텍스트 파일 rdd = sc.textFile(&quot;file:////Users/ralasun/Desktop/ralasun.github.io/_posts/2020-07-11-introRL(1).md&quot;) &amp;gt;&amp;gt;&amp;gt; rdd.collect() ['---', 'layout : post', 'title: Reinforcement Learning 소개[1]', 'category: Reinforcement Learning', 'tags: cs234 reinforcement-learning david-silver sutton', '---', '', '이번 포스팅은 강화학습이 기존에 알려진 여러 방법론들과의 비교를 통한 강화학습 특성과 구성요소를 다룹니다. ...``` “file:///”처럼 ///를 세개 작성해야 함. HDFS와 구별하기 위해서임 또한 클러스터내 각 서버에서 동일한 경로를 통해 지정한 파일에 접근이 가능해야 함 sc.textFile(path, n)에서, n을 통해 파티션 개수 정할 수 있음 #save rdd.saveAsTextFile(&quot;&amp;lt;path_to_save&amp;gt;/sub1&quot;) #save(gzip) rdd.saveAsTextFile(&quot;&amp;lt;path_to_save&amp;gt;/sub1&quot;, codec) 위와 같이 rdd를 text파일로도 저장이 가능함. 두번째는 압축을 사용하는 방법임 2. 오브젝트 파일 오브젝트 직렬화 방법으로 RDD를 저장함. python의 경우, pickle형태로 저장함 텍스트파일도 가능함 &amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize(range(1,1000),3) &amp;gt;&amp;gt;&amp;gt; rdd.saveAsPickleFile(&quot;/Users/ralasun/Desktop/pythonpickle.pkl&quot;) &amp;gt;&amp;gt;&amp;gt; rdd2 = sc.pickleFile(&quot;/Users/ralasun/Desktop/pythonpickle.pkl&quot;) &amp;gt;&amp;gt;&amp;gt; rdd2.take(2) [667, 668] 3. 시퀀스 파일 시퀀스파일이란, 키와 값으로 구성된 데이터를 저장하는 이진 파일 포맷으로, 하둡에서 자주 사용됨 오브젝트 파일과의 차이점은 오브젝트 파일은 RDD에 포함된 각 데이터가 serializable 인터페이스를 구현하고 있어야 하는 것처럼 시퀀스 파일로 만들고 싶은 RDD가 하둡의 writable 인터페이스를 구현하고 있어야 함. saveAsNewAPIHadoopFile(path, outputFormatClass, keyClass=None, valueClass=None, keyConverter=None, valueConverter=None, conf=None) sequence파일로 저장하기 위해선 outputFormatClass에 문자열의 형태로 하둡내 시퀀스포맷의 풀네임을 작성해야 함. keyclass와 valueclass도 마찬가지임. 이렇게 하는 이유는 하둡의 writable 인터페이스를 구현해야 할 객체가 필요하기 때문임. 따라서 내부에서는 keyclass와 valueclass 인자에 전달한 포맷으로 rdd를 변환한 뒤 sequencefile포맷으로 저장하는 작업을 거치는 것임 path = &quot;/Users/ralasun/Desktop/ppkl&quot; &amp;gt;&amp;gt;&amp;gt; outputFormatClass = &quot;org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat&quot; &amp;gt;&amp;gt;&amp;gt; inputformatClass = &quot;org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat&quot; &amp;gt;&amp;gt;&amp;gt; keyClass = &quot;org.apache.hadoop.io.Text&quot; &amp;gt;&amp;gt;&amp;gt; valueClass = &quot;org.apache.hadoop.io.IntWritable&quot; &amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize([&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;b&quot;,&quot;c&quot;]) &amp;gt;&amp;gt;&amp;gt; rdd2 = rdd.map(lambda x:(x,1)) &amp;gt;&amp;gt;&amp;gt; rdd2.collect() [('a', 1), ('b', 1), ('c', 1), ('b', 1), ('c', 1)] &amp;gt;&amp;gt;&amp;gt; rdd2.saveAsNewAPIHadoopFile(path, outputFormatClass, keyClass, valueClass) rdd3 = sc.newAPIHadoopFile(path, inputformatClass, keyClass, valueClass) &amp;gt;&amp;gt;&amp;gt; for k, v in rdd3.collect(): ... print(k,v) ... a 1 b 1 b 1 c 1 c 1 클러스터 환경에서의 공유 변수 클러스터 환경에서 하나의 잡을 수행하기 위해 다수의 서버가 여러 개의 프로세스를 실행합니다. 따라서, 여러 프로세스가 공유할 수 있는 자원을 관리(읽기/쓰기 자원)할 수 있도록 스파크는 지원하는데, 브로드캐스트 변수와 어큐뮬레이터라 합니다. 브로드캐스트 변수(Broadcast Variables) 스파크 잡이 실행되는 동안 클러스터 내의 모든 서버에서 공유할 수 있는 읽기전용 자원을 설정할 수 있는 변수임 예를 들어, 온라인 쇼핑몰에서 사용자 ID와 구매 정보가 담긴 10TB짜리 로그를 분석할 때, 우리가 찾고자 하는 사용자 ID목록이 담긴 세트 컬렉션 타입의 데이터를 공유 변수로 설정해 각 서버에서 로그를 처리하면서 현재 처리하려는 로그가 우리가 찾고자 하는 로그가 맞는지 확인하는 용도로 사용 가능함 &amp;gt;&amp;gt;&amp;gt; bu = sc.broadcast([&quot;u1&quot;,&quot;u2&quot;]) #1. sparkcontext의 broadcast인자를 이용해서 broadcast변수 생성 &amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize([&quot;u1&quot;,&quot;u2&quot;,&quot;u3&quot;,&quot;u4&quot;,&quot;u5&quot;,&quot;u6&quot;],3) &amp;gt;&amp;gt;&amp;gt; result = rdd.filter(lambda v: v in bu.value) #2. broadcast변수 요소 접근시 value매서드를 이용 &amp;gt;&amp;gt;&amp;gt; result.collect() ['u1', 'u2'] 어큐뮬레이터(Accumulators) 어큐뮬레이터는 쓰기 동작을 위한 것임 예를 들어, 온라인 쇼핑몰에서 사용자 접속 로그파일을 각 서버에서 취합해서 분석하는 경우임 또한 다수의 서버로 구성된 클러스터 환경에서 오류가 발생 했을 시, 어느 프로세스에서 오류가 난건지 확인이 필요함. 그러기 위해선 에러 정보를 한곳에 모아서 볼 수 있는 방법이 필요함. 어큐뮬레이터는 이렇게 클러스터내의 모든 서버가 공유하는 쓰기 공간을 제공해서, 각 서버에서 발생하는 이벤트나 정보를 모아두는 용도로 사용함. #accumulator 기본 예제 def accumulate(v, acc): if(len(v.split(&quot;:&quot;)) !=2): acc.add(1) if __name__ =='__main__': conf = SparkConf() conf.set(&quot;spark.driver.host&quot;, &quot;127.0.0.1&quot;) sc = SparkContext(master=&quot;local[*]&quot;, appName=&quot;RDDOpSample&quot;, conf=conf) acc1 = sc.accumulator(0) data = [&quot;U1:Addr1&quot;, &quot;U2:Addr2&quot;, &quot;U3&quot;, &quot;U4:Addr4&quot;, &quot;U5:Addr5&quot;,&quot;U6:Addr6&quot;, &quot;U7&quot;] rdd = sc.parallelize(data) rdd.foreach(lambda v : accumulate(v, acc1)) print(acc1.value) 파이썬의 경우 어큐뮬레이터의 이름 지정 불가능함 기본 제공하는 어큐뮬레이터는 sparkcontext의 accumulator 메서드를 이용하는데, 초깃값으로 정수, 실수, 복소수 타입중 하나여야 함. 따라서, 사용자 정의 데이터 타입에 대한 어큐뮬레이터는 아래와 같이 사용해야 함. from pyspark import AccumulatorParam from record import Record from builtins import isinstance class RecordAccumulatorParam(AccumulatorParam): def zero(self, initialValue): return Record(0) def addInPlace(self, v1, v2): if(isinstance(v2, Record)): return v1+v2 else: return v1.addAmt(v2) def accumulate(v, acc): if(len(v.split(&quot;:&quot;))!=2): acc.add(1) if __name__ =='__main__': conf = SparkConf() conf.set(&quot;spark.driver.host&quot;, &quot;127.0.0.1&quot;) sc = SparkContext(master=&quot;local[*]&quot;, appName=&quot;RDDOpSample&quot;, conf=conf) acc = sc.accumulator(Record(0), RecordAccumulatorParam()) data = [&quot;U1:Addr1&quot;, &quot;U2:Addr2&quot;, &quot;U3&quot;, &quot;U4:Addr4&quot;, &quot;U5:Addr5&quot;,&quot;U6:Addr6&quot;, &quot;U7&quot;] rdd = sc.parallelize(data) rdd.foreach(lambda v: accumulate(v, acc)) print(acc.value.amount) #&amp;gt;&amp;gt; 2 #AccumulatorParam에 대한 pyspark Document class pyspark.AccumulatorParam # Helper object that defines how to accumulate values of a given type. addInPlace(value1, value2) # Add two values of the accumulator’s data type, returning a new value; for efficiency, can also update value1 in place and return it. zero(value) # Provide a “zero value” for the type, compatible in dimensions with the provided value (e.g., a zero vector) Recordclass타입에 대한 accumulator를 작성한 것임. 어큐뮬레이터 사용시 주의할 점 두 가지 어큐뮬레이터를 증가시키는 동작은 클러스터 내 모든 서버에서 가능하나, 어큐뮬레이터 내 데이터를 읽는 동작은 드라이버 프로그램 내에서만 가능 transformation 또는 action 연산 내부에서는 어큐뮬레이터를 증가시킬 수 있으나, 그 값을 참조해서 사용은 불가능하다는 것을 뜻함. 어큐뮬레이터는 액션 메서드 내에서만 수행하는 것이 좋음. 트렌스포메이션은 여러번 수행될 수 있기 때문에 집계가 잘못될 수 있음 이상으로 본 포스팅을 마치겠습니다.</summary></entry><entry><title type="html">RDD, Resilient Distributed Dataset에 대하여[2] - RDD기본액션, RDD트랜스포메이션</title><link href="http://localhost:4000/spark%20programming/2020/12/07/rdd(2)/" rel="alternate" type="text/html" title="RDD, Resilient Distributed Dataset에 대하여[2] - RDD기본액션, RDD트랜스포메이션" /><published>2020-12-07T00:00:00+09:00</published><updated>2020-12-07T00:00:00+09:00</updated><id>http://localhost:4000/spark%20programming/2020/12/07/rdd(2)</id><content type="html" xml:base="http://localhost:4000/spark%20programming/2020/12/07/rdd(2)/">&lt;p&gt;이번 포스팅은 지난 포스팅 &amp;lt;RDD, Resilient Distributed DataSet에 대하여[1]&amp;gt; 에 이어서 진행하도록 하겠습니다. 교재는 빅데이터 분석을 위한 스파크2 프로그래밍을 참고하였습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1&gt;2. RDD&lt;/h1&gt;

&lt;h2&gt;2.1.1 들어가기에 앞서&lt;/h2&gt;
&lt;p&gt;지난 포스팅 &lt;a href=&quot;https://ralasun.github.io/spark%20programming/2020/11/20/rdd/&quot;&gt;2-1. RDD Resilient Distributed Dataset에 대하여&lt;/a&gt; 에서 다뤘습니다.&lt;/p&gt;

&lt;h2&gt;2.1.2. 스파크컨텍스트 생성&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;스파크 컨텍스트는 스파크 애플리케이션과 클러스터의 연결을 관리하는 객체임&lt;/li&gt;
  &lt;li&gt;따라서, 스파크 애플리케이션을 사용하려면 무조건 스파크 컨텍스트를 생성하고 이용해야 함&lt;/li&gt;
  &lt;li&gt;RDD 생성도 스파크컨텍스트를 이용해 생성 가능함&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Scala&quot;&gt;val conf = new SparkConf().setMaster(“local[*]”).setAppName(“RDDCreateSample”)
val sc = new SparkContext(conf)
&lt;/code&gt;&lt;/pre&gt;
&lt;figcaption align=&quot;center&quot;&gt;[예] scala - sparkcontext 생성&lt;/figcaption&gt;
&lt;p&gt; &lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt; sc = SparkContext(master=“local[*], appName=“RDDCreateTest”, conf=conf)
&lt;/code&gt;&lt;/pre&gt;
&lt;figcaption align=&quot;center&quot;&gt;[예] python - spark context 생성&lt;/figcaption&gt;
&lt;p&gt; &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;스파크 동작에 필요한 여러 설정 정보 지정 가능함&lt;/li&gt;
  &lt;li&gt;SparkConf(), conf=conf 부분에서 config을 통과시켜서 지정 가능함&lt;/li&gt;
  &lt;li&gt;지정해야 하는 정보 중에, master 정보와 appName 정보는 필수 지정 정보임
    &lt;ul&gt;
      &lt;li&gt;master 정보란 ? 스파크가 동작할 클러스터의 마스터 서버를 의미하는 것. 로컬모드에서 local, local[3], local[*]와 같이 사용. [*]는 쓰레드 개수를 의미하며, *는 사용 가능한 모든 쓰레드를 이용하겠다는 이야기임&lt;/li&gt;
      &lt;li&gt;appName은 애플리케이션 이름으로, 구분하기 위한 목적임. 스파크 UI화면에 사용됨&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;2.1.3. RDD생성&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;RDD를 생성하는 방법은 크게 2가지임&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;&lt;li&gt; 드라이버 프로그램의 컬렉션 객체 이용&lt;/li&gt;
&lt;ul&gt;&lt;li&gt;
자바 or 파이썬 ? 리스트 이용, 스칼라 ? 시퀀스타입 이용&lt;/li&gt;
&lt;li&gt;드라이버 프로그램?&lt;br /&gt;
	- 최초로 메인 함수를 실행해 RDD등을 생성하고 각종 연산을 호출하는 프로그램&lt;br /&gt;
	- 드라이버 내의 메인 함수는 스파크 애플리케이션과 스파크 컨텍스트 객체를 생성함&lt;br /&gt;
	- 스파크 컨텍스트를 통해 RDD의 연산 정보를 DAG스케쥴러에 전달하면 스케쥴러는 이 정보를 가지고 실행 계획을 수립한 후 이를 클러스터 매니저에게 전달함&lt;br /&gt;&lt;/li&gt;

&lt;pre lang=&quot;Scala&quot;&gt;&lt;code&gt;
val rdd1 = sc.parallelize(List(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;,&quot;e&quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;figcaption align=&quot;center&quot;&gt;[예] scala - rdd 생성&lt;/figcaption&gt; 

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/MxtJV7I.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;[예] python - 드라이버의 컬렉션 객체를 이용한 RDD 생성&lt;/figcaption&gt;&lt;/p&gt;

&lt;li&gt;
문자열을 포함한 컬렉션 객체 생성 example) python : ['a','b','c','d']&lt;/li&gt;
&lt;li&gt;parallelize() 메서드를 이용해 RDD 생성&lt;br /&gt; - RDD의 파티션 수를 지정하고 싶을 때,  parallelize() 메서드의 두 번째 매개변수로 파티션 개수 지정 가능&lt;/li&gt;

&lt;pre lang=&quot;scala&quot;&gt;&lt;code&gt;
val rdd1 = sc.parallelize(1 to 1000, 10)
&lt;/code&gt;&lt;/pre&gt;&lt;/ul&gt;

&lt;li&gt;외부 데이터를 읽어서 새로운 RDD를 생성&lt;/li&gt; 
&lt;ul&gt;
&lt;li&gt;기본적으로 하둡의 다루는 모든 입출력 유형 가능&lt;/li&gt;
&lt;li&gt;내부적으로 하둡의 입출력을 사용하기 때문임&lt;/li&gt;&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/eByLARc.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;[예] python - 외부데이터를 이용한 RDD 생성&lt;/figcaption&gt;&lt;/p&gt;&lt;/ol&gt;

&lt;h2&gt;2.1.4 RDD 기본 액션&lt;/h2&gt;
&lt;p&gt;기본 액션 연산의 종류에 대해 알아보도록 하겠습니다.&lt;/p&gt;

&lt;h3&gt;1. collect&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;collect은 RDD의 모든 원소를 모아서 배열로 리턴&lt;/li&gt;
  &lt;li&gt;&lt;b&gt;반환 타입이 RDD가 아닌 배열&lt;/b&gt;이므로 액션 연산&lt;/li&gt;
  &lt;li&gt;RDD에 있는 모든 요소들이 서버의 메모리에 수집됨. 즉, 대용량 데이터를 다룰 땐 조심하고, 주로 작은 용량의 데이터 디버깅용으로 사용함&lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/a1uu1V0.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;[예] python - collect 연산&lt;/figcaption&gt;&lt;/p&gt;

&lt;h3&gt;2. count&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;RDD 구성하는 전체 요소 개수 반환&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;2.1.5 RDD 트랜스포메이션&lt;/h2&gt;
&lt;p&gt;기존 RDD를 이용해 새로운 RDD를 생성하는 연산입니다.&lt;/p&gt;

&lt;h3&gt;1. Map 연산&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;RDD에 속하는 &lt;b&gt;모든 요소에 적용&lt;/b&gt;하여 새로운 RDD 생성하는 연산&lt;/li&gt;
  &lt;li&gt;RDD의 몇몇 연산은 특정 데이터 타입에만 적용 가능함&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;1.1. map&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;b&gt;하나의 인자를 받는 함수 자체&lt;/b&gt;가 map의 인자로 들어감&lt;/li&gt;
  &lt;li&gt;이 함수를 이용해 rdd의 모든 요소에 적용한 뒤 새로운 RDD 리턴&lt;/li&gt;
  &lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/1nFR0IH.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;[예] python - map 연산&lt;/figcaption&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;map()에 전달되는 함수의 입력 데이터 타입과 출력 데이터 타입이 일치할 필요 없음. 문자열을 입력받아 정수로 반환하는 함수 사용 가능&lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/lHGsvGD.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;[예] python - map 연산, 입력/출력 일치하지 않는 경우&lt;/figcaption&gt;&lt;/p&gt;

&lt;h4&gt;1.2. flatMap&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;map()과 마찬가지로, 하나의 인자를 받는 함수가 flatMap의 인자로 들어감&lt;/li&gt;
  &lt;li&gt;map()과 차이점은 각 함수의 인자가 반환하는 값의 타입이 다름&lt;/li&gt;
  &lt;li&gt;flatMap()에 사용하는 함수 f는 반환값으로 리스트나 시퀀스 같은 여러 개의 값을 담은 (이터레이션이 가능한) 일종의 컬렉션과 유사한 타입의 값을 반환해야 함
    &lt;ul&gt;
      &lt;li&gt;map[U](f:(T) -&amp;gt; U):RDD[U]&lt;/li&gt;
      &lt;li&gt;flatMap&lt;a href=&quot;f:(T) -&amp;gt; TraversableOnce\[U\]\&quot;&gt;U&lt;/a&gt;:RDD[U])\
        &lt;ul&gt;
          &lt;li&gt;TraversableOnce는 이터레이터 타입을 의미&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h5&gt;map()과 flatMap() 차이점 예시&lt;/h5&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/hQ7sfCG.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;[예] python - map 연산 vs. flatMap 연산&lt;/figcaption&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;map연산은 문자열의 배열로 구성된 RDD를 생성함&lt;/li&gt;
  &lt;li&gt;각 요소의 문자열(T)이 단어가 포함된 배열(U)이기 때문임&lt;/li&gt;
  &lt;li&gt;반면, flatMap 연산은 문자열로 구성된 RDD를 생성함&lt;/li&gt;
  &lt;li&gt;TraversableOnce(U)이기 때문에 문자열의 배열 내의 요소가 모두 끄집어져 나오는 작업을 하게 됨&lt;/li&gt;
  &lt;li&gt;flatMap()은 하나의 입력값(“apple, orange”)에 대해 출력 값이 여러개인 경우([“apple”, “orange”]) 유용하게 사용할 수 있음&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;1.3. mapPartitions&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;map과 flatMap은 하나의 인자만을 받는 함수가 인자로 들어가지만, mapPartitions은 여러 인자를 받는 함수가 인자로 들어갈 수 있음 ex) 이터레이터를 인자로 받는 함수&lt;/li&gt;
  &lt;li&gt;mapartitions은 인자로 받은 함수가 파티션 단위로 적용하여 새로운 RDD를 생성함. 반면에, map과 flatMap은 인자로 받은 함수가 요소 한개 단위로 적용됨&lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/bAIoEqG.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;[예] python - mapPartitions&lt;/figcaption&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;sc.parallelize(range(1,11),3)으로 파티션 3개로 나뉨&lt;/li&gt;
  &lt;li&gt;DB 연결!!! 가 세번 출력된 걸 보니 파티션 단위로 처리한 것을 확인할 수 있음&lt;/li&gt;
  &lt;li&gt;increase함수는 각 파티션 내의 요소에 대한 이터레이터를 전달받아 함수 내부에서 파티션의 개별 요소에 대한 작업을 처리하고 그 결과를 다시 이터레이터 타입으로 반환&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;1.4. mapPartitionsWithIndex&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;mapPartions와 동일하고 다른 점은 인자로 전달되는 함수를 호출할 때 파티션에 속한 요소의 정보뿐만 아니라 해당 파티션의 인덱스 정보도 함께 전달해 준다는 것임&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;def IncreaseWithIndex(idx, numbers):
	for i in numbers:
		if(idx == 1):
			yield i+1
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;mapPartitionswithIndex에 인자로 들어갈 함수는 위와 같이 인덱스 정보도 같이 들어감&lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/wO8pOFo.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;[예] python - mapPartitionsWithIndex&lt;/figcaption&gt;&lt;/p&gt;

&lt;h4&gt;1.5. mapValues&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;RDD의 모든 요소들이 키와 값의 쌍을 이루고 있는 경우에만 사용 가능한 메서드이며, 인자로 전달받은 함수를 “값”에 해당하는 요소에만 적용하고 그 결과로 구성된 새로운 RDD를 생성&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;rdd1 = sc.parallelize([&quot;a&quot;,&quot;b&quot;,&quot;c&quot;]).map(lambda v:(v,1)) //(키,값)으로 구성된 rdd생성
rdd2 = rdd1.mapValues(lambda i:i+1)
print(rdd2.collect())
&lt;/code&gt;&lt;/pre&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/aFNZfuf.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;[예] python - mapValues&lt;/figcaption&gt;&lt;/p&gt;

&lt;h4&gt;1.6. flatMapValues&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;MapValues 처럼 키에 해당되는 값에 함수를 적용하나 flatMap() 연산을 적용할 수 있음&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;rdd1 = sc.parallelize([(1, &quot;a,b&quot;),(2, &quot;a,c&quot;),(3, &quot;d,e&quot;)])
rdd2 = rdd1.flatMapValues(lambda v:v.split(','))
rdd2.collect()
&lt;/code&gt;&lt;/pre&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/5J8kSE1.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;[예] python - flatMapValues&lt;/figcaption&gt;&lt;/p&gt;

&lt;h3&gt;2. 그룹화 연산&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;특정 조건에 따라 요소를 그룹화하거나 특정 함수 적용&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;2.1. zip&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;두 개의 서로 다른 RDD를 각 요소의 인덱스에 따라 첫번째 RDD의 ‘인덱스’번째를 키로, 두번째 RDD의 ‘인덱스’번째를 값으로 하는 순서쌍을 생성&lt;/li&gt;
  &lt;li&gt;두 개 RDD는 같은 개수의 파티션과 각 파티션 당 요소개수가 동일해야 함&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;rdd1 = sc.parallelize([&quot;a&quot;,&quot;b&quot;,&quot;c&quot;])
rdd2 = sc.parallelize([1,2,3])
rdd3 = rdd1.zip(rdd2)
rdd3.collect()
&amp;gt;&amp;gt;&amp;gt; [('a', 1), ('b', 2), ('c', 3)]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;rdd1 = sc.parallelize(range(1,10),3)
rdd2 = sc.parallelize(range(11,20),3)
rdd3 = rdd1.zip(rdd2)
rdd3.collect()
&amp;gt;&amp;gt;&amp;gt; [(1, 11), (2, 12), (3, 13), (4, 14), (5, 15), (6, 16), (7, 17), (8, 18), (9, 19)]
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;2.2. zipPartitions&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;zip()과 다르게 파티션의 개수만 동일하면 됨&lt;/li&gt;
  &lt;li&gt;zipPartitions()은 최대 4개 RDD까지 인자로 넣을 수 있음&lt;/li&gt;
  &lt;li&gt;파이썬 사용불가!!&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;2.3. groupBy&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;RDD의 요소를 &lt;b&gt;일정한 기준&lt;/b&gt;에 따라 그룹을 나누고, 각 그룹으로 구성된 새로운 RDD를 생성함&lt;/li&gt;
  &lt;li&gt;각 그룹은 키와 각 키에 속한 요소의 시퀀스(iterator)로 구성됨&lt;/li&gt;
  &lt;li&gt;인자로 전달하는 함수가 각 그룹의 키를 결정하는 역할을 담당함&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd1 = sc.parallelize(range(1,11))
&amp;gt;&amp;gt;&amp;gt; rdd2 = rdd1.groupBy(lambda v: &quot;even&quot; if v%2==0 else &quot;odd&quot;) 
/// groupBy에 인자로 전달된 함수에 의해 키(even/odd) 결정

print(rdd2.collect())
&amp;gt;&amp;gt;&amp;gt; [('even', &amp;lt;pyspark.resultiterable.ResultIterable object at 0x7fbc3f090e80&amp;gt;), ('odd', &amp;lt;pyspark.resultiterable.ResultIterable object at 0x7fbc3f090470&amp;gt;)]
/// 각 키에 해당하는 값은 iterator임을 확인할 수 있음

&amp;gt;&amp;gt;&amp;gt; for x in rdd2.collect():
...     print(x[0], list(x[1]))
... 
even [2, 4, 6, 8, 10]
odd [1, 3, 5, 7, 9]
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;2.3. groupByKey&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;이미 키와 값의 쌍으로 구성된 RDD에만 적용 가능함&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd1 = sc.parallelize([&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;b&quot;,&quot;c&quot;]).map(lambda v:(v,1))
&amp;gt;&amp;gt;&amp;gt; rdd1.collect()
[('a', 1), ('b', 1), ('c', 1), ('b', 1), ('c', 1)]
/// (키,값) 쌍으로 구성된 RDD 생성

&amp;gt;&amp;gt;&amp;gt; rdd2 = rdd1.groupByKey()
&amp;gt;&amp;gt;&amp;gt; rdd2.collect()
[('b', &amp;lt;pyspark.resultiterable.ResultIterable object at 0x7fbc3f0ab6a0&amp;gt;), ('c', &amp;lt;pyspark.resultiterable.ResultIterable object at 0x7fbc3f0ab6d8&amp;gt;), ('a', &amp;lt;pyspark.resultiterable.ResultIterable object at 0x7fbc3f0ab0b8&amp;gt;)]
/// 키에 따라 그룹화함. 그 결과 키에 해당하는 시퀀스 생성

&amp;gt;&amp;gt;&amp;gt; for x in rdd2.collect():
...     print(x[0], list(x[1]))
... 
b [1, 1]
c [1, 1]
a [1]
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;2.4. cogroup&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;이미 키와 값의 쌍으로 구성된 RDD에만 적용 가능함&lt;/li&gt;
  &lt;li&gt;여러 개의 RDD를 인자로 받음(최대 3개)&lt;/li&gt;
  &lt;li&gt;여러 RDD에서 동일한 키에 해당하는 요소들로 구성된 시퀀스를 만든 후, (키, 시퀀스)의 튜플을 구성. 그 튜플들로 구성된 새로운 RDD를 생성함&lt;/li&gt;
  &lt;li&gt;Tuple(키, Tuple(rdd1요소들의 집합, rdd2요소들의 집합, …))&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd1 = sc.parallelize([(&quot;k1&quot;,&quot;v1&quot;),(&quot;k2&quot;,&quot;v2&quot;),(&quot;k1&quot;,&quot;v3&quot;)])
&amp;gt;&amp;gt;&amp;gt; rdd2 = sc.parallelize([(&quot;k1&quot;,&quot;v4&quot;)])
&amp;gt;&amp;gt;&amp;gt; rdd1.collect()
[('k1', 'v1'), ('k2', 'v2'), ('k1', 'v3')]
&amp;gt;&amp;gt;&amp;gt; rdd2.collect()
[('k1', 'v4')]
/// (키, 값)쌍으로 구성된 RDD 2개 생성

&amp;gt;&amp;gt;&amp;gt; rdd3 = rdd1.cogroup(rdd2)
&amp;gt;&amp;gt;&amp;gt; rdd3.collect()
[('k1', (&amp;lt;pyspark.resultiterable.ResultIterable object at 0x7fbc3f0b2fd0&amp;gt;, &amp;lt;pyspark.resultiterable.ResultIterable object at 0x7fbc3f00a828&amp;gt;)), ('k2', (&amp;lt;pyspark.resultiterable.ResultIterable object at 0x7fbc3f00ac18&amp;gt;, &amp;lt;pyspark.resultiterable.ResultIterable object at 0x7fbc3f00a5f8&amp;gt;))]
///k1 키에 대해 rdd1의 v1과 v3요소를 묶고, rdd2의 v4요소를 묶어서 튜플로 구성

&amp;gt;&amp;gt;&amp;gt; for x in rdd3.collect():
...     print(x[0], list(x[1][0]), list(x[1][1]))
... 
k1 ['v1', 'v3'] ['v4']
k2 ['v2'] []
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;3. 집합 연산&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;RDD에 포함된 요소를 하나의 집합으로 간주하여 집합 연산을 수행(합/교집합)&lt;/li&gt;
  &lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;3.1. distinct&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;RDD의 원소에서 중복을 제외한 요소로만 새로운 RDD 구성&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize([1,2,3,1,2,3,1,2,3])
&amp;gt;&amp;gt;&amp;gt; rdd2 = rdd.distinct()
&amp;gt;&amp;gt;&amp;gt; rdd2.collect()
[1, 2, 3]
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;3.1. cartesian&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;두 RDD요소의 카테시안곱을 구하고 그 결과를 요소로 하는 새로운 RDD구성&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd1 = sc.parallelize([1,2,3])
&amp;gt;&amp;gt;&amp;gt; rdd2 = sc.parallelize(['a','b','c'])
&amp;gt;&amp;gt;&amp;gt; rdd3 = rdd1.cartesian(rdd2)
&amp;gt;&amp;gt;&amp;gt; rdd3.collect()
[(1, 'a'), (1, 'b'), (1, 'c'), (2, 'a'), (2, 'b'), (2, 'c'), (3, 'a'), (3, 'b'), (3, 'c')]
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;3.2. subtract&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;rdd1.subtract(rdd2) : (rdd1의 요소집합 - rdd2의 요소집합)의 차집합&lt;/li&gt;
  &lt;li&gt;rdd2.subtract(rdd1) : (rdd2의 요소집합 - rdd1의 요소집합)의 차집합&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd1 = sc.parallelize([&quot;a&quot;, &quot;b&quot;,&quot;c&quot;,&quot;d&quot;,&quot;e&quot;])
&amp;gt;&amp;gt;&amp;gt; rdd2 = sc.parallelize([&quot;d&quot;,&quot;e&quot;])
&amp;gt;&amp;gt;&amp;gt; rdd3 = rdd1.subtract(rdd2)
&amp;gt;&amp;gt;&amp;gt; rdd3.collect()
['a', 'b', 'c']  
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;3.2. union&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;두 RDD요소의 합집합&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd1 = sc.parallelize([&quot;a&quot;, &quot;b&quot;,&quot;c&quot;,&quot;d&quot;,&quot;e&quot;])
&amp;gt;&amp;gt;&amp;gt; rdd2 = sc.parallelize([&quot;d&quot;,&quot;e&quot;])
&amp;gt;&amp;gt;&amp;gt; rdd3 = rdd1.union(rdd2)
&amp;gt;&amp;gt;&amp;gt; rdd3.collect()
['a', 'b', 'c', 'd', 'e', 'd', 'e']
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;3.3. intersection&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;두 RDD요소의 교집합으로 중복되지 않은 요소로 구성&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd1 = sc.parallelize([&quot;a&quot;,&quot;a&quot;,&quot;b&quot;,&quot;c&quot;])
&amp;gt;&amp;gt;&amp;gt; rdd2 = sc.parallelize([&quot;a&quot;,&quot;a&quot;,&quot;c&quot;,&quot;c&quot;])
&amp;gt;&amp;gt;&amp;gt; rdd3 = rdd1.intersection(rdd2)
&amp;gt;&amp;gt;&amp;gt; rdd3.collect()
['a', 'c']
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;3.4. join&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;RDD의 구성요소가 키와 값의 쌍으로 구성된 경우에 사용할 수 있는 메서드&lt;/li&gt;
  &lt;li&gt;공통된 키에 대해서만 join수행&lt;/li&gt;
  &lt;li&gt;join 수행 결과 Tuple(키, Tuple(첫번째 RDD요소, 두번쨰 RDD요소))&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd1 = sc.parallelize([&quot;a&quot;, &quot;b&quot;,&quot;c&quot;,&quot;d&quot;,&quot;e&quot;]).map(lambda v : (v,1))
&amp;gt;&amp;gt;&amp;gt; rdd1.collect()
[('a', 1), ('b', 1), ('c', 1), ('d', 1), ('e', 1)]
&amp;gt;&amp;gt;&amp;gt; rdd2 = sc.parallelize([&quot;b&quot;,&quot;c&quot;]).map(lambda v:(v,2))
&amp;gt;&amp;gt;&amp;gt; rdd2.collect()
[('b', 2), ('c', 2)]
&amp;gt;&amp;gt;&amp;gt; rdd3 = rdd1.join(rdd2)
&amp;gt;&amp;gt;&amp;gt; rdd3.collect()
[('b', (1, 2)), ('c', (1, 2))]
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;3.5. leftOuterJoin, rightOuterJoin&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;키와 값의 쌍으로 구성된 RDD에 사용가능&lt;/li&gt;
  &lt;li&gt;leftjoin, rightjoin을 수행&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd1 = sc.parallelize([&quot;a&quot;, &quot;b&quot;,&quot;c&quot;,&quot;d&quot;,&quot;e&quot;]).map(lambda v : (v,1))
&amp;gt;&amp;gt;&amp;gt; rdd2 = sc.parallelize([&quot;b&quot;,&quot;c&quot;]).map(lambda v:(v,2))
&amp;gt;&amp;gt;&amp;gt; rdd3 = rdd1.leftOuterJoin(rdd2)
&amp;gt;&amp;gt;&amp;gt; rdd3.collect()
[('a', (1, None)), ('e', (1, None)), ('b', (1, 2)), ('c', (1, 2)), ('d', (1, None))]
///rdd2에는 a,d,e 키가 없기 때문에 해당 키에 대한 튜플 요소는 (rdd1의 요소, None)으로 구성됨

&amp;gt;&amp;gt;&amp;gt; rdd4 = rdd1.rightOuterJoin(rdd2)
&amp;gt;&amp;gt;&amp;gt; rdd4.collect()
[('b', (1, 2)), ('c', (1, 2))]
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;3.6. subtractByKey&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;키와 값의 쌍으로 구성된 RDD에 사용가능&lt;/li&gt;
  &lt;li&gt;rdd1의 요소 중에서 rdd2와 겹치지 않는 키로 구성된 새로운 RDD 생성&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd1 = sc.parallelize([&quot;a&quot;,&quot;b&quot;]).map(lambda v:(v,1))
&amp;gt;&amp;gt;&amp;gt; rdd2 = sc.parallelize([&quot;b&quot;]).map(lambda v:(v,1))
&amp;gt;&amp;gt;&amp;gt; rdd3 = rdd1.subtractByKey(rdd2)
&amp;gt;&amp;gt;&amp;gt; rdd3.collect()
[('a', 1)]
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;4. 집계와 관련된 연산들&lt;/h3&gt;

&lt;h4&gt;4.1 reduceByKey&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;키와 값의 쌍으로 구성된 RDD에서 사용 가능&lt;/li&gt;
  &lt;li&gt;RDD 내의 동일한 키를 하나로 병합해 (키,값) 쌍으로 구성된 새로운 RDD 생성&lt;/li&gt;
  &lt;li&gt;함수를 인자로 받음.&lt;/li&gt;
  &lt;li&gt;왜냐하면, 파티션 별로 연산을 수행했을 때, 항상 같은 순서로 연산이 수행되는 것을 보장 못하므로, 함수가 수행하는 연산은 교환법칙과 결합법칙이 성립해야 함&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize(['a','b','b']).map(lambda v:(v,1))
&amp;gt;&amp;gt;&amp;gt; rdd.collect()
[('a', 1), ('b', 1), ('b', 1)]
&amp;gt;&amp;gt;&amp;gt; rdd2 = rdd.reduceByKey(lambda v1, v2:(v1+v2))
&amp;gt;&amp;gt;&amp;gt; rdd2.collect()
[('b', 2), ('a', 1)]
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;&lt;b&gt;(키,값)쌍으로 하는 RDD를 인자로 받는 트랜스포메이션 메서드&lt;/b&gt;&lt;br /&gt;
- 데이터 처리 과정에서 사용할 파티셔너와 파티션 개수를 지정할 수 있는 옵션이 있음
- 자체적으로 작성한 파티셔너나 파티션 개수를 통해 병렬 처리 수준 변경 가능
&lt;/blockquote&gt;

&lt;h4&gt;4.2 foldByKey&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;키와 값으로 구성된 RDD에 사용 가능&lt;/li&gt;
  &lt;li&gt;reduceByKey()와 유사하지만, 병합 연산의 초기값을 인자로 전달할 수 있음&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;rdd = sc.parallelize([&quot;a&quot;,&quot;b&quot;,&quot;b&quot;]).map(lambda v:(v,1))
&amp;gt;&amp;gt;&amp;gt; rdd2 = rdd.foldByKey(0, lambda v1,v2:v1+v2)
&amp;gt;&amp;gt;&amp;gt; rdd2.collect()
[('b', 2), ('a', 1)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;My view) 개인적으로, foldByKey와 reduceByKey의 차이가 잘 이해가 되지 않아, 초기값과 문자열 병합으로 pyspark를 실행해 보았습니다.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize([&quot;a&quot;,&quot;b&quot;,&quot;b&quot;]).map(lambda v:(v,1))

///초기값을 1로 준 경우
&amp;gt;&amp;gt;&amp;gt; rdd2 = rdd.foldByKey(1, lambda v1,v2:v1+v2)

///초기값을 0으로 준 경우와 다른 결과임
&amp;gt;&amp;gt;&amp;gt; rdd2.collect()
[('b', 4), ('a', 2)]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize([&quot;a&quot;,&quot;b&quot;,&quot;b&quot;]).map(lambda v:(v,'c'))

///초기값을 t로 준 경우
&amp;gt;&amp;gt;&amp;gt; rdd2 = rdd.foldByKey('t', lambda v1,v2:v1+v2)
&amp;gt;&amp;gt;&amp;gt; rdd2.collect()
[('b', 'tctc'), ('a', 'tc')]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;위의 두 연산를 보니, foldByKey는 초기값 처리를 아래와 같이 진행하는 것 같습니다. 예를 들어, 초기 rdd가 [(‘a’, 1), (‘b’, 1), (‘b’, 1)] 라 한다면, foldByKey는 키 ‘a’와 ‘b’에 대해 각각 초기값을 가지고 병합연산을 수행합니다. 이 때 먼저, 초기값을 가지고 병합 연산을 수행합니다. 키 ‘a’인 경우, 초기값 1라면, v1=1, v2=1(키 ‘a’에 대응되는 값)이 병합연산을 수행해 (‘a’,2)가 됩니다. 그 다음 reducebykey와 같은 연산을 수행하나, 키 ‘a’는 초기 rdd에 하나밖에 없기 때문에, v1=2, v2=None이 되어 최종 foldByKey연산 결과는 키 ‘a’에 대해서 2값을 가지게 됩니다.&lt;/p&gt;

&lt;p&gt;‘b’키 같은 경우도 마찬가지입니다. 먼저 초기값을 가지고 연산을 수행합니다. v1=1(초기값), v2=1(b에 대응되는 값)가 병합연산을 수행해 v1=2가 됩니다. ‘b’키는 두개가 있으므로, 나머지 ‘b’키에 대해 v1=1, v2=1가 병합연산을 거쳐 v2=2가 됩니다. 그다음 두개의 ‘b’키에 대해 다시 병합연산이 수행되어 v1=2, v2=2가 되어 최종적으로 ‘b’키에 대해 4의 값이 생성됩니다.&lt;/p&gt;

&lt;h4&gt;4.3 combineByKey&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;키와 값인 RDD에 사용 가능&lt;/li&gt;
  &lt;li&gt;foldByKey와 reduceByKey와 유사함. 차이점은 병합연산 수행 결과 값의 타입이 바뀔 수 있음&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Scala&quot;&gt;def reduceByKey(func:(V,V)=&amp;gt;V):RDD[K,V]
def foldByKey(zeroValue: V)(func:(V,V)=&amp;gt;V):RDD[K,V]
def combineByKey[C](createCombiner:(V)=&amp;gt;C, mergeValue:(C,V)=&amp;gt;V, mergeCombiners:(C,C)=&amp;gt;C):RDD[K,C]
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;combineByKey는 reduceByKey와 foldByKey와 다르게 타입이 C로 바뀌어 있음&lt;/li&gt;
  &lt;li&gt;위의 combineByKey를 보면 메서드가 총 createCombiner, mergeValue, mergeCombiners 세 개임을 알 수 있음&lt;/li&gt;
  &lt;li&gt;아래는 combineByKey에서 자주 등장하는 평균구하기 예시임&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;#record.py 에 따로 저장해야 함. 
#아니면, _pickle.PicklingError: Can't pickle &amp;lt;class '__main__.Record'&amp;gt;: attribute lookup Record on __main__ failed 에러 발생.

class Record:

    def __init__(self, amount, number=1):
        self.amount = amount
        self.number = number
        
    def addAmt(self, amount):
        return Record(self.amount + amount, self.number + 1)
    
    def __add__(self, other):
        amount = self.amount + other.amount
        number = self.number + other.number 
        return Record(amount, number)
        
    def __str__(self):
        return &quot;avg:&quot; + str(self.amount / self.number)

    def __repr__(self):
        return 'Record(%r, %r)' % (self.amount, self.number)
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#createCombiner, mergeValue, mergeCombiners 정의
def createCombiner(v):
    return Record(v)

def mergeValue(c, v):
    return c.addAmt(v)

def mergeCombiners(c1, c2):
    return c1 + c2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;#combineByKey 실행
rdd = sc.parallelize([(&quot;Math&quot;, 100), (&quot;Eng&quot;, 80), (&quot;Math&quot;, 50), (&quot;Eng&quot;, 70), (&quot;Eng&quot;, 90)])
rdd2 = rdd.combineByKey(lambda v:createCombiner(v), lambda c,v:mergeValue(c,v), lambda c1,c2:mergeCombiners(c1,c2))
print('Math', rdd2.collectAsMap()['Math'], 'Eng', rdd2.collectAsMap()['Eng'])
&amp;gt;&amp;gt;&amp;gt; Math avg:75.0 Eng avg:80.0
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;createCombiner()
    &lt;ul&gt;
      &lt;li&gt;값을 병합하기 위한 컴바이너. 컴바이너는 병합을 수행하고 컴바이너 타입으로 내부에 저장함. 위의 예로는 createCombiner()는 Record클래스 타입으로 저장됨. 즉, rdd의 각 키에 해당되는 값이 record클래스 타입으로 변환되어 저장됨.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;mergeValue()
    &lt;ul&gt;
      &lt;li&gt;키에 대한 컴바이너가 존재한다면, 컴바이너를 이용해 값을 병합함.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;My View) 위의 예 같은 경우, rdd요소 순서대로 combineByKey가 작동한다면 먼저 (“Math”, 100)에 대해 Combiner가 생성되어, math 키에 대한 Record(100)이 생성됩니다.&lt;/p&gt;

&lt;p&gt;그 다음 (“Eng”, 70)에 대해 작동합니다. “Eng”키는 기존에 없던 키이기 때문에 새로 Combiner인 Record(70)이 생성됩니다.&lt;/p&gt;

&lt;p&gt;그 다음 (“Math”, 50)인데, 기존 Math키에 대한 컴바이너가 존재하기 때문에, 기존 Math 키 컴바이너를 이용하여 병합합니다. 즉, Record(100).addAmt(50)이 발생합니다. 이렇게 Math키와 Eng키에 대해 모든 요소에 대해 병합을 실시하게 됩니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;mergeCombiners()
    &lt;ul&gt;
      &lt;li&gt;createCombiner()와 mergeValue()는 파티션별로 수행됨. 그다음 모든 파티션에 생성된 combiner를 병합하는 과정을 mergeCombiners()를 통해 수행하는 것임. 이를 통해 최종결과가 발생함&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;4.4 aggregateByKey&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;키와 값의 RDD에서 사용 가능&lt;/li&gt;
  &lt;li&gt;초깃값을 설정할 수 있는 점을 제외하면 comebineByKey와 동일&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Scala&quot;&gt;def combineByKey[C](createCombiner:(V)=&amp;gt;C, mergeValue:(C,V)=&amp;gt;V, mergeCombiners:(C,C)=&amp;gt;C):RDD[(K,C)]
def aggregateByKey[U](zeroValue: U)(seqOp:(U,V)=&amp;gt;U, combOp:(U,U)=&amp;gt;U):RDD[(K,U)]
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;aggregateByKey에서 seqOp는 mergeValue역할을, comOp는 mergeCombiner역할을 함.&lt;/li&gt;
  &lt;li&gt;combineByKey에서 createCombiner로 병합을 위한 초깃값을 구하지만 aggregateByKey는 함수를 이용해 초깃값을 설정하는 대신 바로 ‘값’으로 초기값을 설정함&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;rdd = sc.parallelize([(&quot;Math&quot;, 100), (&quot;Eng&quot;, 80), (&quot;Math&quot;, 50), (&quot;Eng&quot;, 70), (&quot;Eng&quot;, 90)])
rdd2 = rdd.aggregateByKey(Record(0,0), lambda c,v:mergeValue(c,v), lambda c1,c2:mergeCombiners(c1,c2))
print('Math :', rdd2.collectAsMap()['Math'], 'Eng :', rdd2.collectAsMap()['Eng'])
&amp;gt;&amp;gt;&amp;gt;Math : avg:75.0 Eng : avg:80.0
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;5. pipe 및 파티션과 관련된 연산&lt;/h3&gt;

&lt;h4&gt;5.1 pipe&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;pipe를 이용하면 데이터를 처리하는 과정에서 외부 프로세스를 활용할 수 있음&lt;/li&gt;
  &lt;li&gt;세 개의 숫자로 구성된 문자열을 리눅스의 cut 유틸리티를 이용해 분리한 뒤첫번째와 세번재 숫자를 뽑아내는 예제&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize([&quot;1,2,3&quot;, &quot;4,5,6&quot;, &quot;7,8,9&quot;])
&amp;gt;&amp;gt;&amp;gt; rdd2 = rdd.pipe(&quot;cut -f 1,3 -d ,&quot;)
&amp;gt;&amp;gt;&amp;gt; rdd2.collect()
['1,3', '4,6', '7,9']
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;5.2. coalesce와 repartition&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;현재 RDD에 사용된 파티션 개수를 조정함&lt;/li&gt;
  &lt;li&gt;coalesce는 파티션 개수를 줄이기만 되고, repartition은 늘리는 것과 줄이는 것 둘 다 가능&lt;/li&gt;
  &lt;li&gt;coalesce가 따로 있는 이유는 처리 방식에 따른 성능 차이 때문임
    &lt;ul&gt;
      &lt;li&gt;repartition은 셔플을 기반으로 동작을 수행하는 데 반해, coalesce는 강제로 셔플을 수행하라는 옵션을 지정하지 않는 한 셔플을 사용하지 않음.&lt;/li&gt;
      &lt;li&gt;따라서 필터링 등으로 인해 데이터 개수가 줄어든 경우 coalesce을 사용하는 것이 좋음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize(list(range(1,11)),10)
&amp;gt;&amp;gt;&amp;gt; rdd2 = rdd.coalesce(5)
&amp;gt;&amp;gt;&amp;gt; rdd3 = rdd2.repartition(10)
&amp;gt;&amp;gt;&amp;gt; print(&quot;partition size : %d&quot; 
&amp;gt;&amp;gt;&amp;gt; print(&quot;partition size : %d&quot; %rdd2.getNumPartitions())
partition size : 5
&amp;gt;&amp;gt;&amp;gt; print(&quot;partition size : %d&quot; %rdd3.getNumPartitions())
partition size : 10
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;5.3. repartitionAndSortWithinPartitions&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;키와 값으로 구성된 RDD에서 사용 가능&lt;/li&gt;
  &lt;li&gt;RDD를 구성하는 모든 데이터를 특정 기준에 따라 여러 개의 파티션으로 분리하고 각 파티션 단위로 정렬을 수행한 뒤 새로운 RDD를 생성해 주는 메서드임&lt;/li&gt;
  &lt;li&gt;각 데이터가 어떤 파티션에 속할지 결정하기 위한 파티셔너(org.apache.spark.Partitioner)설정
    &lt;ul&gt;
      &lt;li&gt;키 값을 이용하여 어떤 파티션에 속할지 결정할 뿐만 아니라 키 값을 이용한 정렬도 수행함&lt;/li&gt;
      &lt;li&gt;파티션 재할당을 위해 셔플을 수행하는 단계에서 정렬도 함께 다루게 되어 파티션과 정렬을 각각 따로하는 것에 비해 더 높은 성능을 발휘할 수 있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;10개의 무작위 숫자를 위 메서드를 이용해 3개의 파티션으로 분리해 보는 예제&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; data = [random.randrange(1,100) for i in range(0,10)]
&amp;gt;&amp;gt;&amp;gt; rdd1 = sc.parallelize(data).map(lambda v:(v,&quot;-&quot;))
&amp;gt;&amp;gt;&amp;gt; rdd2 = rdd1.repartitionAndSortWithinPartitions(3, lambda x:x)
&amp;gt;&amp;gt;&amp;gt; rdd2.foreachPartition(lambda values:print(list(values)))
[(50, '-')]
[(16, '-'), (52, '-'), (61, '-'), (67, '-')]
[(6, '-'), (12, '-'), (48, '-'), (51, '-'), (87, '-')]
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;pyspark에서 repartitionAndSortWithinPartitions에서 default 파티셔너는 hash 파티셔너로 되어있음&lt;/li&gt;
  &lt;li&gt;foreachPartition은 partition단위로 특정함수를 실행해주는 메서드임. 위의 예제에서는 파티션단위로 파티션에 속해있는 값을 프린트해주는 함수를 실행했음&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;5.4. partitionBy&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;키와 값으로 구성된 RDD에서 사용가능&lt;/li&gt;
  &lt;li&gt;파티션을 변경하고 싶을 때 사용가능&lt;/li&gt;
  &lt;li&gt;기본적으로, hashpartitioner와 rangepartitioner가 있음&lt;/li&gt;
  &lt;li&gt;org.apache.spark.partitioner 클래스를 상속해서 파티셔너를 커스터마이징도 가능&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd1 = sc.parallelize([(&quot;apple&quot;,1),(&quot;mouse&quot;,1),(&quot;monitor&quot;,1)],5)
&amp;gt;&amp;gt;&amp;gt; rdd1.collect()
[('apple', 1), ('mouse', 1), ('monitor', 1)]
&amp;gt;&amp;gt;&amp;gt; rdd2 = rdd1.partitionBy(3)
&amp;gt;&amp;gt;&amp;gt; print(&quot;rdd1: %d, rdd2: %d&quot; %(rdd1.getNumPartitions(), rdd2.getNumPartitions()))
rdd1: 5, rdd2: 3
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;위의 예제에서 partitionby에 의해 파티션 갯수가 변경된 것을 확인할 수 있음&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;6. 필터와 정렬 연산&lt;/h3&gt;
&lt;p&gt;특정 조건을 만족하는 요소만 선택하거나, 각 요소를 정해진 기준에 따라 정렬함&lt;/p&gt;

&lt;h4&gt;6.1. filter&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;RDD의 각각 요소에 조건에 따라 True/False로 가려내는 함수를 적용하여 True에 해당하는 요소만 걸러냄&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd1 = sc.parallelize(range(1,6))
&amp;gt;&amp;gt;&amp;gt; rdd2 = rdd1.filter(lambda i:i&amp;gt;2)
&amp;gt;&amp;gt;&amp;gt; print(rdd2.collect())
[3, 4, 5]
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;6.2. sortByKey&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;키 값을 기준으로 요소를 정렬하는 연산임&lt;/li&gt;
  &lt;li&gt;따라서, 키와 값으로 구성된 RDD에 적용 가능함&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd1 = sc.parallelize([(&quot;q&quot;,1),(&quot;z&quot;,1),(&quot;a&quot;,1)])
&amp;gt;&amp;gt;&amp;gt; result = rdd1.sortByKey()
&amp;gt;&amp;gt;&amp;gt; print(result.collect())
[('a', 1), ('q', 1), ('z', 1)]
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;6.3. keys, values&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;키와 값으로 구성된 RDD에 적용 가능함&lt;/li&gt;
  &lt;li&gt;keys는 RDD의 키 요소로 구성된 RDD를 생성하고, values는 RDD의 value요소로 구성된 RDD를 생성함&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd1 = sc.parallelize([(&quot;q&quot;,1),(&quot;z&quot;,1),(&quot;a&quot;,1)])
&amp;gt;&amp;gt;&amp;gt; rdd2 = rdd1.keys()
&amp;gt;&amp;gt;&amp;gt; rdd3 = rdd1.values()
&amp;gt;&amp;gt;&amp;gt; print(rdd2.collect(), rdd3.collect())
['q', 'z', 'a'] [1, 1, 1]
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;6.4. sample&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;샘플을 추출하는 RDD메서드&lt;/li&gt;
  &lt;li&gt;sample(withReplacement, fraction, seed=None)
    &lt;ul&gt;
      &lt;li&gt;withReplacement : True/False복원추출 결정&lt;/li&gt;
      &lt;li&gt;fraction
        &lt;ul&gt;
          &lt;li&gt;복원추출인 경우, RDD각 요소당 평균 추출횟수를 의미함&lt;/li&gt;
          &lt;li&gt;비복원추출인 경우, RDD각 요소당 샘플될 확률을 의미함&lt;/li&gt;
          &lt;li&gt;fraction이 sample사이즈를 결정하는 것은 아님. 아래 예제를 보면, sample사이즈는 random한 것을 알 수 있음.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd1 = sc.parallelize(range(100))
&amp;gt;&amp;gt;&amp;gt; rdd2 = rdd1.sample(True, 1.5, seed=1234)
&amp;gt;&amp;gt;&amp;gt; rdd3 = rdd1.sample(False, 0.2, seed=1234)
&amp;gt;&amp;gt;&amp;gt; 
&amp;gt;&amp;gt;&amp;gt; rdd2.collect()
[1, 1, 2, 6, 6, 7, 7, 9, 10, 10, 11, 12, 12, 12, 13, 15, 17, 18, 19, 19, 19, 20, 21, 21, 23, 24, 25, 25, 26, 26, 26, 26, 26, 27, 28, 28, 28, 29, 30, 31, 32, 33, 33, 34, 35, 36, 36, 36, 37, 37, 38, 39, 39, 42, 42, 44, 44, 45, 45, 46, 48, 48, 48, 49, 49, 49, 50, 50, 50, 51, 53, 54, 57, 58, 59, 60, 63, 64, 65, 65, 66, 69, 71, 71, 71, 72, 72, 72, 73, 73, 75, 76, 77, 79, 80, 80, 81, 84, 84, 85, 85, 85, 86, 88, 88, 88, 89, 89, 89, 90, 90, 91, 91, 92, 92, 92, 94, 94, 95, 95, 95, 95, 96, 97, 97, 99]
&amp;gt;&amp;gt;&amp;gt; rdd3.collect()
[0, 5, 6, 7, 8, 11, 15, 35, 39, 41, 55, 56, 58, 61, 62, 71, 72, 78, 81, 89, 90, 93, 97, 99]
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;이상으로 본 포스팅을 마치겠습니다. 다음 포스팅은 &amp;lt;RDD, Resilient Distributed Dataset에 대하여[3] - RDD액션, RDD데이터 불러오기와 저장하기&amp;gt; 에 대해 진행하도록 하겠습니다.&lt;/p&gt;</content><author><name>Seonhwa Lee</name></author><category term="Spark Programming" /><category term="data-engineering" /><summary type="html">이번 포스팅은 지난 포스팅 &amp;lt;RDD, Resilient Distributed DataSet에 대하여[1]&amp;gt; 에 이어서 진행하도록 하겠습니다. 교재는 빅데이터 분석을 위한 스파크2 프로그래밍을 참고하였습니다. 2. RDD 2.1.1 들어가기에 앞서 지난 포스팅 2-1. RDD Resilient Distributed Dataset에 대하여 에서 다뤘습니다. 2.1.2. 스파크컨텍스트 생성 스파크 컨텍스트는 스파크 애플리케이션과 클러스터의 연결을 관리하는 객체임 따라서, 스파크 애플리케이션을 사용하려면 무조건 스파크 컨텍스트를 생성하고 이용해야 함 RDD 생성도 스파크컨텍스트를 이용해 생성 가능함 val conf = new SparkConf().setMaster(“local[*]”).setAppName(“RDDCreateSample”) val sc = new SparkContext(conf) [예] scala - sparkcontext 생성   sc = SparkContext(master=“local[*], appName=“RDDCreateTest”, conf=conf) [예] python - spark context 생성   스파크 동작에 필요한 여러 설정 정보 지정 가능함 SparkConf(), conf=conf 부분에서 config을 통과시켜서 지정 가능함 지정해야 하는 정보 중에, master 정보와 appName 정보는 필수 지정 정보임 master 정보란 ? 스파크가 동작할 클러스터의 마스터 서버를 의미하는 것. 로컬모드에서 local, local[3], local[*]와 같이 사용. [*]는 쓰레드 개수를 의미하며, *는 사용 가능한 모든 쓰레드를 이용하겠다는 이야기임 appName은 애플리케이션 이름으로, 구분하기 위한 목적임. 스파크 UI화면에 사용됨 2.1.3. RDD생성 RDD를 생성하는 방법은 크게 2가지임 드라이버 프로그램의 컬렉션 객체 이용 자바 or 파이썬 ? 리스트 이용, 스칼라 ? 시퀀스타입 이용 드라이버 프로그램? - 최초로 메인 함수를 실행해 RDD등을 생성하고 각종 연산을 호출하는 프로그램 - 드라이버 내의 메인 함수는 스파크 애플리케이션과 스파크 컨텍스트 객체를 생성함 - 스파크 컨텍스트를 통해 RDD의 연산 정보를 DAG스케쥴러에 전달하면 스케쥴러는 이 정보를 가지고 실행 계획을 수립한 후 이를 클러스터 매니저에게 전달함 val rdd1 = sc.parallelize(List(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;,&quot;e&quot;)) [예] scala - rdd 생성  [예] python - 드라이버의 컬렉션 객체를 이용한 RDD 생성 문자열을 포함한 컬렉션 객체 생성 example) python : ['a','b','c','d'] parallelize() 메서드를 이용해 RDD 생성 - RDD의 파티션 수를 지정하고 싶을 때, parallelize() 메서드의 두 번째 매개변수로 파티션 개수 지정 가능 val rdd1 = sc.parallelize(1 to 1000, 10) 외부 데이터를 읽어서 새로운 RDD를 생성 기본적으로 하둡의 다루는 모든 입출력 유형 가능 내부적으로 하둡의 입출력을 사용하기 때문임 [예] python - 외부데이터를 이용한 RDD 생성 2.1.4 RDD 기본 액션 기본 액션 연산의 종류에 대해 알아보도록 하겠습니다. 1. collect collect은 RDD의 모든 원소를 모아서 배열로 리턴 반환 타입이 RDD가 아닌 배열이므로 액션 연산 RDD에 있는 모든 요소들이 서버의 메모리에 수집됨. 즉, 대용량 데이터를 다룰 땐 조심하고, 주로 작은 용량의 데이터 디버깅용으로 사용함 [예] python - collect 연산 2. count RDD 구성하는 전체 요소 개수 반환 2.1.5 RDD 트랜스포메이션 기존 RDD를 이용해 새로운 RDD를 생성하는 연산입니다. 1. Map 연산 RDD에 속하는 모든 요소에 적용하여 새로운 RDD 생성하는 연산 RDD의 몇몇 연산은 특정 데이터 타입에만 적용 가능함 1.1. map 하나의 인자를 받는 함수 자체가 map의 인자로 들어감 이 함수를 이용해 rdd의 모든 요소에 적용한 뒤 새로운 RDD 리턴 [예] python - map 연산 map()에 전달되는 함수의 입력 데이터 타입과 출력 데이터 타입이 일치할 필요 없음. 문자열을 입력받아 정수로 반환하는 함수 사용 가능 [예] python - map 연산, 입력/출력 일치하지 않는 경우 1.2. flatMap map()과 마찬가지로, 하나의 인자를 받는 함수가 flatMap의 인자로 들어감 map()과 차이점은 각 함수의 인자가 반환하는 값의 타입이 다름 flatMap()에 사용하는 함수 f는 반환값으로 리스트나 시퀀스 같은 여러 개의 값을 담은 (이터레이션이 가능한) 일종의 컬렉션과 유사한 타입의 값을 반환해야 함 map[U](f:(T) -&amp;gt; U):RDD[U] flatMapU:RDD[U])\ TraversableOnce는 이터레이터 타입을 의미 map()과 flatMap() 차이점 예시 [예] python - map 연산 vs. flatMap 연산 map연산은 문자열의 배열로 구성된 RDD를 생성함 각 요소의 문자열(T)이 단어가 포함된 배열(U)이기 때문임 반면, flatMap 연산은 문자열로 구성된 RDD를 생성함 TraversableOnce(U)이기 때문에 문자열의 배열 내의 요소가 모두 끄집어져 나오는 작업을 하게 됨 flatMap()은 하나의 입력값(“apple, orange”)에 대해 출력 값이 여러개인 경우([“apple”, “orange”]) 유용하게 사용할 수 있음 1.3. mapPartitions map과 flatMap은 하나의 인자만을 받는 함수가 인자로 들어가지만, mapPartitions은 여러 인자를 받는 함수가 인자로 들어갈 수 있음 ex) 이터레이터를 인자로 받는 함수 mapartitions은 인자로 받은 함수가 파티션 단위로 적용하여 새로운 RDD를 생성함. 반면에, map과 flatMap은 인자로 받은 함수가 요소 한개 단위로 적용됨 [예] python - mapPartitions sc.parallelize(range(1,11),3)으로 파티션 3개로 나뉨 DB 연결!!! 가 세번 출력된 걸 보니 파티션 단위로 처리한 것을 확인할 수 있음 increase함수는 각 파티션 내의 요소에 대한 이터레이터를 전달받아 함수 내부에서 파티션의 개별 요소에 대한 작업을 처리하고 그 결과를 다시 이터레이터 타입으로 반환 1.4. mapPartitionsWithIndex mapPartions와 동일하고 다른 점은 인자로 전달되는 함수를 호출할 때 파티션에 속한 요소의 정보뿐만 아니라 해당 파티션의 인덱스 정보도 함께 전달해 준다는 것임 def IncreaseWithIndex(idx, numbers): for i in numbers: if(idx == 1): yield i+1 mapPartitionswithIndex에 인자로 들어갈 함수는 위와 같이 인덱스 정보도 같이 들어감 [예] python - mapPartitionsWithIndex 1.5. mapValues RDD의 모든 요소들이 키와 값의 쌍을 이루고 있는 경우에만 사용 가능한 메서드이며, 인자로 전달받은 함수를 “값”에 해당하는 요소에만 적용하고 그 결과로 구성된 새로운 RDD를 생성 rdd1 = sc.parallelize([&quot;a&quot;,&quot;b&quot;,&quot;c&quot;]).map(lambda v:(v,1)) //(키,값)으로 구성된 rdd생성 rdd2 = rdd1.mapValues(lambda i:i+1) print(rdd2.collect()) [예] python - mapValues 1.6. flatMapValues MapValues 처럼 키에 해당되는 값에 함수를 적용하나 flatMap() 연산을 적용할 수 있음 rdd1 = sc.parallelize([(1, &quot;a,b&quot;),(2, &quot;a,c&quot;),(3, &quot;d,e&quot;)]) rdd2 = rdd1.flatMapValues(lambda v:v.split(',')) rdd2.collect() [예] python - flatMapValues 2. 그룹화 연산 특정 조건에 따라 요소를 그룹화하거나 특정 함수 적용 2.1. zip 두 개의 서로 다른 RDD를 각 요소의 인덱스에 따라 첫번째 RDD의 ‘인덱스’번째를 키로, 두번째 RDD의 ‘인덱스’번째를 값으로 하는 순서쌍을 생성 두 개 RDD는 같은 개수의 파티션과 각 파티션 당 요소개수가 동일해야 함 rdd1 = sc.parallelize([&quot;a&quot;,&quot;b&quot;,&quot;c&quot;]) rdd2 = sc.parallelize([1,2,3]) rdd3 = rdd1.zip(rdd2) rdd3.collect() &amp;gt;&amp;gt;&amp;gt; [('a', 1), ('b', 2), ('c', 3)] rdd1 = sc.parallelize(range(1,10),3) rdd2 = sc.parallelize(range(11,20),3) rdd3 = rdd1.zip(rdd2) rdd3.collect() &amp;gt;&amp;gt;&amp;gt; [(1, 11), (2, 12), (3, 13), (4, 14), (5, 15), (6, 16), (7, 17), (8, 18), (9, 19)] 2.2. zipPartitions zip()과 다르게 파티션의 개수만 동일하면 됨 zipPartitions()은 최대 4개 RDD까지 인자로 넣을 수 있음 파이썬 사용불가!! 2.3. groupBy RDD의 요소를 일정한 기준에 따라 그룹을 나누고, 각 그룹으로 구성된 새로운 RDD를 생성함 각 그룹은 키와 각 키에 속한 요소의 시퀀스(iterator)로 구성됨 인자로 전달하는 함수가 각 그룹의 키를 결정하는 역할을 담당함 &amp;gt;&amp;gt;&amp;gt; rdd1 = sc.parallelize(range(1,11)) &amp;gt;&amp;gt;&amp;gt; rdd2 = rdd1.groupBy(lambda v: &quot;even&quot; if v%2==0 else &quot;odd&quot;) /// groupBy에 인자로 전달된 함수에 의해 키(even/odd) 결정 print(rdd2.collect()) &amp;gt;&amp;gt;&amp;gt; [('even', &amp;lt;pyspark.resultiterable.ResultIterable object at 0x7fbc3f090e80&amp;gt;), ('odd', &amp;lt;pyspark.resultiterable.ResultIterable object at 0x7fbc3f090470&amp;gt;)] /// 각 키에 해당하는 값은 iterator임을 확인할 수 있음 &amp;gt;&amp;gt;&amp;gt; for x in rdd2.collect(): ... print(x[0], list(x[1])) ... even [2, 4, 6, 8, 10] odd [1, 3, 5, 7, 9] 2.3. groupByKey 이미 키와 값의 쌍으로 구성된 RDD에만 적용 가능함 &amp;gt;&amp;gt;&amp;gt; rdd1 = sc.parallelize([&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;b&quot;,&quot;c&quot;]).map(lambda v:(v,1)) &amp;gt;&amp;gt;&amp;gt; rdd1.collect() [('a', 1), ('b', 1), ('c', 1), ('b', 1), ('c', 1)] /// (키,값) 쌍으로 구성된 RDD 생성 &amp;gt;&amp;gt;&amp;gt; rdd2 = rdd1.groupByKey() &amp;gt;&amp;gt;&amp;gt; rdd2.collect() [('b', &amp;lt;pyspark.resultiterable.ResultIterable object at 0x7fbc3f0ab6a0&amp;gt;), ('c', &amp;lt;pyspark.resultiterable.ResultIterable object at 0x7fbc3f0ab6d8&amp;gt;), ('a', &amp;lt;pyspark.resultiterable.ResultIterable object at 0x7fbc3f0ab0b8&amp;gt;)] /// 키에 따라 그룹화함. 그 결과 키에 해당하는 시퀀스 생성 &amp;gt;&amp;gt;&amp;gt; for x in rdd2.collect(): ... print(x[0], list(x[1])) ... b [1, 1] c [1, 1] a [1] 2.4. cogroup 이미 키와 값의 쌍으로 구성된 RDD에만 적용 가능함 여러 개의 RDD를 인자로 받음(최대 3개) 여러 RDD에서 동일한 키에 해당하는 요소들로 구성된 시퀀스를 만든 후, (키, 시퀀스)의 튜플을 구성. 그 튜플들로 구성된 새로운 RDD를 생성함 Tuple(키, Tuple(rdd1요소들의 집합, rdd2요소들의 집합, …)) &amp;gt;&amp;gt;&amp;gt; rdd1 = sc.parallelize([(&quot;k1&quot;,&quot;v1&quot;),(&quot;k2&quot;,&quot;v2&quot;),(&quot;k1&quot;,&quot;v3&quot;)]) &amp;gt;&amp;gt;&amp;gt; rdd2 = sc.parallelize([(&quot;k1&quot;,&quot;v4&quot;)]) &amp;gt;&amp;gt;&amp;gt; rdd1.collect() [('k1', 'v1'), ('k2', 'v2'), ('k1', 'v3')] &amp;gt;&amp;gt;&amp;gt; rdd2.collect() [('k1', 'v4')] /// (키, 값)쌍으로 구성된 RDD 2개 생성 &amp;gt;&amp;gt;&amp;gt; rdd3 = rdd1.cogroup(rdd2) &amp;gt;&amp;gt;&amp;gt; rdd3.collect() [('k1', (&amp;lt;pyspark.resultiterable.ResultIterable object at 0x7fbc3f0b2fd0&amp;gt;, &amp;lt;pyspark.resultiterable.ResultIterable object at 0x7fbc3f00a828&amp;gt;)), ('k2', (&amp;lt;pyspark.resultiterable.ResultIterable object at 0x7fbc3f00ac18&amp;gt;, &amp;lt;pyspark.resultiterable.ResultIterable object at 0x7fbc3f00a5f8&amp;gt;))] ///k1 키에 대해 rdd1의 v1과 v3요소를 묶고, rdd2의 v4요소를 묶어서 튜플로 구성 &amp;gt;&amp;gt;&amp;gt; for x in rdd3.collect(): ... print(x[0], list(x[1][0]), list(x[1][1])) ... k1 ['v1', 'v3'] ['v4'] k2 ['v2'] [] 3. 집합 연산 RDD에 포함된 요소를 하나의 집합으로 간주하여 집합 연산을 수행(합/교집합) 3.1. distinct RDD의 원소에서 중복을 제외한 요소로만 새로운 RDD 구성 &amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize([1,2,3,1,2,3,1,2,3]) &amp;gt;&amp;gt;&amp;gt; rdd2 = rdd.distinct() &amp;gt;&amp;gt;&amp;gt; rdd2.collect() [1, 2, 3] 3.1. cartesian 두 RDD요소의 카테시안곱을 구하고 그 결과를 요소로 하는 새로운 RDD구성 &amp;gt;&amp;gt;&amp;gt; rdd1 = sc.parallelize([1,2,3]) &amp;gt;&amp;gt;&amp;gt; rdd2 = sc.parallelize(['a','b','c']) &amp;gt;&amp;gt;&amp;gt; rdd3 = rdd1.cartesian(rdd2) &amp;gt;&amp;gt;&amp;gt; rdd3.collect() [(1, 'a'), (1, 'b'), (1, 'c'), (2, 'a'), (2, 'b'), (2, 'c'), (3, 'a'), (3, 'b'), (3, 'c')] 3.2. subtract rdd1.subtract(rdd2) : (rdd1의 요소집합 - rdd2의 요소집합)의 차집합 rdd2.subtract(rdd1) : (rdd2의 요소집합 - rdd1의 요소집합)의 차집합 &amp;gt;&amp;gt;&amp;gt; rdd1 = sc.parallelize([&quot;a&quot;, &quot;b&quot;,&quot;c&quot;,&quot;d&quot;,&quot;e&quot;]) &amp;gt;&amp;gt;&amp;gt; rdd2 = sc.parallelize([&quot;d&quot;,&quot;e&quot;]) &amp;gt;&amp;gt;&amp;gt; rdd3 = rdd1.subtract(rdd2) &amp;gt;&amp;gt;&amp;gt; rdd3.collect() ['a', 'b', 'c'] 3.2. union 두 RDD요소의 합집합 &amp;gt;&amp;gt;&amp;gt; rdd1 = sc.parallelize([&quot;a&quot;, &quot;b&quot;,&quot;c&quot;,&quot;d&quot;,&quot;e&quot;]) &amp;gt;&amp;gt;&amp;gt; rdd2 = sc.parallelize([&quot;d&quot;,&quot;e&quot;]) &amp;gt;&amp;gt;&amp;gt; rdd3 = rdd1.union(rdd2) &amp;gt;&amp;gt;&amp;gt; rdd3.collect() ['a', 'b', 'c', 'd', 'e', 'd', 'e'] 3.3. intersection 두 RDD요소의 교집합으로 중복되지 않은 요소로 구성 &amp;gt;&amp;gt;&amp;gt; rdd1 = sc.parallelize([&quot;a&quot;,&quot;a&quot;,&quot;b&quot;,&quot;c&quot;]) &amp;gt;&amp;gt;&amp;gt; rdd2 = sc.parallelize([&quot;a&quot;,&quot;a&quot;,&quot;c&quot;,&quot;c&quot;]) &amp;gt;&amp;gt;&amp;gt; rdd3 = rdd1.intersection(rdd2) &amp;gt;&amp;gt;&amp;gt; rdd3.collect() ['a', 'c'] 3.4. join RDD의 구성요소가 키와 값의 쌍으로 구성된 경우에 사용할 수 있는 메서드 공통된 키에 대해서만 join수행 join 수행 결과 Tuple(키, Tuple(첫번째 RDD요소, 두번쨰 RDD요소)) &amp;gt;&amp;gt;&amp;gt; rdd1 = sc.parallelize([&quot;a&quot;, &quot;b&quot;,&quot;c&quot;,&quot;d&quot;,&quot;e&quot;]).map(lambda v : (v,1)) &amp;gt;&amp;gt;&amp;gt; rdd1.collect() [('a', 1), ('b', 1), ('c', 1), ('d', 1), ('e', 1)] &amp;gt;&amp;gt;&amp;gt; rdd2 = sc.parallelize([&quot;b&quot;,&quot;c&quot;]).map(lambda v:(v,2)) &amp;gt;&amp;gt;&amp;gt; rdd2.collect() [('b', 2), ('c', 2)] &amp;gt;&amp;gt;&amp;gt; rdd3 = rdd1.join(rdd2) &amp;gt;&amp;gt;&amp;gt; rdd3.collect() [('b', (1, 2)), ('c', (1, 2))] 3.5. leftOuterJoin, rightOuterJoin 키와 값의 쌍으로 구성된 RDD에 사용가능 leftjoin, rightjoin을 수행 &amp;gt;&amp;gt;&amp;gt; rdd1 = sc.parallelize([&quot;a&quot;, &quot;b&quot;,&quot;c&quot;,&quot;d&quot;,&quot;e&quot;]).map(lambda v : (v,1)) &amp;gt;&amp;gt;&amp;gt; rdd2 = sc.parallelize([&quot;b&quot;,&quot;c&quot;]).map(lambda v:(v,2)) &amp;gt;&amp;gt;&amp;gt; rdd3 = rdd1.leftOuterJoin(rdd2) &amp;gt;&amp;gt;&amp;gt; rdd3.collect() [('a', (1, None)), ('e', (1, None)), ('b', (1, 2)), ('c', (1, 2)), ('d', (1, None))] ///rdd2에는 a,d,e 키가 없기 때문에 해당 키에 대한 튜플 요소는 (rdd1의 요소, None)으로 구성됨 &amp;gt;&amp;gt;&amp;gt; rdd4 = rdd1.rightOuterJoin(rdd2) &amp;gt;&amp;gt;&amp;gt; rdd4.collect() [('b', (1, 2)), ('c', (1, 2))] 3.6. subtractByKey 키와 값의 쌍으로 구성된 RDD에 사용가능 rdd1의 요소 중에서 rdd2와 겹치지 않는 키로 구성된 새로운 RDD 생성 &amp;gt;&amp;gt;&amp;gt; rdd1 = sc.parallelize([&quot;a&quot;,&quot;b&quot;]).map(lambda v:(v,1)) &amp;gt;&amp;gt;&amp;gt; rdd2 = sc.parallelize([&quot;b&quot;]).map(lambda v:(v,1)) &amp;gt;&amp;gt;&amp;gt; rdd3 = rdd1.subtractByKey(rdd2) &amp;gt;&amp;gt;&amp;gt; rdd3.collect() [('a', 1)] 4. 집계와 관련된 연산들 4.1 reduceByKey 키와 값의 쌍으로 구성된 RDD에서 사용 가능 RDD 내의 동일한 키를 하나로 병합해 (키,값) 쌍으로 구성된 새로운 RDD 생성 함수를 인자로 받음. 왜냐하면, 파티션 별로 연산을 수행했을 때, 항상 같은 순서로 연산이 수행되는 것을 보장 못하므로, 함수가 수행하는 연산은 교환법칙과 결합법칙이 성립해야 함 &amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize(['a','b','b']).map(lambda v:(v,1)) &amp;gt;&amp;gt;&amp;gt; rdd.collect() [('a', 1), ('b', 1), ('b', 1)] &amp;gt;&amp;gt;&amp;gt; rdd2 = rdd.reduceByKey(lambda v1, v2:(v1+v2)) &amp;gt;&amp;gt;&amp;gt; rdd2.collect() [('b', 2), ('a', 1)] (키,값)쌍으로 하는 RDD를 인자로 받는 트랜스포메이션 메서드 - 데이터 처리 과정에서 사용할 파티셔너와 파티션 개수를 지정할 수 있는 옵션이 있음 - 자체적으로 작성한 파티셔너나 파티션 개수를 통해 병렬 처리 수준 변경 가능 4.2 foldByKey 키와 값으로 구성된 RDD에 사용 가능 reduceByKey()와 유사하지만, 병합 연산의 초기값을 인자로 전달할 수 있음 rdd = sc.parallelize([&quot;a&quot;,&quot;b&quot;,&quot;b&quot;]).map(lambda v:(v,1)) &amp;gt;&amp;gt;&amp;gt; rdd2 = rdd.foldByKey(0, lambda v1,v2:v1+v2) &amp;gt;&amp;gt;&amp;gt; rdd2.collect() [('b', 2), ('a', 1)] My view) 개인적으로, foldByKey와 reduceByKey의 차이가 잘 이해가 되지 않아, 초기값과 문자열 병합으로 pyspark를 실행해 보았습니다. &amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize([&quot;a&quot;,&quot;b&quot;,&quot;b&quot;]).map(lambda v:(v,1)) ///초기값을 1로 준 경우 &amp;gt;&amp;gt;&amp;gt; rdd2 = rdd.foldByKey(1, lambda v1,v2:v1+v2) ///초기값을 0으로 준 경우와 다른 결과임 &amp;gt;&amp;gt;&amp;gt; rdd2.collect() [('b', 4), ('a', 2)] &amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize([&quot;a&quot;,&quot;b&quot;,&quot;b&quot;]).map(lambda v:(v,'c')) ///초기값을 t로 준 경우 &amp;gt;&amp;gt;&amp;gt; rdd2 = rdd.foldByKey('t', lambda v1,v2:v1+v2) &amp;gt;&amp;gt;&amp;gt; rdd2.collect() [('b', 'tctc'), ('a', 'tc')] 위의 두 연산를 보니, foldByKey는 초기값 처리를 아래와 같이 진행하는 것 같습니다. 예를 들어, 초기 rdd가 [(‘a’, 1), (‘b’, 1), (‘b’, 1)] 라 한다면, foldByKey는 키 ‘a’와 ‘b’에 대해 각각 초기값을 가지고 병합연산을 수행합니다. 이 때 먼저, 초기값을 가지고 병합 연산을 수행합니다. 키 ‘a’인 경우, 초기값 1라면, v1=1, v2=1(키 ‘a’에 대응되는 값)이 병합연산을 수행해 (‘a’,2)가 됩니다. 그 다음 reducebykey와 같은 연산을 수행하나, 키 ‘a’는 초기 rdd에 하나밖에 없기 때문에, v1=2, v2=None이 되어 최종 foldByKey연산 결과는 키 ‘a’에 대해서 2값을 가지게 됩니다. ‘b’키 같은 경우도 마찬가지입니다. 먼저 초기값을 가지고 연산을 수행합니다. v1=1(초기값), v2=1(b에 대응되는 값)가 병합연산을 수행해 v1=2가 됩니다. ‘b’키는 두개가 있으므로, 나머지 ‘b’키에 대해 v1=1, v2=1가 병합연산을 거쳐 v2=2가 됩니다. 그다음 두개의 ‘b’키에 대해 다시 병합연산이 수행되어 v1=2, v2=2가 되어 최종적으로 ‘b’키에 대해 4의 값이 생성됩니다. 4.3 combineByKey 키와 값인 RDD에 사용 가능 foldByKey와 reduceByKey와 유사함. 차이점은 병합연산 수행 결과 값의 타입이 바뀔 수 있음 def reduceByKey(func:(V,V)=&amp;gt;V):RDD[K,V] def foldByKey(zeroValue: V)(func:(V,V)=&amp;gt;V):RDD[K,V] def combineByKey[C](createCombiner:(V)=&amp;gt;C, mergeValue:(C,V)=&amp;gt;V, mergeCombiners:(C,C)=&amp;gt;C):RDD[K,C] combineByKey는 reduceByKey와 foldByKey와 다르게 타입이 C로 바뀌어 있음 위의 combineByKey를 보면 메서드가 총 createCombiner, mergeValue, mergeCombiners 세 개임을 알 수 있음 아래는 combineByKey에서 자주 등장하는 평균구하기 예시임 #record.py 에 따로 저장해야 함. #아니면, _pickle.PicklingError: Can't pickle &amp;lt;class '__main__.Record'&amp;gt;: attribute lookup Record on __main__ failed 에러 발생. class Record: def __init__(self, amount, number=1): self.amount = amount self.number = number def addAmt(self, amount): return Record(self.amount + amount, self.number + 1) def __add__(self, other): amount = self.amount + other.amount number = self.number + other.number return Record(amount, number) def __str__(self): return &quot;avg:&quot; + str(self.amount / self.number) def __repr__(self): return 'Record(%r, %r)' % (self.amount, self.number) #createCombiner, mergeValue, mergeCombiners 정의 def createCombiner(v): return Record(v) def mergeValue(c, v): return c.addAmt(v) def mergeCombiners(c1, c2): return c1 + c2 #combineByKey 실행 rdd = sc.parallelize([(&quot;Math&quot;, 100), (&quot;Eng&quot;, 80), (&quot;Math&quot;, 50), (&quot;Eng&quot;, 70), (&quot;Eng&quot;, 90)]) rdd2 = rdd.combineByKey(lambda v:createCombiner(v), lambda c,v:mergeValue(c,v), lambda c1,c2:mergeCombiners(c1,c2)) print('Math', rdd2.collectAsMap()['Math'], 'Eng', rdd2.collectAsMap()['Eng']) &amp;gt;&amp;gt;&amp;gt; Math avg:75.0 Eng avg:80.0 createCombiner() 값을 병합하기 위한 컴바이너. 컴바이너는 병합을 수행하고 컴바이너 타입으로 내부에 저장함. 위의 예로는 createCombiner()는 Record클래스 타입으로 저장됨. 즉, rdd의 각 키에 해당되는 값이 record클래스 타입으로 변환되어 저장됨. mergeValue() 키에 대한 컴바이너가 존재한다면, 컴바이너를 이용해 값을 병합함. My View) 위의 예 같은 경우, rdd요소 순서대로 combineByKey가 작동한다면 먼저 (“Math”, 100)에 대해 Combiner가 생성되어, math 키에 대한 Record(100)이 생성됩니다. 그 다음 (“Eng”, 70)에 대해 작동합니다. “Eng”키는 기존에 없던 키이기 때문에 새로 Combiner인 Record(70)이 생성됩니다. 그 다음 (“Math”, 50)인데, 기존 Math키에 대한 컴바이너가 존재하기 때문에, 기존 Math 키 컴바이너를 이용하여 병합합니다. 즉, Record(100).addAmt(50)이 발생합니다. 이렇게 Math키와 Eng키에 대해 모든 요소에 대해 병합을 실시하게 됩니다. mergeCombiners() createCombiner()와 mergeValue()는 파티션별로 수행됨. 그다음 모든 파티션에 생성된 combiner를 병합하는 과정을 mergeCombiners()를 통해 수행하는 것임. 이를 통해 최종결과가 발생함 4.4 aggregateByKey 키와 값의 RDD에서 사용 가능 초깃값을 설정할 수 있는 점을 제외하면 comebineByKey와 동일 def combineByKey[C](createCombiner:(V)=&amp;gt;C, mergeValue:(C,V)=&amp;gt;V, mergeCombiners:(C,C)=&amp;gt;C):RDD[(K,C)] def aggregateByKey[U](zeroValue: U)(seqOp:(U,V)=&amp;gt;U, combOp:(U,U)=&amp;gt;U):RDD[(K,U)] aggregateByKey에서 seqOp는 mergeValue역할을, comOp는 mergeCombiner역할을 함. combineByKey에서 createCombiner로 병합을 위한 초깃값을 구하지만 aggregateByKey는 함수를 이용해 초깃값을 설정하는 대신 바로 ‘값’으로 초기값을 설정함 rdd = sc.parallelize([(&quot;Math&quot;, 100), (&quot;Eng&quot;, 80), (&quot;Math&quot;, 50), (&quot;Eng&quot;, 70), (&quot;Eng&quot;, 90)]) rdd2 = rdd.aggregateByKey(Record(0,0), lambda c,v:mergeValue(c,v), lambda c1,c2:mergeCombiners(c1,c2)) print('Math :', rdd2.collectAsMap()['Math'], 'Eng :', rdd2.collectAsMap()['Eng']) &amp;gt;&amp;gt;&amp;gt;Math : avg:75.0 Eng : avg:80.0 5. pipe 및 파티션과 관련된 연산 5.1 pipe pipe를 이용하면 데이터를 처리하는 과정에서 외부 프로세스를 활용할 수 있음 세 개의 숫자로 구성된 문자열을 리눅스의 cut 유틸리티를 이용해 분리한 뒤첫번째와 세번재 숫자를 뽑아내는 예제 &amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize([&quot;1,2,3&quot;, &quot;4,5,6&quot;, &quot;7,8,9&quot;]) &amp;gt;&amp;gt;&amp;gt; rdd2 = rdd.pipe(&quot;cut -f 1,3 -d ,&quot;) &amp;gt;&amp;gt;&amp;gt; rdd2.collect() ['1,3', '4,6', '7,9'] 5.2. coalesce와 repartition 현재 RDD에 사용된 파티션 개수를 조정함 coalesce는 파티션 개수를 줄이기만 되고, repartition은 늘리는 것과 줄이는 것 둘 다 가능 coalesce가 따로 있는 이유는 처리 방식에 따른 성능 차이 때문임 repartition은 셔플을 기반으로 동작을 수행하는 데 반해, coalesce는 강제로 셔플을 수행하라는 옵션을 지정하지 않는 한 셔플을 사용하지 않음. 따라서 필터링 등으로 인해 데이터 개수가 줄어든 경우 coalesce을 사용하는 것이 좋음 &amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize(list(range(1,11)),10) &amp;gt;&amp;gt;&amp;gt; rdd2 = rdd.coalesce(5) &amp;gt;&amp;gt;&amp;gt; rdd3 = rdd2.repartition(10) &amp;gt;&amp;gt;&amp;gt; print(&quot;partition size : %d&quot; &amp;gt;&amp;gt;&amp;gt; print(&quot;partition size : %d&quot; %rdd2.getNumPartitions()) partition size : 5 &amp;gt;&amp;gt;&amp;gt; print(&quot;partition size : %d&quot; %rdd3.getNumPartitions()) partition size : 10 5.3. repartitionAndSortWithinPartitions 키와 값으로 구성된 RDD에서 사용 가능 RDD를 구성하는 모든 데이터를 특정 기준에 따라 여러 개의 파티션으로 분리하고 각 파티션 단위로 정렬을 수행한 뒤 새로운 RDD를 생성해 주는 메서드임 각 데이터가 어떤 파티션에 속할지 결정하기 위한 파티셔너(org.apache.spark.Partitioner)설정 키 값을 이용하여 어떤 파티션에 속할지 결정할 뿐만 아니라 키 값을 이용한 정렬도 수행함 파티션 재할당을 위해 셔플을 수행하는 단계에서 정렬도 함께 다루게 되어 파티션과 정렬을 각각 따로하는 것에 비해 더 높은 성능을 발휘할 수 있음 10개의 무작위 숫자를 위 메서드를 이용해 3개의 파티션으로 분리해 보는 예제 &amp;gt;&amp;gt;&amp;gt; data = [random.randrange(1,100) for i in range(0,10)] &amp;gt;&amp;gt;&amp;gt; rdd1 = sc.parallelize(data).map(lambda v:(v,&quot;-&quot;)) &amp;gt;&amp;gt;&amp;gt; rdd2 = rdd1.repartitionAndSortWithinPartitions(3, lambda x:x) &amp;gt;&amp;gt;&amp;gt; rdd2.foreachPartition(lambda values:print(list(values))) [(50, '-')] [(16, '-'), (52, '-'), (61, '-'), (67, '-')] [(6, '-'), (12, '-'), (48, '-'), (51, '-'), (87, '-')] pyspark에서 repartitionAndSortWithinPartitions에서 default 파티셔너는 hash 파티셔너로 되어있음 foreachPartition은 partition단위로 특정함수를 실행해주는 메서드임. 위의 예제에서는 파티션단위로 파티션에 속해있는 값을 프린트해주는 함수를 실행했음 5.4. partitionBy 키와 값으로 구성된 RDD에서 사용가능 파티션을 변경하고 싶을 때 사용가능 기본적으로, hashpartitioner와 rangepartitioner가 있음 org.apache.spark.partitioner 클래스를 상속해서 파티셔너를 커스터마이징도 가능 &amp;gt;&amp;gt;&amp;gt; rdd1 = sc.parallelize([(&quot;apple&quot;,1),(&quot;mouse&quot;,1),(&quot;monitor&quot;,1)],5) &amp;gt;&amp;gt;&amp;gt; rdd1.collect() [('apple', 1), ('mouse', 1), ('monitor', 1)] &amp;gt;&amp;gt;&amp;gt; rdd2 = rdd1.partitionBy(3) &amp;gt;&amp;gt;&amp;gt; print(&quot;rdd1: %d, rdd2: %d&quot; %(rdd1.getNumPartitions(), rdd2.getNumPartitions())) rdd1: 5, rdd2: 3 위의 예제에서 partitionby에 의해 파티션 갯수가 변경된 것을 확인할 수 있음 6. 필터와 정렬 연산 특정 조건을 만족하는 요소만 선택하거나, 각 요소를 정해진 기준에 따라 정렬함 6.1. filter RDD의 각각 요소에 조건에 따라 True/False로 가려내는 함수를 적용하여 True에 해당하는 요소만 걸러냄 &amp;gt;&amp;gt;&amp;gt; rdd1 = sc.parallelize(range(1,6)) &amp;gt;&amp;gt;&amp;gt; rdd2 = rdd1.filter(lambda i:i&amp;gt;2) &amp;gt;&amp;gt;&amp;gt; print(rdd2.collect()) [3, 4, 5] 6.2. sortByKey 키 값을 기준으로 요소를 정렬하는 연산임 따라서, 키와 값으로 구성된 RDD에 적용 가능함 &amp;gt;&amp;gt;&amp;gt; rdd1 = sc.parallelize([(&quot;q&quot;,1),(&quot;z&quot;,1),(&quot;a&quot;,1)]) &amp;gt;&amp;gt;&amp;gt; result = rdd1.sortByKey() &amp;gt;&amp;gt;&amp;gt; print(result.collect()) [('a', 1), ('q', 1), ('z', 1)] 6.3. keys, values 키와 값으로 구성된 RDD에 적용 가능함 keys는 RDD의 키 요소로 구성된 RDD를 생성하고, values는 RDD의 value요소로 구성된 RDD를 생성함 &amp;gt;&amp;gt;&amp;gt; rdd1 = sc.parallelize([(&quot;q&quot;,1),(&quot;z&quot;,1),(&quot;a&quot;,1)]) &amp;gt;&amp;gt;&amp;gt; rdd2 = rdd1.keys() &amp;gt;&amp;gt;&amp;gt; rdd3 = rdd1.values() &amp;gt;&amp;gt;&amp;gt; print(rdd2.collect(), rdd3.collect()) ['q', 'z', 'a'] [1, 1, 1] 6.4. sample 샘플을 추출하는 RDD메서드 sample(withReplacement, fraction, seed=None) withReplacement : True/False복원추출 결정 fraction 복원추출인 경우, RDD각 요소당 평균 추출횟수를 의미함 비복원추출인 경우, RDD각 요소당 샘플될 확률을 의미함 fraction이 sample사이즈를 결정하는 것은 아님. 아래 예제를 보면, sample사이즈는 random한 것을 알 수 있음. &amp;gt;&amp;gt;&amp;gt; rdd1 = sc.parallelize(range(100)) &amp;gt;&amp;gt;&amp;gt; rdd2 = rdd1.sample(True, 1.5, seed=1234) &amp;gt;&amp;gt;&amp;gt; rdd3 = rdd1.sample(False, 0.2, seed=1234) &amp;gt;&amp;gt;&amp;gt; &amp;gt;&amp;gt;&amp;gt; rdd2.collect() [1, 1, 2, 6, 6, 7, 7, 9, 10, 10, 11, 12, 12, 12, 13, 15, 17, 18, 19, 19, 19, 20, 21, 21, 23, 24, 25, 25, 26, 26, 26, 26, 26, 27, 28, 28, 28, 29, 30, 31, 32, 33, 33, 34, 35, 36, 36, 36, 37, 37, 38, 39, 39, 42, 42, 44, 44, 45, 45, 46, 48, 48, 48, 49, 49, 49, 50, 50, 50, 51, 53, 54, 57, 58, 59, 60, 63, 64, 65, 65, 66, 69, 71, 71, 71, 72, 72, 72, 73, 73, 75, 76, 77, 79, 80, 80, 81, 84, 84, 85, 85, 85, 86, 88, 88, 88, 89, 89, 89, 90, 90, 91, 91, 92, 92, 92, 94, 94, 95, 95, 95, 95, 96, 97, 97, 99] &amp;gt;&amp;gt;&amp;gt; rdd3.collect() [0, 5, 6, 7, 8, 11, 15, 35, 39, 41, 55, 56, 58, 61, 62, 71, 72, 78, 81, 89, 90, 93, 97, 99] 이상으로 본 포스팅을 마치겠습니다. 다음 포스팅은 &amp;lt;RDD, Resilient Distributed Dataset에 대하여[3] - RDD액션, RDD데이터 불러오기와 저장하기&amp;gt; 에 대해 진행하도록 하겠습니다.</summary></entry><entry><title type="html">dataframe, numpy 등 array에서 double-colon(::) slicing</title><link href="http://localhost:4000/code%20snippet/2020/12/03/double-colon-slicing/" rel="alternate" type="text/html" title="dataframe, numpy 등 array에서 double-colon(::) slicing" /><published>2020-12-03T00:00:00+09:00</published><updated>2020-12-03T00:00:00+09:00</updated><id>http://localhost:4000/code%20snippet/2020/12/03/double-colon-slicing</id><content type="html" xml:base="http://localhost:4000/code%20snippet/2020/12/03/double-colon-slicing/">&lt;hr /&gt;

&lt;p&gt;pandas, numpy 등 자주 헷갈리는 코드 사용을 모아두었습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;dfc&quot;&gt;df[::c]&lt;/h1&gt;

&lt;p&gt;시작부터 c 간격마다 있는 row를 슬라이싱해줍니다. 자세히 설명하면, 1번째, (1+c)번째, (1+2c)번째, …, (1+nc)번째 row가 선택됩니다. 아래는 예제입니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sampledf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'A'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'B'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;sampledf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[::&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div&gt;
&lt;style scoped=&quot;&quot;&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;A&lt;/th&gt;
      &lt;th&gt;B&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0.312234&lt;/td&gt;
      &lt;td&gt;0.788584&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;-0.123720&lt;/td&gt;
      &lt;td&gt;0.445176&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0.411344&lt;/td&gt;
      &lt;td&gt;0.617469&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;-0.434367&lt;/td&gt;
      &lt;td&gt;0.674210&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;-0.563221&lt;/td&gt;
      &lt;td&gt;0.009331&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;190&lt;/th&gt;
      &lt;td&gt;1.797756&lt;/td&gt;
      &lt;td&gt;0.963394&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;192&lt;/th&gt;
      &lt;td&gt;-0.679177&lt;/td&gt;
      &lt;td&gt;0.033222&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;194&lt;/th&gt;
      &lt;td&gt;0.975527&lt;/td&gt;
      &lt;td&gt;0.041236&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;196&lt;/th&gt;
      &lt;td&gt;-1.354463&lt;/td&gt;
      &lt;td&gt;0.450887&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;198&lt;/th&gt;
      &lt;td&gt;-2.341788&lt;/td&gt;
      &lt;td&gt;0.009804&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;100 rows × 2 columns&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;위에 sampledf[::2]를 보시면 첫번째(index=0), 세번째(index=2), …., 199번째(index=198)이 선택되는 것을 확인하실 수 있습니다. 2의 간격 크기로 행이 선택되는 것입니다.&lt;/p&gt;

&lt;h1&gt;df[::-1]&lt;/h1&gt;

&lt;p&gt;df[::-1] 인 경우는 열의 배치를 뒤집어줍니다. 아래는 예시입니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;sampledf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[::&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div&gt;
&lt;style scoped=&quot;&quot;&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;A&lt;/th&gt;
      &lt;th&gt;B&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;199&lt;/th&gt;
      &lt;td&gt;2.600890&lt;/td&gt;
      &lt;td&gt;0.775489&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;198&lt;/th&gt;
      &lt;td&gt;-2.341788&lt;/td&gt;
      &lt;td&gt;0.009804&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;197&lt;/th&gt;
      &lt;td&gt;-0.365103&lt;/td&gt;
      &lt;td&gt;0.413758&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;196&lt;/th&gt;
      &lt;td&gt;-1.354463&lt;/td&gt;
      &lt;td&gt;0.450887&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;195&lt;/th&gt;
      &lt;td&gt;0.685687&lt;/td&gt;
      &lt;td&gt;0.933069&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0.411344&lt;/td&gt;
      &lt;td&gt;0.617469&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1.703587&lt;/td&gt;
      &lt;td&gt;0.718288&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;-0.123720&lt;/td&gt;
      &lt;td&gt;0.445176&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;0.208545&lt;/td&gt;
      &lt;td&gt;0.459722&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0.312234&lt;/td&gt;
      &lt;td&gt;0.788584&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;200 rows × 2 columns&lt;/p&gt;
&lt;/div&gt;

&lt;h1&gt;df[::-c]&lt;/h1&gt;

&lt;p&gt;마찬가지로, df[::-c] 이면 뒤에 row부터 2간격마다 row가 선택됩니다. 아래는 예시입니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;sampledf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[::&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div&gt;
&lt;style scoped=&quot;&quot;&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;A&lt;/th&gt;
      &lt;th&gt;B&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;199&lt;/th&gt;
      &lt;td&gt;2.600890&lt;/td&gt;
      &lt;td&gt;0.775489&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;197&lt;/th&gt;
      &lt;td&gt;-0.365103&lt;/td&gt;
      &lt;td&gt;0.413758&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;195&lt;/th&gt;
      &lt;td&gt;0.685687&lt;/td&gt;
      &lt;td&gt;0.933069&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;193&lt;/th&gt;
      &lt;td&gt;0.267967&lt;/td&gt;
      &lt;td&gt;0.020342&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;191&lt;/th&gt;
      &lt;td&gt;-0.918194&lt;/td&gt;
      &lt;td&gt;0.917082&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;0.924938&lt;/td&gt;
      &lt;td&gt;0.837344&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;0.890616&lt;/td&gt;
      &lt;td&gt;0.096270&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;-0.603043&lt;/td&gt;
      &lt;td&gt;0.697143&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1.703587&lt;/td&gt;
      &lt;td&gt;0.718288&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;0.208545&lt;/td&gt;
      &lt;td&gt;0.459722&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;100 rows × 2 columns&lt;/p&gt;
&lt;/div&gt;</content><author><name>Seonhwa Lee</name></author><category term="Code Snippet" /><category term="pandas" /><summary type="html">pandas, numpy 등 자주 헷갈리는 코드 사용을 모아두었습니다. df[::c] 시작부터 c 간격마다 있는 row를 슬라이싱해줍니다. 자세히 설명하면, 1번째, (1+c)번째, (1+2c)번째, …, (1+nc)번째 row가 선택됩니다. 아래는 예제입니다. import pandas as pd import numpy as np a = np.random.normal(size=200) b = np.random.uniform(size=200) sampledf = pd.DataFrame({'A':a,'B':b}) sampledf[::2] A B 0 0.312234 0.788584 2 -0.123720 0.445176 4 0.411344 0.617469 6 -0.434367 0.674210 8 -0.563221 0.009331 ... ... ... 190 1.797756 0.963394 192 -0.679177 0.033222 194 0.975527 0.041236 196 -1.354463 0.450887 198 -2.341788 0.009804 100 rows × 2 columns 위에 sampledf[::2]를 보시면 첫번째(index=0), 세번째(index=2), …., 199번째(index=198)이 선택되는 것을 확인하실 수 있습니다. 2의 간격 크기로 행이 선택되는 것입니다. df[::-1] df[::-1] 인 경우는 열의 배치를 뒤집어줍니다. 아래는 예시입니다. sampledf[::-1] A B 199 2.600890 0.775489 198 -2.341788 0.009804 197 -0.365103 0.413758 196 -1.354463 0.450887 195 0.685687 0.933069 ... ... ... 4 0.411344 0.617469 3 1.703587 0.718288 2 -0.123720 0.445176 1 0.208545 0.459722 0 0.312234 0.788584 200 rows × 2 columns df[::-c] 마찬가지로, df[::-c] 이면 뒤에 row부터 2간격마다 row가 선택됩니다. 아래는 예시입니다. sampledf[::-2] A B 199 2.600890 0.775489 197 -0.365103 0.413758 195 0.685687 0.933069 193 0.267967 0.020342 191 -0.918194 0.917082 ... ... ... 9 0.924938 0.837344 7 0.890616 0.096270 5 -0.603043 0.697143 3 1.703587 0.718288 1 0.208545 0.459722 100 rows × 2 columns</summary></entry><entry><title type="html">pandas.DataFrame.any(), numpy.any()</title><link href="http://localhost:4000/code%20snippet/2020/12/01/any-all-usage/" rel="alternate" type="text/html" title="pandas.DataFrame.any(), numpy.any()" /><published>2020-12-01T00:00:00+09:00</published><updated>2020-12-01T00:00:00+09:00</updated><id>http://localhost:4000/code%20snippet/2020/12/01/any-all-usage</id><content type="html" xml:base="http://localhost:4000/code%20snippet/2020/12/01/any-all-usage/">&lt;hr /&gt;

&lt;p&gt;평소에 헷갈리는 any(), all()에 대해 정리하였습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1&gt;df.isna()&lt;/h1&gt;

&lt;div class=&quot;prompt input_prompt&quot;&gt;
In&amp;nbsp;[1]:
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;prompt input_prompt&quot;&gt;
In&amp;nbsp;[2]:
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'./data/top1_1880109251922.csv'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index_col&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'date'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'date'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Date'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'date'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Date'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;asfreq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'D'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;df.isna()는 데이터프레임에서 NaN 요소에 해당되는 부분을 True로 리턴해준다.&lt;/p&gt;

&lt;div class=&quot;prompt input_prompt&quot;&gt;
In&amp;nbsp;[3]:
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;isna&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div&gt;
&lt;style scoped=&quot;&quot;&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;date&lt;/th&gt;
      &lt;th&gt;store&lt;/th&gt;
      &lt;th&gt;product_c&lt;/th&gt;
      &lt;th&gt;sales&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Date&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;2018-02-01&lt;/th&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2018-02-02&lt;/th&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2018-02-03&lt;/th&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2018-02-04&lt;/th&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2018-02-05&lt;/th&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2019-07-27&lt;/th&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2019-07-28&lt;/th&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2019-07-29&lt;/th&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2019-07-30&lt;/th&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2019-07-31&lt;/th&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;546 rows × 4 columns&lt;/p&gt;
&lt;/div&gt;

&lt;h1&gt;df.any()&lt;/h1&gt;

&lt;p&gt;여기서, dataframe.any(axis=0)인 경우엔 각 column의 row를 다 훑어서, row요소들 중 적어도 하나의 row애 True가 있으면, True를 반환합니다.&lt;/p&gt;

&lt;div class=&quot;prompt input_prompt&quot;&gt;
In&amp;nbsp;[4]:
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;isna&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;any&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext output_data_text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;date         True
store        True
product_c    True
sales        True
dtype: bool
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;반면에, dataframe.any(axis=1)인 경우엔 각 index별로 column요소를 다 훑어서 적어도 하나의 column에 True가 있으면 True를 반환합니다.
아래 코드를 보면, 해당 index에 data, store, product_c, sales가 모두 True이면 해당 index row는 True를 반환합니다.&lt;/p&gt;

&lt;div class=&quot;prompt input_prompt&quot;&gt;
In&amp;nbsp;[5]:
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;isna&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;any&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext output_data_text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Date
2018-02-01    False
2018-02-02    False
2018-02-03    False
2018-02-04    False
2018-02-05    False
              ...  
2019-07-27     True
2019-07-28     True
2019-07-29    False
2019-07-30    False
2019-07-31    False
Freq: D, Length: 546, dtype: bool
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1&gt;np.any()&lt;/h1&gt;

&lt;p&gt;np.any() 는 dataframe.any()와 유사합니다. 주어진 축(axis) 정보에 따라 해당 요소에서 True가 하나 이상이라도 있으면 True를 반환합니다. 아래는 np.any()의 예제입니다.&lt;/p&gt;

&lt;div class=&quot;prompt input_prompt&quot;&gt;
In&amp;nbsp;[6]:
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;먼저, 예제 array를 생성합니다. True, False로 구성된 random한 array를 만들었습니다.&lt;/p&gt;

&lt;div class=&quot;prompt input_prompt&quot;&gt;
In&amp;nbsp;[7]:
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;samplearr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;choice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;samplearr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;47&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;choice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;samplearr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;47&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;choice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;samplearr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;47&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext output_stream highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[ True  True  True  True  True  True False  True  True  True False False
 False  True False  True False False False False  True False  True  True
 False  True False False False  True False  True False  True  True False
 False  True False False  True False False  True  True  True False]
[False False  True False  True False  True  True False  True  True False
  True False  True False False False False False  True  True False False
 False False  True False False  True False False False  True False  True
  True  True False False  True  True  True  True False  True  True]
[False False  True False False False False  True False False  True  True
  True False False  True  True  True  True False False False False  True
 False  True  True  True  True  True  True  True False False False  True
 False False False False False False  True  True  True  True  True]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;np.any(a,b,c)는 에러를 발생합니다. 반드시, 하나의 array나 아니면 array와 유사한 list형식으로 묶어서 넣어줘야합니다.&lt;/p&gt;

&lt;div class=&quot;prompt input_prompt&quot;&gt;
In&amp;nbsp;[8]:
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;any&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext output_traceback_line highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;---------------------------------------------------------------------------
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext output_traceback_line highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;TypeError                                 Traceback (most recent call last)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext output_traceback_line highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;ipython-input-8-7a7facd3228c&amp;gt; in &amp;lt;module&amp;gt;
----&amp;gt; 1 np.any(a,b,c)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext output_traceback_line highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;__array_function__ internals&amp;gt; in any(*args, **kwargs)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext output_traceback_line highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py in any(a, axis, out, keepdims)
   2328 
   2329     &quot;&quot;&quot;
-&amp;gt; 2330     return _wrapreduction(a, np.logical_or, 'any', axis, None, out, keepdims=keepdims)
   2331 
   2332 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext output_traceback_line highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py in _wrapreduction(obj, ufunc, method, axis, dtype, out, **kwargs)
     85                 return reduction(axis=axis, out=out, **passkwargs)
     86 
---&amp;gt; 87     return ufunc.reduce(obj, axis, dtype, out, **passkwargs)
     88 
     89 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext output_traceback_line highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;TypeError: only integer scalar arrays can be converted to a scalar index
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2&gt;axis=0 vs. axis=1&lt;/h2&gt;

&lt;p&gt;그전에 axis=0과 1에 따라 차이를 살펴봅시다. axis=0인 경우엔 각 column의 모든 row를 훑고, axis=1인 경우엔 각 row의 모든 column을 훑습니다. 아래는 관련 그림입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2020-12-01-any-all-usage_files/axis.jpg&quot; alt=&quot;jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;axis=0인 경우, 각각의 column요소에서 모든 row를 훑어서 하나 이상이 True요소라면 True를 반환합니다. 결과는 [a,b,c]의 column의 갯수만큼 출력됩니다.&lt;/p&gt;

&lt;div class=&quot;prompt input_prompt&quot;&gt;
In&amp;nbsp;[9]:
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;any&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext output_data_text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True, False,  True,  True,  True,  True, False,  True,  True,
        True,  True,  True,  True,  True, False,  True,  True,  True,
        True,  True, False, False,  True,  True,  True,  True,  True,
        True,  True])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;axis=1인 경우, 각각의 row요소에서 모든 column을 훑어서 하나 이상이 True요소라면 True를 반환합니다. 결과는 [a,b,c]의 row 갯수만큼 출력됩니다.&lt;/p&gt;

&lt;div class=&quot;prompt input_prompt&quot;&gt;
In&amp;nbsp;[10]:
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;any&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext output_data_text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;array([ True,  True,  True])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1&gt;np.all()&lt;/h1&gt;

&lt;p&gt;np.all()은 np.any()와 반대로, 검사할 축에 모든 요소가 True여야지만 True를 반환합니다. 아래는 예제입니다.&lt;/p&gt;

&lt;div class=&quot;prompt input_prompt&quot;&gt;
In&amp;nbsp;[11]:
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext output_data_text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;array([False, False,  True, False, False, False, False,  True, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False,  True, False, False, False, False, False, False,
       False, False, False, False, False, False, False,  True, False,
        True, False])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;prompt input_prompt&quot;&gt;
In&amp;nbsp;[12]:
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext output_data_text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;array([False, False, False])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Seonhwa Lee</name></author><category term="Code Snippet" /><category term="pandas" /><category term="numpy" /><summary type="html">평소에 헷갈리는 any(), all()에 대해 정리하였습니다. df.isna() In&amp;nbsp;[1]: import pandas as pd In&amp;nbsp;[2]: df = pd.read_csv('./data/top1_1880109251922.csv', index_col=[0]) df['date'] = pd.to_datetime(df['date']) df['Date'] = pd.to_datetime(df['date']) df = df.set_index('Date') df = df.asfreq('D') df.isna()는 데이터프레임에서 NaN 요소에 해당되는 부분을 True로 리턴해준다. In&amp;nbsp;[3]: df.isna() date store product_c sales Date 2018-02-01 False False False False 2018-02-02 False False False False 2018-02-03 False False False False 2018-02-04 False False False False 2018-02-05 False False False False ... ... ... ... ... 2019-07-27 True True True True 2019-07-28 True True True True 2019-07-29 False False False False 2019-07-30 False False False False 2019-07-31 False False False False 546 rows × 4 columns df.any() 여기서, dataframe.any(axis=0)인 경우엔 각 column의 row를 다 훑어서, row요소들 중 적어도 하나의 row애 True가 있으면, True를 반환합니다. In&amp;nbsp;[4]: df.isna().any(axis=0) date True store True product_c True sales True dtype: bool 반면에, dataframe.any(axis=1)인 경우엔 각 index별로 column요소를 다 훑어서 적어도 하나의 column에 True가 있으면 True를 반환합니다. 아래 코드를 보면, 해당 index에 data, store, product_c, sales가 모두 True이면 해당 index row는 True를 반환합니다. In&amp;nbsp;[5]: df.isna().any(axis=1) Date 2018-02-01 False 2018-02-02 False 2018-02-03 False 2018-02-04 False 2018-02-05 False ... 2019-07-27 True 2019-07-28 True 2019-07-29 False 2019-07-30 False 2019-07-31 False Freq: D, Length: 546, dtype: bool np.any() np.any() 는 dataframe.any()와 유사합니다. 주어진 축(axis) 정보에 따라 해당 요소에서 True가 하나 이상이라도 있으면 True를 반환합니다. 아래는 np.any()의 예제입니다. In&amp;nbsp;[6]: import numpy as np 먼저, 예제 array를 생성합니다. True, False로 구성된 random한 array를 만들었습니다. In&amp;nbsp;[7]: samplearr = [True, False] a = np.random.choice(samplearr, size=(47)) b = np.random.choice(samplearr, size=(47)) c = np.random.choice(samplearr, size=(47)) print(a) print(b) print(c) [ True True True True True True False True True True False False False True False True False False False False True False True True False True False False False True False True False True True False False True False False True False False True True True False] [False False True False True False True True False True True False True False True False False False False False True True False False False False True False False True False False False True False True True True False False True True True True False True True] [False False True False False False False True False False True True True False False True True True True False False False False True False True True True True True True True False False False True False False False False False False True True True True True] np.any(a,b,c)는 에러를 발생합니다. 반드시, 하나의 array나 아니면 array와 유사한 list형식으로 묶어서 넣어줘야합니다. In&amp;nbsp;[8]: np.any(a,b,c) --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &amp;lt;ipython-input-8-7a7facd3228c&amp;gt; in &amp;lt;module&amp;gt; ----&amp;gt; 1 np.any(a,b,c) &amp;lt;__array_function__ internals&amp;gt; in any(*args, **kwargs) /opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py in any(a, axis, out, keepdims) 2328 2329 &quot;&quot;&quot; -&amp;gt; 2330 return _wrapreduction(a, np.logical_or, 'any', axis, None, out, keepdims=keepdims) 2331 2332 /opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py in _wrapreduction(obj, ufunc, method, axis, dtype, out, **kwargs) 85 return reduction(axis=axis, out=out, **passkwargs) 86 ---&amp;gt; 87 return ufunc.reduce(obj, axis, dtype, out, **passkwargs) 88 89 TypeError: only integer scalar arrays can be converted to a scalar index axis=0 vs. axis=1 그전에 axis=0과 1에 따라 차이를 살펴봅시다. axis=0인 경우엔 각 column의 모든 row를 훑고, axis=1인 경우엔 각 row의 모든 column을 훑습니다. 아래는 관련 그림입니다. axis=0인 경우, 각각의 column요소에서 모든 row를 훑어서 하나 이상이 True요소라면 True를 반환합니다. 결과는 [a,b,c]의 column의 갯수만큼 출력됩니다. In&amp;nbsp;[9]: np.any([a,b,c], axis=0) array([ True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, False, True, True, True, True, True, True, True, False, True, True, True, True, True, False, False, True, True, True, True, True, True, True]) axis=1인 경우, 각각의 row요소에서 모든 column을 훑어서 하나 이상이 True요소라면 True를 반환합니다. 결과는 [a,b,c]의 row 갯수만큼 출력됩니다. In&amp;nbsp;[10]: np.any([a,b,c], axis=1) array([ True, True, True]) np.all() np.all()은 np.any()와 반대로, 검사할 축에 모든 요소가 True여야지만 True를 반환합니다. 아래는 예제입니다. In&amp;nbsp;[11]: np.all([a,b,c], axis=0) array([False, False, True, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, True, False]) In&amp;nbsp;[12]: np.all([a,b,c], axis=1) array([False, False, False])</summary></entry><entry><title type="html">General Approach to Time Series Analysis - Time Series Data, Stationarity 등에 대하여</title><link href="http://localhost:4000/time%20series%20analysis/2020/11/23/time-series-intro(1)/" rel="alternate" type="text/html" title="General Approach to Time Series Analysis - Time Series Data, Stationarity 등에 대하여" /><published>2020-11-23T00:00:00+09:00</published><updated>2020-11-23T00:00:00+09:00</updated><id>http://localhost:4000/time%20series%20analysis/2020/11/23/time-series-intro(1)</id><content type="html" xml:base="http://localhost:4000/time%20series%20analysis/2020/11/23/time-series-intro(1)/">&lt;p&gt;이번 포스팅을 시작으로, 시계열 분석에 대해서 다루도록 하겠습니다. 메인 교재는 Brockwell와 Richard A. Davis의 &amp;lt; Introduction to Time Series and Forecasting &amp;gt; 와 패스트캠퍼스의 &amp;lt;파이썬을 활용한 시계열 분석 A-Z&amp;gt; 를 듣고 정리하였습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2&gt;1.1. What is Time Series?&lt;/h2&gt;

&lt;p&gt;시계열이란, 일정 시간 간격으로 배치된 데이터들의 수열입니다. 그 중에서 discrete한 시계열이란 관측이 발생한 시각 t의 집합 $T_0$ 가 discrete한 경우이며, 관측치가 시간 구간 안에서 연속적으로 발생한다면 continuous한 시계열입니다.&lt;/p&gt;

&lt;p&gt;시계열 시퀀스는 일반적으로 자기 상관성이 있는 수열입니다. 즉, 과거의 데이터가 현재를 넘어서 미래까지 영향을 미치는 것을 뜻합니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Cov(X_i, X_j) \neq 0&lt;/script&gt;

&lt;p&gt;따라서, 시계열 데이터로 모델링을 하기 위해선 먼저 데이터를 최대한 분해해서 살펴봐야 합니다. 확률 모델링을 하기 위해선 i.i.d 여야 하기 때문입니다. 일반적으로 시계열 데이터는 trend, seasonality, noise 항으로 구성되어 있습니다. 여기서 시계열 데이터가 자기 상관성을 가지게 되는 요인은 trend와 seasonality 요소 때문이고, noise는 i.i.d한 독립변수로 구성된 에러항입니다.&lt;/p&gt;

&lt;h2&gt;1.2. Objectives of Time Series Analysis&lt;/h2&gt;
&lt;p&gt;시계열 분석의 목적은 주로 시계열 데이터를 보고 앞으로 일어날 일들을 예측하는 것입니다. 그러기 위에선 기존에 있는 시계열 데이터를 가지고 추론을 해야합니다. 따라서, 이러한 추론을 하기 위해선 가정에 맞는 적절한 확률 모델을 선택하여 모델링을 진행해야 합니다.&lt;/p&gt;

&lt;p&gt;그러나, 시계열 데이터는 자기 상관성이 존재하는 데이터입니다. 따라서 확률적 모델링을 통해 이 시계열 데이터를 서로 독립인 데이터로 변환해야 하는데 이 과정이 seasonal adjustment 또는 trend and seasonal decomposition입니다. 그 밖에 log transformation, differencing 같은 과정도 존재합니다.&lt;/p&gt;

&lt;p&gt;어쨌든, 시계열 분석의 궁극적인 목표는 독립적인 변수로 최대한 변환한 뒤, 이를 기반으로 확률적 통계 모델링을 해서, inference를 하는 것입니다. Inference 결과는 다시 우리가 얻고자 하는 예측값으로 바꾸기 위한 reverting 과정을 거쳐야 합니다. 왜냐하면, seasonal adjustment나 Decomposition을 통해 상관성을 제거했기 때문에 원하는 예측값을 얻기 위해선 다시 원래대로 이 과정을 뒤집어서 돌아가야 하는 것입니다.&lt;/p&gt;

&lt;h2&gt;1.3. Some Simple Time Series Models&lt;/h2&gt;
&lt;p&gt;위에서 말씀 드린 것과 같이 시계열 데이터를 보고 적절한 확률 모델을 선택하는 것은 매우 중요합니다. 따라서, 몇가지 간단한 time series model을 소개하겠습니다.&lt;/p&gt;

&lt;h3&gt;1.3.1. Definition of a time series model&lt;/h3&gt;
&lt;p&gt;관측된 ${x_t}$ 에 대한 time-series 모델이란, 랜덤 변수 ${X_t}$ 시퀀스들의 joint distribution 을 의미합니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;A time series model for the observed data ${x_t}$ is a specification of the joint distributions(or possibly only the means and covariances) of a sequence of random variables ${X_t}$ of which ${x_t}$ is postulated to be a realization.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;즉, 랜덤 변수들의 시퀀스 ${X_1, X_2, \dots }$ 로 구성된 time-series 확률 모델은 랜덤 벡터 $(X_1, \dots, x_n)’ ,\,\, n=1,2,\dots,$ 의 결합 분포입니다. 아래 그림은 랜던 변수들의 시퀀스 ${S_t, t=1, \dots, 200}$ 로 나올 수 있는 가능성 중 한가지가 ‘실현’ 된 것입니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/izJrvZl.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 1. Time-series 예시&lt;/figcaption&gt;&lt;/p&gt;

&lt;h3&gt;1.3.2. Some Simple Time Series Model&lt;/h3&gt;

&lt;ol&gt;&lt;li&gt;iid Noise&lt;br /&gt;
가장 기본적인 time series 모델은 noise항으로만 이뤄진 경우입니다.(거의 현실세계에선 없다고 생각하시면 됩니다.)&lt;/li&gt;
&lt;li&gt;Binary Process&lt;br /&gt;
i.i.d Noise의 종류로, binary 분포를 따르는 noise인 경우입니다. 랜덤 변수들의 시퀀스 $\{X_t,\,\,t=1,2,\dots,\}$ 가 $P[X_t = 1]=p$ , $P[X_t = -1] = 1-p$ 를 따릅니다.&lt;/li&gt;
&lt;li&gt;Models with only Trend&lt;br /&gt;
trend요소와 noise항만 있는 경우입니다. 여기서 trend요소란 패턴이 선형관계를 가지고 있을 때입니다. 자세히 말하면, 시계열이 시간에 따라 증가, 감소, 또는 일정 수준을 유지하는 경우입니다.

$$X_t = m_t + Y_t$$

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/AjlrE80.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 2. Time series with only trend component&lt;/figcaption&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;Models with only Seaonality&lt;br /&gt;
seasonal요소와 noise항만 있는 경우입니다. 여기서 seasonal요소란 일정한 빈도로 주기적으로 반복되는 패턴을 말합니다. 반면에, 일정하지 않은 빈도로 발생하는 패턴은 Cycle이라 합니다.(여기서는 seasonal 기준으로 설명하겠습니다.)

$$X_t = S_t + Y_t$$&lt;/li&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/6NZcDiO.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 3. Times series with only seasonality(period=12month)&lt;/figcaption&gt;&lt;/p&gt;
&lt;/ol&gt;

&lt;h3&gt;1.3.3 A General Approach to Time Series Modeling&lt;/h3&gt;
&lt;p&gt;시계열 분석에 대해 깊게 들어가기 전에, 시계열 데이터 모델링하는 방법에 대해 대략적으로 알아봅시다.&lt;/p&gt;

&lt;p&gt;1) 그래프로 그린 후, 그래프 상에서 아래와 같은 요소가 있는지 체크한다.(Plot the series and examine the main features of the graph)&lt;br /&gt;&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;trend&lt;/li&gt;&lt;li&gt;a seasonal component&lt;/li&gt;&lt;li&gt;any apparent sharp changes in behavior&lt;/li&gt;&lt;li&gt;any outlying observations&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;2) 정상상태의 잔차를 얻기 위해, trend와 seasonality 요소를 제거한다. (Remove the trend and seasonal components to get stationary residuals)&lt;br /&gt;
     trend와 seasonality 요소를 제거하기 전에, 전처리를 해야하는 경우가 있습니다. 예를 들어, 아래와 같이 지수적으로 증가하는 경우에, 로그를 취해서 variance가 일정하도록 만든 후 모델링을 하면 정확도를 높일 수 있습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/V85l07h.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 1. 로그 취하기 전&lt;/figcaption&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/e0GKRKU.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 2. 로그 취한 후&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;이외에도 여러 방법이 있습니다. 추후에 설명하도록 하겠습니다. 어쨌든, 이 모든 방법들의 핵심은 &lt;b&gt;정상상태의 잔차&lt;/b&gt;를 만드는 것입니다.&lt;/p&gt;

&lt;p&gt;3) auto-correlation 함수, 여러 다양한 통계량을 이용하여 잔차를 핏팅할 모델을 선택한다. (Choose a model to fit the residuals, making use of various sample statistics including the sample autocorrelation function)&lt;/p&gt;

&lt;p&gt;4) 핏팅된 모델로 예측한다.&lt;br /&gt; 
     여기서 잔차를 예측하는 것이고, 예측된 잔차를 원래 예측해야 할 값으로 변환한다.&lt;/p&gt;

&lt;h2&gt;1.4. Stationary Models and the Autocorrelation Function&lt;/h2&gt;

&lt;p&gt;시계열 데이터가 정상상태(stationarity)를 가지기 위해서, 시계열이 확률적인 특징이 시간이 지남에 따라 변하지 않는다는 가정을 충족시켜야 합니다. 그러나 시계열 데이터는 trend와 seasonality요소로 인해, 평균과 분산이 변할 수 있습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;a time series ${{X_t, t=0, \pm1, …}}$ is said to be stationary if it has statistical properties similar to those of the “time-shifted” series ${{X_{t+h}, t=0, \pm1, …}}$ for each integer h.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Trends can result in a varying mean over time, whereas seasonality can result in a changing variance over time, both which define a time series as being non-stationary. Stationary datasets are those that have a stable mean and variance, and are in turn much easier to model.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;시계열에 대한 평균과 공분산은 아래와 같이 정의됩니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/65biJ1q.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 3. 시계열의 평균과 공분산&lt;/figcaption&gt;&lt;/p&gt;

&lt;h4&gt;Strict Stationarity vs. Weak Stationarity&lt;/h4&gt;
&lt;p&gt;엄격한 정상상태가 되려면,  $(X_1,\dots , X_n)$ 의 결합분포와 $(X_{1+h}, \dots, X_{n+h})$ 의 결합분포가 시간간격 h에 상관없이 동일해야 합니다. 그러나 이를 이론적으로 증명하기 어렵기 때문에, 약한 정상상태(weak stationarity)만을 만족하면 정상상태에 있다고 생각하고 시계열 문제를 풉니다. 약한 정상상태는 아래 조건을 만족합니다. 즉, 결합분포가 동일해야 한다는 강력한 조건이 사라졌기 때문에 약한 정상상태라고 하는 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;(1) \,\, E(X_t) = u&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;(2) \,\, Cov(X_{t+h}, X_{t}) = \gamma_h, for\; all\; h&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;(3)\,\, Var(X_t) = Var(X_{t+h})&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;(2)식은 공분산은 t에 독립임을 의미합니다. 정상상태 시계열의 공분산은 아래와 같이 하나의 변수 h에 대해 나타낼 수 있습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\gamma_X(h) = \gamma_X(h,0) = \gamma_X(t+h, t)&lt;/script&gt;

&lt;p&gt;이때 함수 $\gamma_X(\cdot)$ 을 lag h에 대한 auto-covariance 함수(ACVF)라 합니다. auto-correlation 함수(ACF)는 ACVF를 이용해 아래와 같이 정의됩니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\rho_X(h)=\frac{\gamma_X(h)}{\gamma_X(0)}=Cor(X_{t+h}, X_t)&lt;/script&gt;

&lt;h4&gt;White Noise&lt;/h4&gt;
&lt;p&gt;시계열 ${{X_t}}$ 가 독립적인 랜덤 변수의 시퀀스이고, 평균이 0이고, 분산이 $\sigma^2$ 이면, White Noise라 합니다. 아래는 White Noise의 조건입니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;E(X_t)=0&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;V(X_t)=V(X_{t+h})=\sigma^2&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\gamma_X(t+h, t)=0\;(h\neq0)&lt;/script&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;1.4.1 The Sample Autocorrelation Function&lt;/h3&gt;

&lt;p&gt;관측 데이터 가지고 자기 상관의 정도를 볼때, sample auto-correlation 함수(sample ACF)를 사용합니다. Sample ACF는 ACF의 추정으로, 계산은 아래와 같습니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/jjzBk3z.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 4. Sample ACF&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;White Noise인 경우, 시계열 그래프와 ACF 그래프는 아래와 같습니다. lag가 1이상인 경우, 거의 ACF값이 0에 가까운 것을 볼 수 있고, 95% 신뢰구간 안에 들어와 있습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/RaoTZJj.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 5. White Noise ACF&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;아래는 그림 1. 그래프에 플롯된 데이터를 가지고 그린 ACF입니다. 보시면, ACF가 lag가 커짐에 따라 서서히 감소하는 형태를 띄는데 이는 trend가 있는 데이터에서 나타납니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/N6vk5oN.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 6. Sequence with trend ACF&lt;/figcaption&gt;&lt;/p&gt;

&lt;h2&gt;1.5. Estimation and Elimination of Trend and Seasonal Components&lt;/h2&gt;

&lt;p&gt;trend와 seasonality가 존재하는 시계열의 모델링인 경우, 아래와 같이 additive 형태를 띌 수 있습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;X_t = m_t + s_t + Y_t&lt;/script&gt;

&lt;p&gt;시계열 모델링의 최종 목표는 잔차항 $Y_t$ 가 정상상태에 놓이게 하는 것입니다. 따라서 잔차항을 분석하기 위해서 trend 요소 $m_t$ 와 seasonal 요소 $s_t$ 를 제거해야 합니다.&lt;/p&gt;

&lt;h3&gt;1.5.1. Estimation and Elimination of Trend in the Absence of Seasonality&lt;/h3&gt;
&lt;p&gt;seasonal 요소가 없고, trend요소만 있는 모델링은 아래와 같이 진행할 수 있습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;X_t = m_t + Y_t, \quad t=1, \dots ,n, \; where \; EY_t = 0&lt;/script&gt;

&lt;h4&gt;method1. Trend Estimation&lt;/h4&gt;

&lt;p&gt;trend 요소를 추정하는 방법은 Moving Average와 Smoothing을 이용하는 방법 2가지가 있습니다.&lt;/p&gt;

&lt;h5&gt;a) Smoothing with a finite moving average filter&lt;/h5&gt;

&lt;p&gt;과거 n개의 시점을 평균을 구해 다음 시점을 예측하는 방식입니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;W_t = (2q+1)^{-1}\sum_{j=-q}^{q}X_{t-j}&lt;/script&gt;

&lt;p&gt;이때, $X_t = m_t + Y_t$ 이므로, 아래와 같은 식으로 유도됩니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;W_t = (2q+1)^{-1}\sum_{j=-q}^{q}X_{t-j} = (2q+1)^{-1}\sum_{j=-q}^{q}m_{t-j} + (2q+1)^{-1}\sum_{j=-q}^{q}Y_{t-j}&lt;/script&gt;

&lt;p&gt;만약에 $m_t$ 가 대략 선형관계를 띄고 있다면 잔차항의 평균은 0에 가까울 것입니다. 즉, 트렌드가 선형관계를 띄고 있을 때, moving average filter를 씌어주면 trend요소만 추출할 수 있는 것을 의미합니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;W_t = (2q+1)^{-1}\sum_{j=-q}^{q}X_{t-j} = (2q+1)^{-1}\sum_{j=-q}^{q}m_{t-j} + (2q+1)^{-1}\sum_{j=-q}^{q}Y_{t-j} \approx m_t&lt;/script&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/rEHZBt2.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 7. Moving average filter 취하기 전&lt;/figcaption&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/QPByqUu.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 8. Moving average filter 취한 후&lt;/figcaption&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/dPTzLn3.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 9. Trend 제거 후 잔차항&lt;/figcaption&gt;

위에 그림 7,8,9 를 살펴 봅시다. 그림 8은 그림 7에서 과거시점 5개를 이용하여 moving average 필터를 씌운 후입니다. 뚜렷한 트렌드가 있지 않음을 보실 수 있습니다. ~~잔차항에 대한 분석은 다시 한번 살펴봐야 할 것 같습니다.~~

Simple Moving Average Filter는 trend가 linear하고, Noise가 White Noise일 때, 시계열 데이터에서 trend요소를 잘 추출할 수 있습니다. 그러나 Non-linear한 trend라면, Noise가 White Noise라 하더라도, trend 추정이 올바르지 않습니다. 그럴 땐, 적절한 가중치를 부여하여 Moving Average Filter를 씌워야 합니다.

$$\sum_{j=-7}^{j=7}a_jX_{t-j} = \sum_{j=-7}^{j=7}a_j m_{t-j}+\sum_{j=-7}^{j=7}a_jY_{t-j} \approx \sum_{j=-7}^{j=7}a_j m_{t-j} = m_t$$

&lt;h5&gt;b) Exponential smoothing&lt;/h5&gt;
Moving averages는 과거 n개의 시점에 동일한 가중치를 부여하는 방법입니다. 그러나, 현재시점과 가까울수록 좀 더 현재시점에 영향을 많이 미치는 경우가 일반적으로 생각하기엔 자연스러울 수 있습니다. 예로 주식을 생각하면 될 것 같습니다. 따라서, Exponential smoothing 방법은 현재 시점에 가까울수록 더 큰 가중치를 주는 방법입니다. 

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/ciknR6Y.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 10. Exponential Smoothing&lt;/figcaption&gt;&lt;/p&gt;

Exponential Smoothing 수식은 아래와 같습니다.

$$\hat{m}_t = \alpha X_t + (1-\alpha)\hat{m}_{t-1},\,\,t=2, \dots, n,$$
$$\hat{m}_1=X_1$$

아래 그림은 그림 7을 exponential smoothing을 취한 trend 추정 그래프입니다.
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/hKOWuWu.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 11. Exponential Smoothing 취한 후&lt;/figcaption&gt;&lt;/p&gt;

&lt;h5&gt;c) Smoothing by elimination of high-frequency component&lt;/h5&gt;
trend를 추출하는 방법 중 하나로, 여러 frequency의 합으로 trend를 표현해서 이를 제거하는 것입니다(이 부분은 추후에 4장에 가서 다시 설명하도록 하겠습니다).

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/hn90Hgr.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 12. frequency합으로 smoothing을 취한 후( $\alpha=0.4$ )&lt;/figcaption&gt;&lt;/p&gt;

&lt;h5&gt;d) Polynomial fitting&lt;/h5&gt;
$m_t = a_0 + a_1t + a_2t^2 + \dots + a_nt^n$ 으로 모델링하여, $\sum_{t=1}^n(x_t-m_t)^2$ 을 최소화하는 방식으로 파라미터 $a_k,\,(k=0, \dots, k=n$ 을 구하는 방식으로 trend를 추정할 수 있습니다. 

&lt;del&gt;$X_t - Y_t = m_t$ 에서, $Y_t$ 는 stationary state을 가정하고 있기 때문에, polynomial model을 구축할 수 있는 것입니다.&lt;/del&gt;

&lt;h4&gt;method2. Trend Elimination by Differencing&lt;/h4&gt;
method1 방법은 trend를 추정한 뒤, 시계열 $\{X_t\}$ 에서 빼주는 방식으로 trend를 제거하였습니다. 이번엔 difference(차분)를 통해서 trend요소를 제거하는 방법을 알아보도록 하겠습니다. Lag-1 difference operator $\bigtriangledown$ 는 아래와 같습니다.

$$\bigtriangledown X_t = X_t-X_{t-1} = (1-B)X_t$$

B는 backward-shift operator로 $BX_t = X{t-1}$ 입니다. j lag difference는 $\bigtriangledown (X_t) = \bigtriangledown (\bigtriangledown^{j-1} (X_t))$ 입니다. 예를 들어, 2-lag difference는 아래와 같습니다.

$$ \begin{align*} \bigtriangledown^2 X_t&amp;amp;=\bigtriangledown (\bigtriangledown (X_t))=\bigtriangledown ((\bigtriangledown (X_t))\\&amp;amp;=(1-B)(1-B)X_t=(1-2B+B^2)X_t = X_t - 2X_{t-1} + X_{t-2}\end{align*} $$

&lt;h5&gt;Why difference helps eliminating trend components? (Maybe or seasonal components)&lt;/h5&gt;
여기서, 제가 공부하면서 궁금했던 포인트는 왜 difference가 trend 제거에 도움이 되는가? 였습니다. 제가 생각한 답은 아래와 같습니다. trend와 seasonal 요소를 제거하려는 이유는 '고정된 평균과 분산을 가지는 분포'를 가지기 위해서입니다. 그래야지 통계적 모델링이 가능하기 때문입니다. 즉 반대로 말하면, trend와 seasonal 요소는 시간에 따라 평균과 분산이 변함을 의미합니다. 즉 그 변하는 요소를 제거하기 위해서 difference를 하는 것입니다. 

difference를 통해서 변동성을 제거하는 건 고등학교 수학 때 배웠던 미분을 통해 이해할 수 있습니다. 예를 들어, 일차함수 $y=a+bx$ 는 x값에 따라 y값이 변합니다. 그러나 일차미분을 통해 구한 기울기 b값은 고정이 됩니다. 반면에 이차함수 $y=ax^2 + bx + c$ 는 이차미분을 통해 2a라는 고정값을 갖게 됩니다. 여기서 미분 과정을 difference라 생각하시면 됩니다.

&amp;gt; 영어로도 미분이 differentiation 임을 생각하면 와닿습니다.

일차함수 y는 변하는 특성 + 고정된 특성을 둘다 가지고 있는데 일차 미분을 통해 a라는 고정된 특성만을 추출하는 것입니다. 

만약에 trend가 일차함수와 같은 관계를 가지고 있다면 1-lag difference 만으로도 변동성을 잡을 수 있게 되는 것이지요. 마찬가지로 2-lag difference는 trend가 이차함수와 같은 관계를 가지고 있다면 적용되는 것입니다. 

그러나, 과도한 difference는 시계열을 과하게 변동성을 제거해 버려서, over-correction이 될 수도 있기 때문에 조심해야 합니다.

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/dPdnSMm.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 13. Difference 적용 전&lt;/figcaption&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/RPMUFSJ.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 14. Difference 적용 후&lt;/figcaption&gt;&lt;/p&gt;

&lt;h3&gt;1.5.2. Estimation and Elimination of Both Trend and Seasonality&lt;/h3&gt;

trend와 seasonal 요소가 다 있는 경우 아래와 같이 표현될 수 있습니다(additive model인 경우).&lt;del&gt;multiplicative model인 케이스도 있습니다.&lt;/del&gt;

$$X_t = m_t + s_t + Y_t, \,\, t=1, \dots, n,$$
$$where,\,\,EY_t = 0, s_{t+d}=s_t,\,\,and\,\,\sum_{j=1}^{d}s_j=0$$

두 가지 방법을 소개하겠습니다. 먼저, 첫번째 방법입니다.

&lt;h4&gt;method 1. Estimation of Trend and Seasonal components&lt;/h4&gt;
아래와 같은 데이터가 있을 때, trend와 seasonal 요소를 제거해 봅시다. 아래 시계열 같은 경우, 주기가 d=12로, 1년 단위로 싸이클이 반복되는 것을 확인할 수 있습니다.

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/hCcOOp9.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 15. Accidental Deaths, U.S.A., 1973-1978&lt;/figcaption&gt;&lt;/p&gt;

&lt;ol&gt;&lt;li&gt;먼저, trend 요소를 제외합니다. trend 요소를 제외하는 방법으로 moving average filter를 이용할 수 있습니다.&lt;br /&gt;&lt;br /&gt;
예를 들어, 시계열 시퀀스 $\{x_1, x_2, \dots, x_n\}$ 이고, 주기 period $d=2q$ 라 한다면, 아래와 같은 moving average filter 식을 세울 수 있습니다.

$$\hat{m_t} = (0.5x_{t-q} + x_{t-q+1} + \dots + x_{t+q-1} + 0.5x_{t+q})/d,\,\, q&amp;lt;t\leq n-q$$ 

&amp;gt; 양 끝에 0.5씩 붙는 이유는 분자의 갯수는 홀수개 즉 $2q+1$ 이지만, 분모는 짝수 $d=2q$ 이기 때문에, 양 끝에 항의 가중치를 0.5씩만 해주는 것입니다.

반면에, 주기가 $d=2q+1$ 이라면, 아래와 같은 식을 세울 수 있습니다.

$$\hat{m_t} = (2q+1)^{-1}\sum_{j=-q}^{q}X_{t-j}$$
&lt;/li&gt;
&lt;li&gt;그 다음은 seasonality 요소를 구하는 차례입니다. 먼저, 위에서 구한 trend요소를 원 시계열 데이터에서 $x_{k+jd} - \hat{m_{k+jd}}$ 와 같이 제거해야 합니다. 그런 다음, 동일한 주기에 해당하는 $x-\hat{m}$ 요소들을 가지고 평균 $w_k, \,\,(k=1, \dots, d)$ 를 구해줍니다. &lt;/li&gt;
&lt;li&gt;이 때, $w_k$ 들의 평균은 0이 아닐 수 있습니다. 따라서, seasonal 요소들의 평균이 0이 되도록 다시 한번 평균을 빼줍니다.&lt;del&gt;다시 한번 정규화가 되도록 해주는 것입니다.&lt;/del&gt;) 

$$\hat{s_k} = w_k - \frac{1}{d}\sum_{i=1}^{d}w_i,\,\,k=1, \dots, d$$

$$and, \,\, \hat{s_k}=\hat{s_{k-d}},\,k&amp;gt;d$$

따라서, deseaonalized된 데이터는 $d_t = x_t - \hat{s_t},\,\, t=1,\dots ,n$ 이며, detrended된 데이터는 $d_t = x_t - \hat{m_t},\,\, t=1, \dots, n$ 입니다.
&lt;/li&gt;
&lt;li&gt;마지막으로, noise 추정값은 trend와 seasonal 요소를 모두 제거한 항입니다.&lt;/li&gt;
&lt;li&gt;또한 trend 모델링을 위해, 다시 한번 parametric form으로 다시 한번 재추정하는 과정을 거칩니다. Parametric form으로 다시 한번 trend 요소를 재추정 하는 목적은 prediction과 simulation을 하기 위해서 입니다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/srdCVkU.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 16. Trend and seasonal decomposition 예시&lt;/figcaption&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/oYkGLqN.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 17. Trend and seasonal decomposition 예시&lt;/figcaption&gt;&lt;/p&gt;

&lt;h4&gt;method 2. Elimination of Trend and Seasonal components by Differencing&lt;/h4&gt;
Trend 요소를 Differencing 방법을 통해 제거한 것과 동일하게 진행됩니다. Differencing operator $\bigtriangledown_d$ 을 $X_t = m_t + s_t + Y_t$ 식 양변에 취해주면 아래와 같습니다.

$$\bigtriangledown_dX_t = m_t - m_{t-d} + Y_t - Y_{t-d}$$


&lt;h2&gt;1.6. Testing the Estimated Noise Sequence&lt;/h2&gt;

1.5까지 과정을 거치면 우린 Noise 항을 갖게 됩니다. 그러나 이 Noise 항이 White Noise 항인지는 확인이 필요합니다. 만약에 white noise 항이 맞다면, noise sequence를 모델링 하는 것입니다. 만약에 noise 항이 white noise가 아니라 여전히 depedency가 보인다면 다른 방법을 적용해야 합니다.

이번 챕터에서는 white noise인지를 확인하는 방법에 대해 살펴봅니다.

&lt;h3&gt;(a) The sample autocorrelation function&lt;/h3&gt;

Sample acf를 그려서, 95%신뢰구간 안에 대부분 들어와 있는지 확인합니다. 만약 2,3개 이상이 신뢰구간 밖에 있거나 1개가 유난히 구간 안에 멀리 벗어 났다면, 우린 white noise라고 세웠던 가설을 기각해야 합니다.

&lt;h3&gt;(b) The portmanteau test(Ljung-Box test)&lt;/h3&gt;

Portmanteau 검정 통계량은 &lt;b&gt;일정 기간 동안 일련의 관측치가 랜덤이고 독립적인지 여부를 검사하는데 사용합니다&lt;/b&gt;. 통계량은 아래와 같습니다(Box-pierece 검정이라고도 합니다.). 

$$Q = n\sum_{j=1}^{h}\hat{\rho(j)^2}$$

$\hat{\rho(j)}$ 가 0에 가깝다면, $\hat{\rho(j)^2}$ 은 더욱 0에 가까울 것이지만, 몇몇 $\hat{\rho(j)}$ 의 절대값이 크다면, 그 항들에 영향을 받아 전체적인 Q값도 커지게 될 것입니다. 

&amp;gt; h는 lag입니다. h를 무리하게 크게 잡는다면, Q값은 커질 위험이 있습니다. 적당한 h를 잡는 것이 중요합니다.

귀무가설은 시차 h에 대한 자기 상관이 0이라는 귀무가설을 검정합니다. 통계량이 지정된 임계값보다 크면 하나 이상의 시차에 대한 자기 상관이 0과 유의하게 다르며, 일정 기간 랜덤 및 독립적이지 않음을 뜻합니다.

아래는 좀 더 refined된 통계량으로 Ljung-Box 라 합니다.

$$Q = n(n+2)\sum_{k=1}^{h}\hat{\rho(k)}/(n-k)$$

그 밖에, turning point test, difference-sign test, rank test, fitting an autoregressive model, checking for normality등이 있습니다. 

&lt;hr /&gt;
이상으로, &amp;lt;Introduction to Time Series and forecasting 리뷰) 1. Introduction to Time Series&amp;gt; 포스팅을 마치겠습니다. 

&lt;hr /&gt;

&lt;ol&gt;&lt;li&gt;
&lt;a href=&quot;&quot; url=&quot;https://blog.naver.com/sw4r/221024668866&quot;&gt;Strict Stationarity vs. Weak Stationarity, https://blog.naver.com/sw4r/221024668866&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;고려대학교 김성범 교수님 &lt;예측모델&gt; 수업자료&amp;lt;/li&amp;gt;
&lt;li&gt;&lt;a href=&quot;&quot; url=&quot;https://otexts.com/fppkr/residuals.html&quot;&gt;portmanteau 검정 : https://otexts.com/fppkr/residuals.html&amp;lt;/li&amp;gt;&amp;lt;/ol&amp;gt;
&lt;/a&gt;&lt;/li&gt;&lt;/예측모델&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/p&gt;</content><author><name>Seonhwa Lee</name></author><category term="Time Series Analysis" /><category term="time-series-analysis" /><summary type="html">이번 포스팅을 시작으로, 시계열 분석에 대해서 다루도록 하겠습니다. 메인 교재는 Brockwell와 Richard A. Davis의 &amp;lt; Introduction to Time Series and Forecasting &amp;gt; 와 패스트캠퍼스의 &amp;lt;파이썬을 활용한 시계열 분석 A-Z&amp;gt; 를 듣고 정리하였습니다. 1.1. What is Time Series? 시계열이란, 일정 시간 간격으로 배치된 데이터들의 수열입니다. 그 중에서 discrete한 시계열이란 관측이 발생한 시각 t의 집합 $T_0$ 가 discrete한 경우이며, 관측치가 시간 구간 안에서 연속적으로 발생한다면 continuous한 시계열입니다. 시계열 시퀀스는 일반적으로 자기 상관성이 있는 수열입니다. 즉, 과거의 데이터가 현재를 넘어서 미래까지 영향을 미치는 것을 뜻합니다. 따라서, 시계열 데이터로 모델링을 하기 위해선 먼저 데이터를 최대한 분해해서 살펴봐야 합니다. 확률 모델링을 하기 위해선 i.i.d 여야 하기 때문입니다. 일반적으로 시계열 데이터는 trend, seasonality, noise 항으로 구성되어 있습니다. 여기서 시계열 데이터가 자기 상관성을 가지게 되는 요인은 trend와 seasonality 요소 때문이고, noise는 i.i.d한 독립변수로 구성된 에러항입니다. 1.2. Objectives of Time Series Analysis 시계열 분석의 목적은 주로 시계열 데이터를 보고 앞으로 일어날 일들을 예측하는 것입니다. 그러기 위에선 기존에 있는 시계열 데이터를 가지고 추론을 해야합니다. 따라서, 이러한 추론을 하기 위해선 가정에 맞는 적절한 확률 모델을 선택하여 모델링을 진행해야 합니다. 그러나, 시계열 데이터는 자기 상관성이 존재하는 데이터입니다. 따라서 확률적 모델링을 통해 이 시계열 데이터를 서로 독립인 데이터로 변환해야 하는데 이 과정이 seasonal adjustment 또는 trend and seasonal decomposition입니다. 그 밖에 log transformation, differencing 같은 과정도 존재합니다. 어쨌든, 시계열 분석의 궁극적인 목표는 독립적인 변수로 최대한 변환한 뒤, 이를 기반으로 확률적 통계 모델링을 해서, inference를 하는 것입니다. Inference 결과는 다시 우리가 얻고자 하는 예측값으로 바꾸기 위한 reverting 과정을 거쳐야 합니다. 왜냐하면, seasonal adjustment나 Decomposition을 통해 상관성을 제거했기 때문에 원하는 예측값을 얻기 위해선 다시 원래대로 이 과정을 뒤집어서 돌아가야 하는 것입니다. 1.3. Some Simple Time Series Models 위에서 말씀 드린 것과 같이 시계열 데이터를 보고 적절한 확률 모델을 선택하는 것은 매우 중요합니다. 따라서, 몇가지 간단한 time series model을 소개하겠습니다. 1.3.1. Definition of a time series model 관측된 ${x_t}$ 에 대한 time-series 모델이란, 랜덤 변수 ${X_t}$ 시퀀스들의 joint distribution 을 의미합니다. A time series model for the observed data ${x_t}$ is a specification of the joint distributions(or possibly only the means and covariances) of a sequence of random variables ${X_t}$ of which ${x_t}$ is postulated to be a realization. 즉, 랜덤 변수들의 시퀀스 ${X_1, X_2, \dots }$ 로 구성된 time-series 확률 모델은 랜덤 벡터 $(X_1, \dots, x_n)’ ,\,\, n=1,2,\dots,$ 의 결합 분포입니다. 아래 그림은 랜던 변수들의 시퀀스 ${S_t, t=1, \dots, 200}$ 로 나올 수 있는 가능성 중 한가지가 ‘실현’ 된 것입니다. 그림 1. Time-series 예시 1.3.2. Some Simple Time Series Model iid Noise 가장 기본적인 time series 모델은 noise항으로만 이뤄진 경우입니다.(거의 현실세계에선 없다고 생각하시면 됩니다.) Binary Process i.i.d Noise의 종류로, binary 분포를 따르는 noise인 경우입니다. 랜덤 변수들의 시퀀스 $\{X_t,\,\,t=1,2,\dots,\}$ 가 $P[X_t = 1]=p$ , $P[X_t = -1] = 1-p$ 를 따릅니다. Models with only Trend trend요소와 noise항만 있는 경우입니다. 여기서 trend요소란 패턴이 선형관계를 가지고 있을 때입니다. 자세히 말하면, 시계열이 시간에 따라 증가, 감소, 또는 일정 수준을 유지하는 경우입니다. $$X_t = m_t + Y_t$$ 그림 2. Time series with only trend component Models with only Seaonality seasonal요소와 noise항만 있는 경우입니다. 여기서 seasonal요소란 일정한 빈도로 주기적으로 반복되는 패턴을 말합니다. 반면에, 일정하지 않은 빈도로 발생하는 패턴은 Cycle이라 합니다.(여기서는 seasonal 기준으로 설명하겠습니다.) $$X_t = S_t + Y_t$$ 그림 3. Times series with only seasonality(period=12month) 1.3.3 A General Approach to Time Series Modeling 시계열 분석에 대해 깊게 들어가기 전에, 시계열 데이터 모델링하는 방법에 대해 대략적으로 알아봅시다. 1) 그래프로 그린 후, 그래프 상에서 아래와 같은 요소가 있는지 체크한다.(Plot the series and examine the main features of the graph) trenda seasonal componentany apparent sharp changes in behaviorany outlying observations 2) 정상상태의 잔차를 얻기 위해, trend와 seasonality 요소를 제거한다. (Remove the trend and seasonal components to get stationary residuals)      trend와 seasonality 요소를 제거하기 전에, 전처리를 해야하는 경우가 있습니다. 예를 들어, 아래와 같이 지수적으로 증가하는 경우에, 로그를 취해서 variance가 일정하도록 만든 후 모델링을 하면 정확도를 높일 수 있습니다. 그림 1. 로그 취하기 전 그림 2. 로그 취한 후 이외에도 여러 방법이 있습니다. 추후에 설명하도록 하겠습니다. 어쨌든, 이 모든 방법들의 핵심은 정상상태의 잔차를 만드는 것입니다. 3) auto-correlation 함수, 여러 다양한 통계량을 이용하여 잔차를 핏팅할 모델을 선택한다. (Choose a model to fit the residuals, making use of various sample statistics including the sample autocorrelation function) 4) 핏팅된 모델로 예측한다.      여기서 잔차를 예측하는 것이고, 예측된 잔차를 원래 예측해야 할 값으로 변환한다. 1.4. Stationary Models and the Autocorrelation Function 시계열 데이터가 정상상태(stationarity)를 가지기 위해서, 시계열이 확률적인 특징이 시간이 지남에 따라 변하지 않는다는 가정을 충족시켜야 합니다. 그러나 시계열 데이터는 trend와 seasonality요소로 인해, 평균과 분산이 변할 수 있습니다. a time series ${{X_t, t=0, \pm1, …}}$ is said to be stationary if it has statistical properties similar to those of the “time-shifted” series ${{X_{t+h}, t=0, \pm1, …}}$ for each integer h. Trends can result in a varying mean over time, whereas seasonality can result in a changing variance over time, both which define a time series as being non-stationary. Stationary datasets are those that have a stable mean and variance, and are in turn much easier to model. 시계열에 대한 평균과 공분산은 아래와 같이 정의됩니다. 그림 3. 시계열의 평균과 공분산 Strict Stationarity vs. Weak Stationarity 엄격한 정상상태가 되려면, $(X_1,\dots , X_n)$ 의 결합분포와 $(X_{1+h}, \dots, X_{n+h})$ 의 결합분포가 시간간격 h에 상관없이 동일해야 합니다. 그러나 이를 이론적으로 증명하기 어렵기 때문에, 약한 정상상태(weak stationarity)만을 만족하면 정상상태에 있다고 생각하고 시계열 문제를 풉니다. 약한 정상상태는 아래 조건을 만족합니다. 즉, 결합분포가 동일해야 한다는 강력한 조건이 사라졌기 때문에 약한 정상상태라고 하는 것입니다. (2)식은 공분산은 t에 독립임을 의미합니다. 정상상태 시계열의 공분산은 아래와 같이 하나의 변수 h에 대해 나타낼 수 있습니다. 이때 함수 $\gamma_X(\cdot)$ 을 lag h에 대한 auto-covariance 함수(ACVF)라 합니다. auto-correlation 함수(ACF)는 ACVF를 이용해 아래와 같이 정의됩니다. White Noise 시계열 ${{X_t}}$ 가 독립적인 랜덤 변수의 시퀀스이고, 평균이 0이고, 분산이 $\sigma^2$ 이면, White Noise라 합니다. 아래는 White Noise의 조건입니다. 1.4.1 The Sample Autocorrelation Function 관측 데이터 가지고 자기 상관의 정도를 볼때, sample auto-correlation 함수(sample ACF)를 사용합니다. Sample ACF는 ACF의 추정으로, 계산은 아래와 같습니다. 그림 4. Sample ACF White Noise인 경우, 시계열 그래프와 ACF 그래프는 아래와 같습니다. lag가 1이상인 경우, 거의 ACF값이 0에 가까운 것을 볼 수 있고, 95% 신뢰구간 안에 들어와 있습니다. 그림 5. White Noise ACF 아래는 그림 1. 그래프에 플롯된 데이터를 가지고 그린 ACF입니다. 보시면, ACF가 lag가 커짐에 따라 서서히 감소하는 형태를 띄는데 이는 trend가 있는 데이터에서 나타납니다. 그림 6. Sequence with trend ACF 1.5. Estimation and Elimination of Trend and Seasonal Components trend와 seasonality가 존재하는 시계열의 모델링인 경우, 아래와 같이 additive 형태를 띌 수 있습니다. 시계열 모델링의 최종 목표는 잔차항 $Y_t$ 가 정상상태에 놓이게 하는 것입니다. 따라서 잔차항을 분석하기 위해서 trend 요소 $m_t$ 와 seasonal 요소 $s_t$ 를 제거해야 합니다. 1.5.1. Estimation and Elimination of Trend in the Absence of Seasonality seasonal 요소가 없고, trend요소만 있는 모델링은 아래와 같이 진행할 수 있습니다. method1. Trend Estimation trend 요소를 추정하는 방법은 Moving Average와 Smoothing을 이용하는 방법 2가지가 있습니다. a) Smoothing with a finite moving average filter 과거 n개의 시점을 평균을 구해 다음 시점을 예측하는 방식입니다. 이때, $X_t = m_t + Y_t$ 이므로, 아래와 같은 식으로 유도됩니다. 만약에 $m_t$ 가 대략 선형관계를 띄고 있다면 잔차항의 평균은 0에 가까울 것입니다. 즉, 트렌드가 선형관계를 띄고 있을 때, moving average filter를 씌어주면 trend요소만 추출할 수 있는 것을 의미합니다. 그림 7. Moving average filter 취하기 전 그림 8. Moving average filter 취한 후 그림 9. Trend 제거 후 잔차항 위에 그림 7,8,9 를 살펴 봅시다. 그림 8은 그림 7에서 과거시점 5개를 이용하여 moving average 필터를 씌운 후입니다. 뚜렷한 트렌드가 있지 않음을 보실 수 있습니다. ~~잔차항에 대한 분석은 다시 한번 살펴봐야 할 것 같습니다.~~ Simple Moving Average Filter는 trend가 linear하고, Noise가 White Noise일 때, 시계열 데이터에서 trend요소를 잘 추출할 수 있습니다. 그러나 Non-linear한 trend라면, Noise가 White Noise라 하더라도, trend 추정이 올바르지 않습니다. 그럴 땐, 적절한 가중치를 부여하여 Moving Average Filter를 씌워야 합니다. $$\sum_{j=-7}^{j=7}a_jX_{t-j} = \sum_{j=-7}^{j=7}a_j m_{t-j}+\sum_{j=-7}^{j=7}a_jY_{t-j} \approx \sum_{j=-7}^{j=7}a_j m_{t-j} = m_t$$ b) Exponential smoothing Moving averages는 과거 n개의 시점에 동일한 가중치를 부여하는 방법입니다. 그러나, 현재시점과 가까울수록 좀 더 현재시점에 영향을 많이 미치는 경우가 일반적으로 생각하기엔 자연스러울 수 있습니다. 예로 주식을 생각하면 될 것 같습니다. 따라서, Exponential smoothing 방법은 현재 시점에 가까울수록 더 큰 가중치를 주는 방법입니다. 그림 10. Exponential Smoothing Exponential Smoothing 수식은 아래와 같습니다. $$\hat{m}_t = \alpha X_t + (1-\alpha)\hat{m}_{t-1},\,\,t=2, \dots, n,$$ $$\hat{m}_1=X_1$$ 아래 그림은 그림 7을 exponential smoothing을 취한 trend 추정 그래프입니다. 그림 11. Exponential Smoothing 취한 후 c) Smoothing by elimination of high-frequency component trend를 추출하는 방법 중 하나로, 여러 frequency의 합으로 trend를 표현해서 이를 제거하는 것입니다(이 부분은 추후에 4장에 가서 다시 설명하도록 하겠습니다). 그림 12. frequency합으로 smoothing을 취한 후( $\alpha=0.4$ ) d) Polynomial fitting $m_t = a_0 + a_1t + a_2t^2 + \dots + a_nt^n$ 으로 모델링하여, $\sum_{t=1}^n(x_t-m_t)^2$ 을 최소화하는 방식으로 파라미터 $a_k,\,(k=0, \dots, k=n$ 을 구하는 방식으로 trend를 추정할 수 있습니다. $X_t - Y_t = m_t$ 에서, $Y_t$ 는 stationary state을 가정하고 있기 때문에, polynomial model을 구축할 수 있는 것입니다. method2. Trend Elimination by Differencing method1 방법은 trend를 추정한 뒤, 시계열 $\{X_t\}$ 에서 빼주는 방식으로 trend를 제거하였습니다. 이번엔 difference(차분)를 통해서 trend요소를 제거하는 방법을 알아보도록 하겠습니다. Lag-1 difference operator $\bigtriangledown$ 는 아래와 같습니다. $$\bigtriangledown X_t = X_t-X_{t-1} = (1-B)X_t$$ B는 backward-shift operator로 $BX_t = X{t-1}$ 입니다. j lag difference는 $\bigtriangledown (X_t) = \bigtriangledown (\bigtriangledown^{j-1} (X_t))$ 입니다. 예를 들어, 2-lag difference는 아래와 같습니다. $$ \begin{align*} \bigtriangledown^2 X_t&amp;amp;=\bigtriangledown (\bigtriangledown (X_t))=\bigtriangledown ((\bigtriangledown (X_t))\\&amp;amp;=(1-B)(1-B)X_t=(1-2B+B^2)X_t = X_t - 2X_{t-1} + X_{t-2}\end{align*} $$ Why difference helps eliminating trend components? (Maybe or seasonal components) 여기서, 제가 공부하면서 궁금했던 포인트는 왜 difference가 trend 제거에 도움이 되는가? 였습니다. 제가 생각한 답은 아래와 같습니다. trend와 seasonal 요소를 제거하려는 이유는 '고정된 평균과 분산을 가지는 분포'를 가지기 위해서입니다. 그래야지 통계적 모델링이 가능하기 때문입니다. 즉 반대로 말하면, trend와 seasonal 요소는 시간에 따라 평균과 분산이 변함을 의미합니다. 즉 그 변하는 요소를 제거하기 위해서 difference를 하는 것입니다. difference를 통해서 변동성을 제거하는 건 고등학교 수학 때 배웠던 미분을 통해 이해할 수 있습니다. 예를 들어, 일차함수 $y=a+bx$ 는 x값에 따라 y값이 변합니다. 그러나 일차미분을 통해 구한 기울기 b값은 고정이 됩니다. 반면에 이차함수 $y=ax^2 + bx + c$ 는 이차미분을 통해 2a라는 고정값을 갖게 됩니다. 여기서 미분 과정을 difference라 생각하시면 됩니다. &amp;gt; 영어로도 미분이 differentiation 임을 생각하면 와닿습니다. 일차함수 y는 변하는 특성 + 고정된 특성을 둘다 가지고 있는데 일차 미분을 통해 a라는 고정된 특성만을 추출하는 것입니다. 만약에 trend가 일차함수와 같은 관계를 가지고 있다면 1-lag difference 만으로도 변동성을 잡을 수 있게 되는 것이지요. 마찬가지로 2-lag difference는 trend가 이차함수와 같은 관계를 가지고 있다면 적용되는 것입니다. 그러나, 과도한 difference는 시계열을 과하게 변동성을 제거해 버려서, over-correction이 될 수도 있기 때문에 조심해야 합니다. 그림 13. Difference 적용 전 그림 14. Difference 적용 후 1.5.2. Estimation and Elimination of Both Trend and Seasonality trend와 seasonal 요소가 다 있는 경우 아래와 같이 표현될 수 있습니다(additive model인 경우).multiplicative model인 케이스도 있습니다. $$X_t = m_t + s_t + Y_t, \,\, t=1, \dots, n,$$ $$where,\,\,EY_t = 0, s_{t+d}=s_t,\,\,and\,\,\sum_{j=1}^{d}s_j=0$$ 두 가지 방법을 소개하겠습니다. 먼저, 첫번째 방법입니다. method 1. Estimation of Trend and Seasonal components 아래와 같은 데이터가 있을 때, trend와 seasonal 요소를 제거해 봅시다. 아래 시계열 같은 경우, 주기가 d=12로, 1년 단위로 싸이클이 반복되는 것을 확인할 수 있습니다. 그림 15. Accidental Deaths, U.S.A., 1973-1978 먼저, trend 요소를 제외합니다. trend 요소를 제외하는 방법으로 moving average filter를 이용할 수 있습니다. 예를 들어, 시계열 시퀀스 $\{x_1, x_2, \dots, x_n\}$ 이고, 주기 period $d=2q$ 라 한다면, 아래와 같은 moving average filter 식을 세울 수 있습니다. $$\hat{m_t} = (0.5x_{t-q} + x_{t-q+1} + \dots + x_{t+q-1} + 0.5x_{t+q})/d,\,\, q&amp;lt;t\leq n-q$$ &amp;gt; 양 끝에 0.5씩 붙는 이유는 분자의 갯수는 홀수개 즉 $2q+1$ 이지만, 분모는 짝수 $d=2q$ 이기 때문에, 양 끝에 항의 가중치를 0.5씩만 해주는 것입니다. 반면에, 주기가 $d=2q+1$ 이라면, 아래와 같은 식을 세울 수 있습니다. $$\hat{m_t} = (2q+1)^{-1}\sum_{j=-q}^{q}X_{t-j}$$ 그 다음은 seasonality 요소를 구하는 차례입니다. 먼저, 위에서 구한 trend요소를 원 시계열 데이터에서 $x_{k+jd} - \hat{m_{k+jd}}$ 와 같이 제거해야 합니다. 그런 다음, 동일한 주기에 해당하는 $x-\hat{m}$ 요소들을 가지고 평균 $w_k, \,\,(k=1, \dots, d)$ 를 구해줍니다. 이 때, $w_k$ 들의 평균은 0이 아닐 수 있습니다. 따라서, seasonal 요소들의 평균이 0이 되도록 다시 한번 평균을 빼줍니다.다시 한번 정규화가 되도록 해주는 것입니다.) $$\hat{s_k} = w_k - \frac{1}{d}\sum_{i=1}^{d}w_i,\,\,k=1, \dots, d$$ $$and, \,\, \hat{s_k}=\hat{s_{k-d}},\,k&amp;gt;d$$ 따라서, deseaonalized된 데이터는 $d_t = x_t - \hat{s_t},\,\, t=1,\dots ,n$ 이며, detrended된 데이터는 $d_t = x_t - \hat{m_t},\,\, t=1, \dots, n$ 입니다. 마지막으로, noise 추정값은 trend와 seasonal 요소를 모두 제거한 항입니다. 또한 trend 모델링을 위해, 다시 한번 parametric form으로 다시 한번 재추정하는 과정을 거칩니다. Parametric form으로 다시 한번 trend 요소를 재추정 하는 목적은 prediction과 simulation을 하기 위해서 입니다. 그림 16. Trend and seasonal decomposition 예시 그림 17. Trend and seasonal decomposition 예시 method 2. Elimination of Trend and Seasonal components by Differencing Trend 요소를 Differencing 방법을 통해 제거한 것과 동일하게 진행됩니다. Differencing operator $\bigtriangledown_d$ 을 $X_t = m_t + s_t + Y_t$ 식 양변에 취해주면 아래와 같습니다. $$\bigtriangledown_dX_t = m_t - m_{t-d} + Y_t - Y_{t-d}$$ 1.6. Testing the Estimated Noise Sequence 1.5까지 과정을 거치면 우린 Noise 항을 갖게 됩니다. 그러나 이 Noise 항이 White Noise 항인지는 확인이 필요합니다. 만약에 white noise 항이 맞다면, noise sequence를 모델링 하는 것입니다. 만약에 noise 항이 white noise가 아니라 여전히 depedency가 보인다면 다른 방법을 적용해야 합니다. 이번 챕터에서는 white noise인지를 확인하는 방법에 대해 살펴봅니다. (a) The sample autocorrelation function Sample acf를 그려서, 95%신뢰구간 안에 대부분 들어와 있는지 확인합니다. 만약 2,3개 이상이 신뢰구간 밖에 있거나 1개가 유난히 구간 안에 멀리 벗어 났다면, 우린 white noise라고 세웠던 가설을 기각해야 합니다. (b) The portmanteau test(Ljung-Box test) Portmanteau 검정 통계량은 일정 기간 동안 일련의 관측치가 랜덤이고 독립적인지 여부를 검사하는데 사용합니다. 통계량은 아래와 같습니다(Box-pierece 검정이라고도 합니다.). $$Q = n\sum_{j=1}^{h}\hat{\rho(j)^2}$$ $\hat{\rho(j)}$ 가 0에 가깝다면, $\hat{\rho(j)^2}$ 은 더욱 0에 가까울 것이지만, 몇몇 $\hat{\rho(j)}$ 의 절대값이 크다면, 그 항들에 영향을 받아 전체적인 Q값도 커지게 될 것입니다. &amp;gt; h는 lag입니다. h를 무리하게 크게 잡는다면, Q값은 커질 위험이 있습니다. 적당한 h를 잡는 것이 중요합니다. 귀무가설은 시차 h에 대한 자기 상관이 0이라는 귀무가설을 검정합니다. 통계량이 지정된 임계값보다 크면 하나 이상의 시차에 대한 자기 상관이 0과 유의하게 다르며, 일정 기간 랜덤 및 독립적이지 않음을 뜻합니다. 아래는 좀 더 refined된 통계량으로 Ljung-Box 라 합니다. $$Q = n(n+2)\sum_{k=1}^{h}\hat{\rho(k)}/(n-k)$$ 그 밖에, turning point test, difference-sign test, rank test, fitting an autoregressive model, checking for normality등이 있습니다. 이상으로, &amp;lt;Introduction to Time Series and forecasting 리뷰) 1. Introduction to Time Series&amp;gt; 포스팅을 마치겠습니다. Strict Stationarity vs. Weak Stationarity, https://blog.naver.com/sw4r/221024668866 고려대학교 김성범 교수님 수업자료&amp;lt;/li&amp;gt; portmanteau 검정 : https://otexts.com/fppkr/residuals.html&amp;lt;/li&amp;gt;&amp;lt;/ol&amp;gt;</summary></entry><entry><title type="html">RDD, Resilient Distributed DataSet에 대하여[1]</title><link href="http://localhost:4000/spark%20programming/2020/11/20/rdd/" rel="alternate" type="text/html" title="RDD, Resilient Distributed DataSet에 대하여[1]" /><published>2020-11-20T00:00:00+09:00</published><updated>2020-11-20T00:00:00+09:00</updated><id>http://localhost:4000/spark%20programming/2020/11/20/rdd</id><content type="html" xml:base="http://localhost:4000/spark%20programming/2020/11/20/rdd/">&lt;p&gt;이번 포스팅은 “빅데이터 분석을 위한 스파크2 프로그래밍 - Chaper2. RDD” 를 읽고 정리하였습니다. 정리 순서는 책 순서와 동일하고, 책을 읽어가면서 이해가 안되는 부분을 추가적으로 정리하였습니다.&lt;/p&gt;

&lt;h2&gt;2.1 RDD&lt;/h2&gt;
&lt;h3&gt;2.1.1 들어가기에 앞서&lt;/h3&gt;

&lt;p&gt;RDD를 공부하기 전 기억하고 넘어가야 할 것들에 대해 정리하였습니다.&lt;/p&gt;
&lt;h4&gt;1. 스파크 클러스터&lt;/h4&gt;
&lt;p&gt;클러스터란 여러 대의 서버가 마치 한대의 서버처럼 동작하는 것을 뜻합니다. 스파크는 클러스터 환경에서 동작하며 대량의 데이터를 여러 서버에서 병렬 처리합니다&lt;/p&gt;

&lt;h4&gt;2. 분산 데이터로서의 RDD&lt;/h4&gt;
&lt;p&gt;RDD는 Resilient Distrubuted Datasets으로, ‘회복력을 가진 분산 데이터 집합’이란 뜻입니다. (Resilient : 회복력이 있는) 데이터를 처리하는 과정에서 문제가 발생하더라도 스스로 복구할 수 있는 것을 의미합니다.
이는 그 다음 설명 &lt;b&gt;트랜스포메이션과 액션&lt;/b&gt;과 &lt;b&gt;지연(lazy) 동작과 최적화&lt;/b&gt; 부분과 함께 다시 설명드리도록 하겠습니다.&lt;/p&gt;

&lt;h4&gt;3. 트랜스포메이션과 액션&lt;/h4&gt;
&lt;p&gt;RDD가 제공하는 연산은 크게 트랜스포메이션과 액션이 있습니다. “연산”은 흔히 “메서드”로 이해하시면 됩니다.&lt;br /&gt;
트랜스포메이션은 RDD의 변형을 일으키는 연산이고, 실제로 동작이 수행되지는 않습니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/wWLMGK1.jpg&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 1. RDD 예시&lt;/figcaption&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://imgur.com/ooJKxAu.png&quot; /&gt;&lt;figcaption align=&quot;center&quot;&gt;그림 2.RDD 예시(2)&lt;/figcaption&gt;&lt;/p&gt;

&lt;p&gt;아래 예시를 보면, 데이터를 읽어 RDD를 생성해서 file변수에 저장한 뒤, flatMap -&amp;gt; map -&amp;gt; reduceByKey 함수를 거치면서 RDD[2], RDD[3], RDD[8]을 새로 생성하는 것을 볼 수 있습니다. 이렇게 transformation을 이전 RDD를 변형해서 새로운 RDD를 생성하는 것입니다.&lt;/p&gt;

&lt;p&gt;반면에, action은 동작을 수행해서 원하는 타입의 결과를 만들어내는 것이므로, saveAsTextFile로 수행됩니다. 따라서, saveAsTextFile은 action 연산에 해당됩니다.&lt;/p&gt;

&lt;h4&gt;4. 지연 동작과 최적화&lt;/h4&gt;
&lt;p&gt;지연 동작이란, 액션 연산이 수행되기 전까지 실제로 트랜스포메이션 연산을 수행하지 않는 것입니다. 이는 RDD의 특성 중 하나인 ‘회복력’과 관련있습니다. 액션 연산이 수행되기 전까지 동작이 &lt;b&gt;지연&lt;/b&gt;이 되는데, 대신에 RDD가 생성되는 방법을 기억하는 것입니다. 따라서 문제가 발생하더라도 기존에 RDD가 생성되는 방법을 기억하여 연산 수행에 문제가 없도록 하는 것입니다. 이는 위의 예시에서 reduceByKey까지는 실제로 트랜스포메이션 연산을 수행하는 것이 아니라 해당 연산을 순서대로 기억해놨다가, saveAsFile연산이 수행될 때(액션 연산이 수행될 때) 비로소 트랜스포메이션 연산도 수행된 것입니다.&lt;/p&gt;

&lt;p&gt;지연 동작 방식의 큰 장점은 &lt;b&gt;실행계획의 최적화&lt;/b&gt;입니다.&lt;/p&gt;

&lt;h4&gt;RDD의 불변성&lt;/h4&gt;
&lt;p&gt;오류로 인해 스파크의 데이터가 일부 유실되면, 데이터를 다시 만들어내는 방식으로 복구되는 것이 RDD의 불변성입니다. 이는 위에서 계속 언급한 “회복력”과 관련됩니다.&lt;/p&gt;

&lt;p&gt;RDD는 RDD1-&amp;gt;RDD2-&amp;gt; … 가 되면서 한번 만들어진 RDD는 내용이 변경되지 않습니다. RDD를 만드는 방법을 기억해서 문제가 발생 시 언제든지 똑같은 데이터를 생성할 수 있습니다.&lt;/p&gt;

&lt;h4&gt;5. 파티션과 HDFS&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;RDD데이터는 클러스터를 구성하는 여러 서버에 나뉘어서 저장됨&lt;/li&gt;
  &lt;li&gt;이 때, 분할된 데이터를 파티션 단위로 관리합니다.&lt;/li&gt;
  &lt;li&gt;HDFS는 하둡의 파일 시스템(hadoop distributed file system)&lt;/li&gt;
  &lt;li&gt;스파크는 하둡 파일 입출력 API에 의존성을 가지고 있음.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;6. Job, Executor, 드라이버 프로그램&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Job : 스파크 프로그램 실행하는 것 = 스파크 잡(job)을 실행하는 것&lt;/li&gt;
  &lt;li&gt;하나의 잡은 클러스터에서 병렬로 처리됨&lt;/li&gt;
  &lt;li&gt;이 때, 클러스터를 구성하는 각 서버마다 executor라는 프로세스가 생성&lt;/li&gt;
  &lt;li&gt;각 executor는 할당된 파티션 데이터를 처리함&lt;/li&gt;
  &lt;li&gt;드라이버란 ? 스파크에서 잡을 실행하는 프로그램으로, 메인함수를 가지고 있는 프로그램&lt;/li&gt;
  &lt;li&gt;드라이버에서 스파크 컨테스트를 생성하고 그 인스턴스를 포함하고 있는 프로그램&lt;/li&gt;
  &lt;li&gt;스파크컨테스트를 생성해 클러스터의 각 워커 노드들에게 작업을 지시하고 결과를 취합하는 역할을 수행&lt;/li&gt;
  &lt;li&gt;아래 코드를 보면, main함수 안에 sparkcontext를 생성하고 sc라는 인스턴스를 포함하고 있는 것을 볼 수 있음. 즉, main함수를 가지고 있는 프로그램이 ‘드라이버’에 해당됨&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Java&quot;&gt;Public static void main(String[] args){
	...
	JavaSparkContext s c = getSparkContext(&quot;WordCount&quot;, args[0]);
	...}
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;7. 함수의 전달&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;스파크는 함수형 프로그래밍 언어인 스칼라로 작성되어 “함수”를 다른 함수의 “매개변수”로서 전달 가능&lt;/li&gt;
  &lt;li&gt;아래 예제(Scala)를 보면 map의 인자에 ‘_+1’이 전달되는데, 익명 함수로 전달되는 것임&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Scala&quot;&gt;val rdd1 = sc.paralleize(1 to 10)
val rdd2 = rdd1.map(_+1)
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
  &lt;li&gt;파이썬으로 작성하면 아래와 같이, lambda 함수가 매개변수로 들어가게 됨&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;rdd1.map(lambda v:v+1)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;[참고]함수형 프로그래밍&lt;/h4&gt;
&lt;p&gt;함수형 프로그래밍과 객체 지향 프로그래밍의 차이를 통해 이해해보겠습니다. 객체 지향 프로그래밍은 객체 안에 상태를 저장하고, 해당 상태를 이용해서 제공할 수 있는(메소드)를 추가하고 상태변화를 ‘누가 어디까지 볼 수 있게 할지’를 설정하고 조정합니다. 따라서 적절한 상태 변경이 되도록 구성합니다. 반면에 함수형 프로그래밍은 상태 변경을 피하며 함수 간의 데이터 흐름을 사용합니다. 입력은 여러 함수들을 통해 흘러 다니게 됩니다. 따라서, 함수의 인자로 함수가 들어오고 반환의 결과로도 함수가 나올 수 있습니다.&lt;/p&gt;

&lt;h4&gt;함수 전달 시 유의할 점&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&quot;language-Scala&quot;&gt;Class PassingFunctionSample{
	val count=1
	def add(I: int):Int={
	count+i
	}
	
	def runMapSample(sc:SparkContext){
	val rdd1 = sc.parallelize(1 to 10);
	val rdd2 = rdd1.map(add)}
	}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;위와 같이 코드를 작성해서 실행하면, ‘java.io.NotSerializaionException’이라는 오류가 발생합니다. 이는 전달된 add함수가 클러스터를 구성하는 각 서버에서 동작할 수 있도록 전달되어야 하는데, 전달이 안되기 때문입니다. 그 이유는 add함수는 PassingFunctionSample의 메소드로 결국 클래스 PassingFunctionSample이 전체 다 전달되기 때문입니다. 해당 클래스는 Serializable 인터페이스를 구현하지 않습니다. 즉, 클래스가 각 서버에 전달될 수 있는 기능을 가지고 있지 않는 것입니다. 함수만 따로 전달되어야 하는 것입니다.&lt;/p&gt;

&lt;p&gt;스칼라 같은 경우 ‘싱글톤 객체’를 이용하여 해결 할 수 있습니다. 파이썬의 예제도 살펴보면, 아래는 클래스 전체가 전달되는 잘못된 예입니다.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;class PassingFunctionSample():

    def add1(self, i):
        return i + 1

    def runMapSample1(self, sc):
        rdd1 = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
        rdd2 = rdd1.map(self.add1) 
        # rdd2 = rdd1.map(add2)
        print(&quot;, &quot;.join(str(i) for i in rdd2.collect()))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;self로 인해 전체 클래스가 전달됩니다.(파이썬은 예외없이 실행되므로 유의할 것!)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;class PassingFunctionSample():

    @staticmethod
    def add1(self, i):
        return i + 1

    def runMapSample1(self, sc):
        rdd1 = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
        rdd2 = rdd1.map(add2)
        print(&quot;, &quot;.join(str(i) for i in rdd2.collect()))


if __name__ == &quot;__main__&quot;:

    def add2(i):
        return i + 1

    conf = SparkConf()
    sc = SparkContext(master=&quot;local[*]&quot;, appName=&quot;PassingFunctionSample&quot;, conf=conf)

    obj = PassingFunctionSample()
    obj.runMapSample1(sc)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;위와 같이 함수 add2가 독립적으로(클래스 전체가) 전달될 수 있도록 해야합니다.&lt;/p&gt;

&lt;h4&gt;변수 전달 시 유의할 점&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&quot;language-Scala&quot;&gt;class PassingFunctionSample {

	var increment = 1

  def runMapSample3(sc: SparkContext) {
    val rdd1 = sc.parallelize(1 to 10)
    val rdd2 = rdd1.map(_ + increment) \\익명함수 전달
    print(rdd2.collect.toList)
  }

  def runMapSample4(sc: SparkContext) {
    val rdd1 = sc.parallelize(1 to 10)
    val localIncrement = increment
    val rdd2 = rdd1.map(_ + localIncrement)
    print(rdd2.collect().toList)
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;runMapSample3 처럼 변수가 직접 전달되면 안되고, runMapSample4처럼 지역변수로 변환해서 전달해야 합니다. 그래야 나중에 변수가 변경되어 생기는 문제를 방지할 수 있습니다.&lt;/p&gt;

&lt;h4&gt;데이터 타입에 따른 RDD 연산&lt;/h4&gt;
&lt;p&gt;RDD 연산 함수에서 인자 타입을 보고 적절하게 맞는 연산 함수를 사용해야 합니다.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;이상으로 &amp;lt;RDD, Resilient Distributed DataSet에 대하여[1]&amp;gt; 마치겠습니다. 다음 포스팅에서 이어가도록 하겠습니다.&lt;/p&gt;

&lt;hr /&gt;
&lt;ol&gt;
  &lt;li&gt;함수형 언어, &lt;a href=&quot;https://sungjk.github.io/2017/07/17/fp.html&quot;&gt;https://sungjk.github.io/2017/07/17/fp.html&lt;/a&gt;, &lt;a href=&quot;https://docs.python.org/ko/3/howto/functional.html&quot;&gt;https://docs.python.org/ko/3/howto/functional.html&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Seonhwa Lee</name></author><category term="Spark Programming" /><category term="data-engineering" /><summary type="html">이번 포스팅은 “빅데이터 분석을 위한 스파크2 프로그래밍 - Chaper2. RDD” 를 읽고 정리하였습니다. 정리 순서는 책 순서와 동일하고, 책을 읽어가면서 이해가 안되는 부분을 추가적으로 정리하였습니다. 2.1 RDD 2.1.1 들어가기에 앞서 RDD를 공부하기 전 기억하고 넘어가야 할 것들에 대해 정리하였습니다. 1. 스파크 클러스터 클러스터란 여러 대의 서버가 마치 한대의 서버처럼 동작하는 것을 뜻합니다. 스파크는 클러스터 환경에서 동작하며 대량의 데이터를 여러 서버에서 병렬 처리합니다 2. 분산 데이터로서의 RDD RDD는 Resilient Distrubuted Datasets으로, ‘회복력을 가진 분산 데이터 집합’이란 뜻입니다. (Resilient : 회복력이 있는) 데이터를 처리하는 과정에서 문제가 발생하더라도 스스로 복구할 수 있는 것을 의미합니다. 이는 그 다음 설명 트랜스포메이션과 액션과 지연(lazy) 동작과 최적화 부분과 함께 다시 설명드리도록 하겠습니다. 3. 트랜스포메이션과 액션 RDD가 제공하는 연산은 크게 트랜스포메이션과 액션이 있습니다. “연산”은 흔히 “메서드”로 이해하시면 됩니다. 트랜스포메이션은 RDD의 변형을 일으키는 연산이고, 실제로 동작이 수행되지는 않습니다. 그림 1. RDD 예시 그림 2.RDD 예시(2) 아래 예시를 보면, 데이터를 읽어 RDD를 생성해서 file변수에 저장한 뒤, flatMap -&amp;gt; map -&amp;gt; reduceByKey 함수를 거치면서 RDD[2], RDD[3], RDD[8]을 새로 생성하는 것을 볼 수 있습니다. 이렇게 transformation을 이전 RDD를 변형해서 새로운 RDD를 생성하는 것입니다. 반면에, action은 동작을 수행해서 원하는 타입의 결과를 만들어내는 것이므로, saveAsTextFile로 수행됩니다. 따라서, saveAsTextFile은 action 연산에 해당됩니다. 4. 지연 동작과 최적화 지연 동작이란, 액션 연산이 수행되기 전까지 실제로 트랜스포메이션 연산을 수행하지 않는 것입니다. 이는 RDD의 특성 중 하나인 ‘회복력’과 관련있습니다. 액션 연산이 수행되기 전까지 동작이 지연이 되는데, 대신에 RDD가 생성되는 방법을 기억하는 것입니다. 따라서 문제가 발생하더라도 기존에 RDD가 생성되는 방법을 기억하여 연산 수행에 문제가 없도록 하는 것입니다. 이는 위의 예시에서 reduceByKey까지는 실제로 트랜스포메이션 연산을 수행하는 것이 아니라 해당 연산을 순서대로 기억해놨다가, saveAsFile연산이 수행될 때(액션 연산이 수행될 때) 비로소 트랜스포메이션 연산도 수행된 것입니다. 지연 동작 방식의 큰 장점은 실행계획의 최적화입니다. RDD의 불변성 오류로 인해 스파크의 데이터가 일부 유실되면, 데이터를 다시 만들어내는 방식으로 복구되는 것이 RDD의 불변성입니다. 이는 위에서 계속 언급한 “회복력”과 관련됩니다. RDD는 RDD1-&amp;gt;RDD2-&amp;gt; … 가 되면서 한번 만들어진 RDD는 내용이 변경되지 않습니다. RDD를 만드는 방법을 기억해서 문제가 발생 시 언제든지 똑같은 데이터를 생성할 수 있습니다. 5. 파티션과 HDFS RDD데이터는 클러스터를 구성하는 여러 서버에 나뉘어서 저장됨 이 때, 분할된 데이터를 파티션 단위로 관리합니다. HDFS는 하둡의 파일 시스템(hadoop distributed file system) 스파크는 하둡 파일 입출력 API에 의존성을 가지고 있음. 6. Job, Executor, 드라이버 프로그램 Job : 스파크 프로그램 실행하는 것 = 스파크 잡(job)을 실행하는 것 하나의 잡은 클러스터에서 병렬로 처리됨 이 때, 클러스터를 구성하는 각 서버마다 executor라는 프로세스가 생성 각 executor는 할당된 파티션 데이터를 처리함 드라이버란 ? 스파크에서 잡을 실행하는 프로그램으로, 메인함수를 가지고 있는 프로그램 드라이버에서 스파크 컨테스트를 생성하고 그 인스턴스를 포함하고 있는 프로그램 스파크컨테스트를 생성해 클러스터의 각 워커 노드들에게 작업을 지시하고 결과를 취합하는 역할을 수행 아래 코드를 보면, main함수 안에 sparkcontext를 생성하고 sc라는 인스턴스를 포함하고 있는 것을 볼 수 있음. 즉, main함수를 가지고 있는 프로그램이 ‘드라이버’에 해당됨 Public static void main(String[] args){ ... JavaSparkContext s c = getSparkContext(&quot;WordCount&quot;, args[0]); ...} 7. 함수의 전달 스파크는 함수형 프로그래밍 언어인 스칼라로 작성되어 “함수”를 다른 함수의 “매개변수”로서 전달 가능 아래 예제(Scala)를 보면 map의 인자에 ‘_+1’이 전달되는데, 익명 함수로 전달되는 것임 val rdd1 = sc.paralleize(1 to 10) val rdd2 = rdd1.map(_+1) 파이썬으로 작성하면 아래와 같이, lambda 함수가 매개변수로 들어가게 됨 rdd1.map(lambda v:v+1) [참고]함수형 프로그래밍 함수형 프로그래밍과 객체 지향 프로그래밍의 차이를 통해 이해해보겠습니다. 객체 지향 프로그래밍은 객체 안에 상태를 저장하고, 해당 상태를 이용해서 제공할 수 있는(메소드)를 추가하고 상태변화를 ‘누가 어디까지 볼 수 있게 할지’를 설정하고 조정합니다. 따라서 적절한 상태 변경이 되도록 구성합니다. 반면에 함수형 프로그래밍은 상태 변경을 피하며 함수 간의 데이터 흐름을 사용합니다. 입력은 여러 함수들을 통해 흘러 다니게 됩니다. 따라서, 함수의 인자로 함수가 들어오고 반환의 결과로도 함수가 나올 수 있습니다. 함수 전달 시 유의할 점 Class PassingFunctionSample{ val count=1 def add(I: int):Int={ count+i } def runMapSample(sc:SparkContext){ val rdd1 = sc.parallelize(1 to 10); val rdd2 = rdd1.map(add)} } 위와 같이 코드를 작성해서 실행하면, ‘java.io.NotSerializaionException’이라는 오류가 발생합니다. 이는 전달된 add함수가 클러스터를 구성하는 각 서버에서 동작할 수 있도록 전달되어야 하는데, 전달이 안되기 때문입니다. 그 이유는 add함수는 PassingFunctionSample의 메소드로 결국 클래스 PassingFunctionSample이 전체 다 전달되기 때문입니다. 해당 클래스는 Serializable 인터페이스를 구현하지 않습니다. 즉, 클래스가 각 서버에 전달될 수 있는 기능을 가지고 있지 않는 것입니다. 함수만 따로 전달되어야 하는 것입니다. 스칼라 같은 경우 ‘싱글톤 객체’를 이용하여 해결 할 수 있습니다. 파이썬의 예제도 살펴보면, 아래는 클래스 전체가 전달되는 잘못된 예입니다. class PassingFunctionSample(): def add1(self, i): return i + 1 def runMapSample1(self, sc): rdd1 = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) rdd2 = rdd1.map(self.add1) # rdd2 = rdd1.map(add2) print(&quot;, &quot;.join(str(i) for i in rdd2.collect())) self로 인해 전체 클래스가 전달됩니다.(파이썬은 예외없이 실행되므로 유의할 것!) class PassingFunctionSample(): @staticmethod def add1(self, i): return i + 1 def runMapSample1(self, sc): rdd1 = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) rdd2 = rdd1.map(add2) print(&quot;, &quot;.join(str(i) for i in rdd2.collect())) if __name__ == &quot;__main__&quot;: def add2(i): return i + 1 conf = SparkConf() sc = SparkContext(master=&quot;local[*]&quot;, appName=&quot;PassingFunctionSample&quot;, conf=conf) obj = PassingFunctionSample() obj.runMapSample1(sc) 위와 같이 함수 add2가 독립적으로(클래스 전체가) 전달될 수 있도록 해야합니다. 변수 전달 시 유의할 점 class PassingFunctionSample { var increment = 1 def runMapSample3(sc: SparkContext) { val rdd1 = sc.parallelize(1 to 10) val rdd2 = rdd1.map(_ + increment) \\익명함수 전달 print(rdd2.collect.toList) } def runMapSample4(sc: SparkContext) { val rdd1 = sc.parallelize(1 to 10) val localIncrement = increment val rdd2 = rdd1.map(_ + localIncrement) print(rdd2.collect().toList) } } runMapSample3 처럼 변수가 직접 전달되면 안되고, runMapSample4처럼 지역변수로 변환해서 전달해야 합니다. 그래야 나중에 변수가 변경되어 생기는 문제를 방지할 수 있습니다. 데이터 타입에 따른 RDD 연산 RDD 연산 함수에서 인자 타입을 보고 적절하게 맞는 연산 함수를 사용해야 합니다. 이상으로 &amp;lt;RDD, Resilient Distributed DataSet에 대하여[1]&amp;gt; 마치겠습니다. 다음 포스팅에서 이어가도록 하겠습니다. 함수형 언어, https://sungjk.github.io/2017/07/17/fp.html, https://docs.python.org/ko/3/howto/functional.html</summary></entry></feed>