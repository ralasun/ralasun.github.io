<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Blog &middot; Ralasun Resarch Blog
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/main.css">
  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://raw.githubusercontent.com/ralasun/ralasun.github.io/master/public/favicon.png">
  <link rel="shortcut icon" href="https://raw.githubusercontent.com/ralasun/ralasun.github.io/master/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  
  <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_SVG"> </script>
  <script type="text/x-mathjax-config">
MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true } });
  </script>
  
</head>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <div class="sidebar-personal-info">
      <div class="sidebar-personal-info-section">
        <img src="http://localhost:4000/public/img/profile_v1.jpg"/>
      </div>
      <div class="sidebar-personal-info-section">
        <p><strong>한걸음씩</strong>, 그리고 <strong>꾸준히</strong> 나아가기</p>
      </div>
      
      
      
      <div class="sidebar-personal-info-section">
        <p> Follow me: 
        
        
        
        <a href="https://www.linkedin.com/in/ralasun">
          <i class="fa fa-linkedin" aria-hidden="true"></i>
        </a>
        
        |
        
        
        
        <a href="https://github.com/ralasun">
          <i class="fa fa-github" aria-hidden="true"></i>
        </a>
        
        |
        
        
        
        <a href="mailto:sunhwalsh91@gmail.com">
          <i class="fa fa-envelope" aria-hidden="true"></i>
        </a>
        
        
        
        </p>
      </div>
      
    </div>
  </div>

  <nav class="sidebar-nav">
    
      
      
      

      

      <span class="">
        <a class="sidebar-nav-item " href="/">
          Home
        </a>

        
      </span>

    
      
      
      

      

      <span class="foldable">
        <a class="sidebar-nav-item active" href="/blog/">
          Blog
        </a>

        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/categories/">
                Posts
              </a>
          
        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/tags/">
                Tags
              </a>
          
        
      </span>

    
      
      
      

      

      <span class="">
        <a class="sidebar-nav-item " href="/about/">
          About
        </a>

        
      </span>

    
  </nav>

  <div class="sidebar-item">
    <p>
    &copy; 2021 Seonhwa Lee. This work is liscensed under <a href="http://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0</a>.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/" title="Home" title="Ralasun Resarch Blog">
              <img class="masthead-logo" width="200" height="30" src="http://localhost:4000/public/logo.jpg"/>
            </a>
            <small>research blog for data science</small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="/deep%20learning/2021/03/06/gcn(2)/">
        Graph Convolutional Network에 대하여 - Spectral Graph Convolution(2)
      </a>
    </h1>

    <span class="post-date">06 Mar 2021</span>
     | 
    
    <a href="/blog/tags/#graph-neural-network" class="post-tag">graph-neural-network</a>
    
    

    <article>
      <p>gcn(2)</p>

    <article>
    <div class="post-more">
      
      <a href="/deep%20learning/2021/03/06/gcn(2)/#disqus_thread"> <i class="fa fa-comments" aria-hidden="true"></i>Comment</a>&nbsp;
      
      <a href="/deep%20learning/2021/03/06/gcn(2)/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/deep%20learning/2021/02/15/gcn/">
        Graph Convolutional Network에 대하여 - Spectral Graph Convolution
      </a>
    </h1>

    <span class="post-date">15 Feb 2021</span>
     | 
    
    <a href="/blog/tags/#graph-neural-network" class="post-tag">graph-neural-network</a>
    
    

    <article>
      <p>지난 GNN 포스팅&lt;<a href="https://ralasun.github.io/deep%20learning/2021/02/11/gcn/">Introduction to Graph Neural Network - GNN 소개 및 개념</a>&gt;에서 graph neural network의 전반적인 개념에 대해 소개하였습니다. 이번 포스팅은 graph neural network가 더욱 유명해진 계기가 된 <a href="https://arxiv.org/abs/1609.02907">Kipf. et, al.의 Graph Convolutional Neural Network</a>에 대해 다루도록 하겠습니다.</p>

<p>Kipf. et al.의 GCN을 이해하기 위해서는 먼저, spectral graph convolution에서부터 시작해야 합니다. 그러나 :spectral” 이라는 부분이 생소하신 분들이 많을 거라 생각됩니다. 반면에 일반적인 CNN 동작 방식은 많이 알려져 있습니다. 일반적인 CNN 동작은 spatial convolution입니다. 따라서 이를 유사하게 graph에 적용하는 방식을 spatial graph convolution입니다. 따라서, 이번 포스팅에서는 spectral 방식과 spatial 방식을 비교하고, spectral graph convolution에 대해 자세히 설명한 뒤에 Kipf. et al의 Graph Convolutional Network에 대해 다루도록 하겠습니다.</p>

<hr />

<h1>Spatial Graph Convolution vs. <br />Spectral Graph Convolution</h1>

<p>Graph convolution은 크게 2가지 방법이 있습니다. Spatial graph convolution과 Spectral graph convolution입니다. Spatial graph convolution은 convolution 연산을 graph위에서 직접 수행하는 방식으로, 각 노드와 가깝게 연결된 이웃 노드들에 한해서 convolution 연산을 수행합니다. 즉, 노드와 이웃노드들을 특정 grid form으로 재배열하여 convolution 연산을 수행하는 것입니다. 그러나, 우리가 일반적으로 아는 CNN의 filter는 고정된 사이즈를 가집니다(그림 1.).</p>

<p align="center"><img src="https://i.stack.imgur.com/S5B1k.png" /><figcaption align="center">그림 1. CNN operation with fixed-size filter(3x3)</figcaption></p>

<p>따라서, <b><i>spatial graph convolution 방식의 관건은 고정된 크기의 이웃 노드를 선택하는 것입니다.</i></b> 뿐만 아니라, CNN의 특징 중 하나는 “local invariance” 입니다. 입력의 위치가 바뀌어도 출력은 동일함을 의미합니다. 즉, 이미지 내의 강아지 위치가 달라도 CNN은 강아지라는 아웃풋을 출력함을 의미합니다.</p>

<p align="center"><img src="https://miro.medium.com/max/1400/1*HUJ3-xs3nUv-wY_GTBVUMg.png" /><figcaption align="center">그림 2. Local invariance</figcaption></p>

<p>따라서, <b><i>Spatial graph convolution의 또다른 관건은 바로 “local invariance”를 유지를 해야한다는 것입니다.</i></b></p>

<p>앞에서 언급한 spatial graph convolution이 다뤄야 할 문제점과 별개로 또다른 문제점이 존재합니다. <b>Spatial graph convolution은 고정된 이웃 노드에서만 정보는 받아서 노드의 정보를 업데이트를 한다는 점입니다.</b></p>
<p align="center"><img width="300" src="https://imgur.com/KWmqbgk.png" /><figcaption align="center">그림 3. Select neighborhood of red node</figcaption></p>

<p>그러나, 그래프에서의 한 노드의 정보는 시간에 따라 여러 노드들의 정보의 혼합으로 표현될 수 있습니다. &lt;그림 4.&gt;를 살펴보도록 하겠습니다. 1번노드의 t=0일 때 정보는 [1,-1] 이지만 시간에 따라 여러 노드들의 정보(노드들의 signal)들이 밀려 들어오게 됩니다. 즉, 고정된 이웃노드 말고도 멀리 연결되어 있는 노드의 정보도 시간이 흐르면서 밀려 들어올 수 있는 것입니다. 이를 노드 간 message passing이라 합니다.</p>

<p align="center"><img src="https://imgur.com/Fv2FJbC.png" /><figcaption align="center">그림 4. Message Passing in graph</figcaption>
</p>

<p>즉, 한 노드의 정보는 여러 노드의 signal이 혼재해 있는 것으로, 이를 time domain이 아닌 frequency 도메인으로 분석한다면, 한 노드 내에 혼재된 signal들을 여러 signal의 요소로 나눠서 node의 특징을 더 잘 추출할 수 있습니다. 이것에 관한 것이 바로 “Spectral Graph Convolution”입니다. Spectral graph convolution은 spectral 영역에서 convolution을 수행하는 것입니다. 이에 대해 자세히 살펴보도록 하겠습니다.</p>

<h1>Dive into Spectral Graph Convolution</h1>

<p>Signal Processing 분야에서 “spectral analysis”라는 것은 이미지/음성/그래프 신호(signal)을 time/spatial domain이 아니라 frequency domain으로 바꿔서 분석을 진행하는 것입니다. 즉, <em>어떤 특정 신호를 단순한 요소의 합으로 분해하는 것</em>을 의미합니다. 대표적으로 이를 수행할 수 있는 방법이 푸리에 변환(Fourier Transform)입니다.</p>

<blockquote>
  <p>spectral analysis에서 입력 신호가 전파/음성신호면 time domain을 frequency domain으로 변환하는 것이고, 컴퓨터 비전/그래프/영상처리 분야이면 spatial domain을 frequency domain으로 변환하는 것입니다.</p>
</blockquote>

<p>푸리에 변환이란, <span style="text-decoration: underline"><b>임의의 입력 신호를 다양한 주파수를 갖는 주기함수들의 합으로 분해하여 표현</b></span>하는 것입니다. 아래 그림처럼 빨간색 신호를 파란색의 주기함수들의 성분으로 나누는 작업이 바로 푸리에 변환입니다. 즉, 파란색 주기함수들을 합하면 결국 빨간색 신호가 되는 것입니다.</p>

<p align="center"><img src="https://t1.daumcdn.net/cfile/tistory/9967FA3359B63D8122" /><figcaption align="center">그림 5. 푸리에 변환</figcaption></p>

<p>그렇다면 graph signal에서의 푸리에 변환은 어떤 걸까요 ?</p>

<p>결론부터 얘기하면, <span style="text-decoration: underline"><b>graph signal의 푸리에 변환은 graph의 Laplacian matrix를 eigen-decomposition하는 것</b></span>입니다. 아래에서 수식과 함께 자세히 살펴보도록 하겠습니다.</p>

<h3>Fourier transform</h3>

<p>먼저, 푸리에 변환 식에 대해서 살펴봅시다. <span style="color:gray"><del>저도 푸리에 변환에 대한 이해가 아직 한없이 부족합니다. 최대한 공부하고 이해한 내용을 풀어볼려고 노력하였습니다.</del></span></p>

<script type="math/tex; mode=display">\hat{f}(\xi) = \int_{\mathbf{R}^d} f(x)e^{2\pi ix\xi} \,dx \tag{1}</script>

<script type="math/tex; mode=display">f(x) = \int_{\mathbf{R}^d} \hat{f}(\xi) e^{-2\pi ix\xi} \,d\xi \tag{2}</script>

<p>(1)은 f의 푸리에 변환이고, (2)는 푸리에 역변환입니다. 푸리에 변환은 위에서 설명드린 것처럼, time domain을 frequency domain으로 변환한 것으로, 다양한 주파수를 갖는 주기함수의 합입니다. 그렇다면, 푸리에 역변환은 frequency domain의 함수를 다시 time domain으로 변환하는 것입니다. 푸리에 변환을 바라보는 관점은 여러가지가 존재하지만 그 중 하나는 ‘내적’입니다.</p>

<p align="center">"임의의 주파수 $f(x)$ 에 대하여, $\hat{f}(\xi)$ 는 $f(x)$ 와 $e^{-2\pi ix\xi}$ 의 내적"</p>

<p>‘내적’이 내포하고 있는 의미는 유사도입니다. 즉, “a와 b의 내적은 a와 b가 얼마나 닮았는가”를 뜻합니다. 결국 푸리에 변환은 다시 풀어쓰면 아래와 같은 의미를 가지고 있습니다.</p>

<p align="center">"임의의 주파수 $f(x)$ 에 대하여, $\hat{f}(\xi)$ 는 $f(x)$ 와 $e^{-2\pi ix\xi}$ 가 얼마나 닮았는가"</p>

<p>그렇다면, $e^{-2\pi ix\xi}$ 의 의미는 무엇일까요 ? 이를 이해하기 위해선 ‘오일러 공식’이 필요합니다. 오일러 공식은 복소지수함수(complext exponential function)를 삼각함수(trigonometric function)로 표현하는 유명한 식입니다.</p>

<script type="math/tex; mode=display">e^{ix} = cost + isinx \tag{3}</script>

<p>따라서, 오일러 공식에 의해 (1)식의 $e^{2\pi ix\xi}$ 부분을 cos요소와 sin요소의 합으로 표현할 수 있습니다.</p>

<script type="math/tex; mode=display">e^{2\pi ix\xi} = cos(2\pi x\xi) + i sin(2\pi x\xi) \tag{4}</script>

<p>즉, 주어진 주파수 f(x)에 대해 cosine에서 유사한 정도와 sine과 유사한 정도의 합이 푸리에 변환이라고 생각할 수 있습니다.</p>

<p>이번엔 푸리에 변환의 선형대수(linear algebra)적인 의미를 살펴보도록 하겠습니다. 선형 대수에서, 벡터 $a \in R^d$ 를 d차원의 orthonormal basis를 찾을 수 있다면, 벡터 $a$ 를 orhonormal basis의 선형결합으로 표현할 수 있습니다. 이 orthonormal basis를 찾는 방법 중 하나가 바로 Eigen-value decomposition 입니다.</p>

<blockquote>
  <p>orthonormal이란 서로 직교하면서 길이가 1인 벡터들을 의미합니다. 또한, 모든 matrix에 대해서 eigen-value decomposition 결과로 찾은 basis가 orthonormal은 아닙니다. 하지만 real-symmetric matrix에 대하여 구한 eigenvector들은 orthgonal한 관계입니다.</p>
</blockquote>

<p>다시 돌아와서, 푸리에 변환에서 주기함수 요소인 sine과 cosine에 대해 살펴봅시다. 아래와 같이 sine과 sine, sine과 cosine, cosine과 cosine을 내적하면 모두 다 0이 나옵니다. 이는 즉 삼각함수는 직교함을 알 수 있습니다(삼각함수의 직교성).</p>

<p align="center"><img src="https://imgur.com/Bdo17jG.png" /><figcaption align="center">그림 6. 삼각함수의 직교성</figcaption></p>

<p>그렇다면, 선형대수 관점에서, <span style="text-decoration: underline">sine과 cosine 기저들의 선형결합이 즉 푸리에 변환이 되는 것</span>입니다. 즉, <span style="text-decoration:underline; color:red">어떤 특정 graph signal에 관한 행렬이 존재하고 이 행렬이 real-symmetric matrix이며 이들의 eigenvectors를 구할 수 있다면, eigenvector의 선형결합이 graph signal의 푸리에 변환</span>임을 의미하는 것입니다.</p>

<h3>Laplacian(Laplace Operator)</h3>

<p>Graph laplacian을 보기 전에 Laplace Operator에 대해 살펴보도록 하겠습니다. Laplace operator는 differential operator로, 벡터 기울기의 발산(Divergence)을 의미합니다.</p>

<script type="math/tex; mode=display">\triangle f= \triangledown \cdot \triangledown f = \triangledown^2 f \tag{5}</script>

<p>$\triangledown f$ 는 $f$ 의 기울기를 의미하는 것으로 1차함수의 기울기처럼, 한 점에서의 변화하는 정도를 의미합니다. 이를 그림으로 나타나면 아래와 같습니다.</p>

<p align="center"><img src="https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2019-08-28_laplacian/noname01.png" /><figcaption align="center">그림 7. Scalar 함수</figcaption></p>

<p>&lt;그림 7.&gt;의 scalar 함수의 gradient는 아래와 같습니다.</p>

<p align="center"><img src="https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2019-08-28_laplacian/noname03.png" /><figcaption align="center">그림 8. Scalar 함수의 gradient</figcaption></p>

<p>&lt;그림 7.&gt;과 &lt;그림 8.&gt;을 보시면, (x,y)=(0,2) 부근에는 수렴하는 형태의 gradient가 형성되어 있고, (x,y)=(0,-2) 부근에는 발산하는 형태의 gradient가 형성되어 있습니다. Laplace 연산자를 이용해서 이 기울기의 발산 $\triangle f$ 을 구해주면, 아래와 같습니다.</p>

<p align="center"><img src="https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2019-08-28_laplacian/noname04.png" /><figcaption align="center">그림 9. Divergence</figcaption></p>

<p>divergence가 나타내는 의미, 즉, Laplace operator가 나타내는 의미는 무엇일까요 ? 이 함수의 높고 낮음을 표시하는 것입니다. 고등학교 때, 배운 2차 편미분과 비슷합니다. 이차 편미분 값이 양수면 아래로 볼록이고, 이차 편미분 값이 음수면 위로 볼록인 것과 유사합니다. 즉, 노란 부분일수록 양수 이기때문에 위로볼록인 모양이고, 파란부분일수록 음수값이기 때문에 아래로 볼록입니다.</p>

<p>그렇다면, graph signal 영역에서 Laplace operator가 갖는 의미가 무엇일까요 ? graph signal 영역에서 Laplace operator를 적용한다는 건, 한 노드에서의 signal의 흩어짐 정도, 즉, 흐르는 정도를 알 수 있습니다. <span styple="text-decoration:underline; color:red">특정 노드에서 signal이 들어왔을 때 그 signal이 특정 노드와 연결된 노드들로 각각 얼마만큼 빠르게 흩어지는지를 알 수 있고 이는 즉 그 노드의 특징이 될 수 있는 것입니다.</span> 위의 그림을 예를 들어서 설명한다면, 만약에 한 signal이 그림 7.의 가장 높은 부분(노란색 부분)에서 시작된다면 가장 낮은 부분(파란색 부분)까지 빠른 속도로 흘러갈 것입니다.</p>

<blockquote>
  <p>빨간색으로 강조한 부분이 왜 laplacian matrix의 eigen-value decomposition이 fourier transform과 연결되는지에 관한 부분입니다. 포스팅을 쭉 끝까지 읽어보시면 아하! 하면서 이해가 되실겁니다.</p>
</blockquote>

<p>아래는 이와 관련된 gif이미지입니다. Grid 격자를 어떤 graph라고 생각한다면, 어떤 노드에서 signal이 들어왔을 때 흩어지는 양상을 보실 수 있습니다. 이 흩어지는 양상을 자세히 알기 위해서는 laplcian operator를 이용하여 계산하면 됩니다.</p>

<p align="center"><img src="https://miro.medium.com/max/1120/1*gz2hyrcSSJG9MtDzmQLe3w.gif" /><figcaption align="center">그림 9. Diffusion of some signal in a regular grid graph based on the graph Laplacian</figcaption></p>

<h3>Graph Laplacian</h3>

<p>그러나, 이제까지 설명한 laplacian operator는 지난 포스팅에서 언급한 Laplacian matrix랑 무슨 관련이 있는 걸까요? 이름이 비슷한 걸 보니, laplacian matrix도 어떤 differential operator, 즉 ‘변화’에 관한 행렬임을 짐작할 수 있습니다.</p>

<script type="math/tex; mode=display">\triangle f = \triangledown^2 f =\sum_{i=1}^{n}\frac{\partial^2 f}{\partial {x_i}^2} \tag{6}</script>

<p>1차원에서의 laplacian operator는 <a href="https://en.wikipedia.org/wiki/Second_derivative">이차도함수 극한 정의</a>와 동일합니다.</p>

<script type="math/tex; mode=display">\triangledown^2 f = \lim_{h \rightarrow 0} \frac{f(x+h) - 2f(x) - f(x-h)}{h^2} \tag{7}</script>

<p>그렇다면 위 laplacian operator가 graph 상에서 적용되면 어떻게 될까요 ? 위 식 (7)은 아래 그림과 같이 표현될 수 있습니다. 즉, x노드의 signal f(x)와 x노드와 h사이 만큼 떨어진 이웃노드의 signal f(x+h), f(x-h) 와의 변화량을 통해 x노드의 signal 특징을 구한 것입니다.이것이 바로 x노드 signal에 laplacian operator를 적용한 거라고 생각될 수 있습니다. 따라서, 한 노드 의 특징은 해당 노드와 연결된 이웃노드와의 관계라는 관점에서 표현될 수 있고, 즉 이 표현을 위해 이웃 노드와의 차이(즉, 변화)를 이용한 것이 laplacian operator가 됩니다.</p>

<p align="center"><img width="400" src="https://imgur.com/BYvjBKt.png" /><figcaption align="center">그림 10. discretized laplacian operator</figcaption></p>

<p>또한 graph laplacian은 연속적인 성질(continuous)이 아닌 이산적인 성질(discrete)을 띕니다. 이제까지 언급한 특징을 정리되면 아래의 graph laplacian 식이 이해가 되실 것입니다. 아래는 $v_i$ 노드에서 graph laplacian operator를 적용한 것입니다.</p>

<script type="math/tex; mode=display">\triangle f(v_i) = Lf|_{v_i} = \sum_{v_j ~ v_i}[f(v_i) - f(v_j)] \tag{8}</script>

<p>weighted undirected graph인 경우, weighted undirected graph는 노드간 엣지에 가중치가 있는 경우입니다. graph laplacian operator는 아래와 같습니다.</p>

<p align="center"><img width="400" src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSRUOW9mJ2mNgmHDw7_q7uiK2m1slCZgiah1Q&amp;usqp=CAU.png" /><figcaption align="center">그림 11. weighted undirected graph</figcaption></p>

<script type="math/tex; mode=display">\triangle f(v_i) = \sum_{j \in N_i}W_{ij}[f(v_i) - f(v_j)] \tag{9}</script>

<p>여기서 $v_i$ 는 특정노드를 가르키는 인덱스이고, $f(v_i)$ 는 각 노드의 signal 입니다. 즉, 함수 $f \in R^N$ 는 각 노드의 특성을 signal로 맵핑해주는 함수라고 생각하시면 됩니다.</p>

<p>Graph laplacian 예를 들어봅시다. 아래와 같은 간단한 graph가 있을 때, 각 노드에 대한 laplacian operator는 아래와 같습니다.</p>

<p align="center"><img width="400" src="https://imgur.com/DYfJGS9.png" /><figcaption align="center">그림 12. Graph laplacian example</figcaption></p>

<script type="math/tex; mode=display">\triangle f(v_1) = 2f(v_1) - f(v_2) - f(v_3)</script>

<script type="math/tex; mode=display">\triangle f(v_2) = 3f(v_2) - f(v_1) - f(v_3) - f(v_4)</script>

<script type="math/tex; mode=display">\triangle f(v_3) = 2f(v_3) - f(v_1) - f(v_2)</script>

<script type="math/tex; mode=display">\triangle f(v_4) = f(v_4) - f(v_2)</script>

<p>이를 행렬로 표현하면 아래와 같습니다.</p>

<script type="math/tex; mode=display">% <![CDATA[
M = \begin{bmatrix}
       2 & -1 & -1 & 0 \\
       -1 & 3 & -1 & -1 \\
       0 & -1 & 0 & 1\end{bmatrix} 
       \begin{bmatrix} f(v_1) \\ f(v_2) \\ f(v_3) \\ f(v_4)\end{bmatrix} \tag{10} %]]></script>

<p>위 식 (10) 의 앞부분 행렬은 지난 GNN 포스팅&lt;<a href="https://ralasun.github.io/deep%20learning/2021/02/11/gcn/">Introduction to Graph Neural Network - GNN 소개 및 개념</a>&gt; 에서 소개한 Laplacian matrix 입니다. 즉, <span style="text-decoration:underline">laplacian matrix란 graph representation 중에서 이웃 노드와의 변화 흐름을 통해 노드의 특징을 나타내는 그래프 표현이라고 생각할 수 있습니다.</span></p>

<p>이웃 노드와의 차이(변화, variation)는 결국 노드 간의 매끄러움 정도(smoothness)를 의미합니다. 한 노드가 이웃 노드와의 차이가 작다는 건 그 노드는 이웃 노드와 특성이 비슷한 경우이고, 이를 ‘매끄럽다’라고 생각할 수 있습니다. 반면에, 이웃 노드와의 차이가 크다는 건 그 노드는 이웃 노드와 특성이 상이하다는 것이고 이는 ‘매끄럽지 않다’라고 생각할 수 있습니다. 따라서 Laplacian Matrix는 graph의 smoothness와 관련이 있습니다.</p>

<p>이를 ‘신호’라는 관점에서 다시 생각해봅시다. 어떤 한 노드 내에서 흐르는 신호는 크게 2가지로 나눈다면, 나와 비슷한 특성을 가진 이웃노드에서 들어오는 신호와 나와 상이한 특성을 가진 이웃노드에서 들어오는 신호로 나눌 수 있습니다. 즉, <span style="text-decoration:underline; color:red">한 노드 내에 혼잡해 있는 신호는 나와 유사한 특성을 가진 노드에서 오는 신호와 나와 상이한 특성을 가진 노드에서 오는 신호의 결합으로 생각할 수 있습니다.</span> 이는 결국 푸리에 변환과 관련된 개념입니다. 그리고 나와 유사한 속성의 노드와 상이한 속성의 노드를 나눌 수 있는 것에 관한 정보가 바로 Laplacian matrix에 담겨져 있는 것입니다. 따라서, <span style="text-decoration:underline; color:red">lapalcian matrix를 이용한 푸리에 변환이 바로 “Graph Fourier Transform” 이며, 이는 위에서 언급한 “eigen-value decomposition”과 관련이 있는 것입니다.</span></p>

<h3>Graph Fourier Transform</h3>

<p>그렇다면, graph fourier transform이 왜 Laplacian matrix를 eigen-decomposition을 하는지에 대한 궁금증이 여기서 생깁니다. 이 부분을 이제 짚고 넘어가겠습니다. 이 부분까지 이해가 되신다면, 이 이후에 진행할 spectral graph convolution을 제대로 이해하실 수 있으실 것입니다.</p>

<p>위에서 특정 노드와 유사한 노드 집단과 상이한 노드집단을 나눌 수 있다면 특정 노드에 혼잡해 있는 신호를 여러 특성의 신호로 분해할 수 있다고 하였습니다. 그렇다면 먼저 이 집단을 구별할까요 ? 아래 식을 살펴봅시다. 아래 식은 결국, 노드 간의 차이의 합입니다.</p>

<script type="math/tex; mode=display">S = \sum_{(i,j) \in \epsilon}W_{ij}[f(i) - f(j)]^2 = \mathbf {f^{\intercal}Lf} \tag{11}</script>

<p>이는 결국 graph의 smoothness와 관련된 것이며, Laplacian quadratic form으로 표현가능합니다. 위의 식을 최소화하게 하는 $\mathbf f$ 를 찾는다면, 특정 노드와 특성이 유사한 노드 집단과 상이한 노드 집단을 구분할 수 있습니다.</p>

<script type="math/tex; mode=display">min_{f \in R^N, \,||f||_2 = 1} \mathbf {f^{\intercal}Lf} \tag{12}</script>

<p>Lagrange 방정식에 의해 최적화 방정식으로 바꿀 수 있습니다.</p>

<script type="math/tex; mode=display">L = \mathbf {f^{\intercal}Lf} - \lambda(\mathbf {f^{\intercal}f} - 1) \tag{13}</script>

<p>위 식을 최소가 되게 하기 위한 $\mathbf f$ 를 찾기 위해 $\mathbf f$ 로 미분하면 아래와 같습니다.</p>

<script type="math/tex; mode=display">\frac{\partial L}{\partial f} = 2\mathbf {Lf} - 2\lambda \mathbf f = 0 \tag{14}</script>

<p>따라서, 최소가 되는 $\mathbf f$ 는 아래 식을 만족해야 합니다.</p>

<script type="math/tex; mode=display">\mathbf {Lf} = \lambda \mathbf f \tag{15}</script>

<p>위의 식은 laplacian 행렬의 eigen-value decomposition입니다. laplacian matrix의 eigenvector eigen vector들이 특정 노드와 유사한 노드집단과 상이한 노드집단을 구분하는 기준이 되고, 특히 작은 값의 eigenvalue의 대응하는 eigenvector일수록 graph를 더욱 smooth하게 나누는 기준이 됩니다.</p>

<p>laplacian matrix의 eigenvector의 예를 들어봅시다. 아래 그림은 사람의 behavior를 graph으로 표현했을 때, 해당 graph의 laplacian 행렬의 eigenvector $u_2, u_3, u_4, u_8, \,\,, (0 &lt; \lambda_2 \leq \lambda_3 \leq \lambda_4 \leq \lambda_8)$ 에 graph 노드를 임베딩한 것입니다. 즉, 그래프 노드 $f(v_i)$ 라면, $u^{\intercal}f(v_i)$ 를 계산한 겁니다.</p>

<p align="center"><img width="400" src="https://imgur.com/JWVlsGQ.png" /><figcaption align="center">그림 12. Projection on eigenvector of Laplacian Matrix with each graph node</figcaption></p>

<p>사람의 행동을 graph로 표현했을 때, 위의 그림처럼 가까이에 있는 부분끼리 이웃노드에 해당될 가능성이 높습니다. 머리 쪽과 팔쪽은 가까우니깐 이웃노드일 가능성이 높고, 머리와 다리 쪽은 상이한 노드일 가능성이 높습니다. 따라서, eigenvalue가 가장 작은 eigenvector $u_2$ 에 임베딩했을 때, 해당 graph의 노드들을 이웃노드 집단과 상이한 노드 집단으로 잘 분리되는 것을 보실 수 있습니다.</p>

<p>이를 다시 돌아와서 fourier transform 관점에서 해석해봅시다. <span style="text-decoration:underline; color:red">graph의 특정 노드 signal을 $f(v_i)$ 를 작은 값의 eigenvalue에 대응되는 eigenvector에 사영시키면, 혼재되어 있는 signal 중 가까운 이웃노드에서 들어오는 signal의 성분을 추출한 것으로 볼 수 있고, 큰 값의 eigenvalue에 대응되는 eigenvector에 사영시키면, 혼재되어 있는 signal 중 멀리 있는 상이한 노드에서 들어오는 signal의 성분을 추출한 것입니다.</span> 따라서, 혼재되어 있는 graph signal을 eigenvector의 선형결합으로 표현하여, 여러 집단(가장 유사한 집단 &lt; … &lt; 매우 상이한 집단)에서 들어오는 signal의 합으로 표현할 수 있습니다.</p>

<p>또한 위에서, fourier transform은  혼합 signal을 sine과 cosine의 주파수의 성분으로 쪼개어 이 성분들의 선형결합이라고 하였습니다. 그리고, 이 때 주파수의 성분은 삼각함수의 직교성으로 인해, 직교기저를 이룬다고 하였습니다. 마찬가지로, laplacian matrix은 real-symmetric 행렬이어서 eigenvector들이 직교기저를 이룹니다. 즉, graph signal을 laplacian eigenvector 행렬에 사영시키면, graph signal을 laplacian의 직교 기저들의 성분으로 분해하여 이를 합한 선형결합에 해당됩니다.</p>

<p>이제까지 설명한 것을 토대로 Graph Fourier Transform은 바로 Laplacian 행렬의 Eigen-value decomposition과 관련이 있게 되는 것입니다. 수식으로도 왜 그렇게 되는지 알 수 있는데, 이는 다음과 같습니다.</p>

<p>Fourier transform의 식을 다시 쓰면 아래와 같습니다.</p>

<script type="math/tex; mode=display">% <![CDATA[
\hat{f}(\xi) = <f, e^{2\pi ix\xi}>\int_{\mathbf{R}^d} f(x)e^{2\pi ix\xi} \,dx \tag{16} %]]></script>

<p>주파수 성분은 $e^{2\pi ix\xi}$ 이며, 여기에 laplace operator를 적용하면 아래와 같습니다.</p>

<script type="math/tex; mode=display">Lf = -\triangle(e^{2\pi ix\xi}) = -\frac{\partial^2}{\partial x^2}e^{2\pi ix\xi} = (2\pi \xi)^2 e^{2\pi ix\xi} = \lambda f \tag{17}</script>

<p>즉, 주파수 성분 $e^{2\pi ix\xi}$ 도 laplace operator의 eigen-function라고 볼 수 있습니다.</p>

<blockquote>
  <p>1차원 같은 경우, eigen-function이라 한 이유는 eigen-vector는 2차원 이상일 때의 eigen-function이라고 볼 수 있기 때문입니다.</p>
</blockquote>

<p>이제까지는 왜 Graph Fourier Transform이 Laplacian 행렬이 eigen-value decomposition과 관련 있는지 개념적인 이유와 수식적인 이유에 대해서 살펴봤습니다. 그러면 마지막으로, Graph Fourier Transform의 수식을 정리해보도록 하겠습니다.</p>

<p>Laplacian 행렬의 eigen-value decomposition은 아래와 같습니다.</p>

<script type="math/tex; mode=display">\mathbf {L} = \mathbf {U^{\intercal}\Lambda U} \tag{18}</script>

<p>이를 이용한 graph fourier transform과 inverse graph fourier transform은 아래와 같습니다.</p>

<script type="math/tex; mode=display">\mathcal {F}(\mathbf x) = \mathbf {U^{\intercal}x} \tag{19}</script>

<script type="math/tex; mode=display">\mathcal {F^{-1}}(\hat {\mathbf {x}}) = \mathbf {U} \hat {\mathbf {x}} \tag{20}</script>

<blockquote>
  <p>inverse fourier transform은 frequency domain으로 변환된 signal을 다시 time domain으로 변환하는 것입니다.</p>
</blockquote>

<h3>Spectral Graph Convolution</h3>

<p>드디어 Spectral Graph Convolution 입니다. 꽤 많이 돌아왔으나, Spectral Graph Convolution을 이해하기 위해서 필요한 Fourier Transform, Graph Laplacian, Graph Fourier Transform을 살펴봤습니다. 그렇다면 이제까지 배운 개념을 가지고 Spectral Graph Convolution이 어떻게 작동하는지 알아보겠습니다.</p>

<h5>convolution theorem</h5>

<p>입력과 출력이 있는 시스템에서, 출력 값은 현재의 입력에만 영향을 받는 것이 아니라 이전의 입력값에도 영향을 받습니다. <span style="text-decoration:underline">따라서 이전의 값까지의 영향을 고려하여 시스템의 출력을 계산하기 위한 연산이 convolution입니다.</span> 어떤 시스템에 입력신호가 들어가서 출력신호가 있다고 했을 때, 출력신호는 입력신호와 시스템함수의 convolution 연산을 통해서 나오는 것입니다.</p>

<p align="center"><img width="400" src="https://imgur.com/dk7otIh.png" /><figcaption align="center">그림 13. convolution</figcaption></p>

<script type="math/tex; mode=display">(f*g)(t) = \int_{-\infty}^{\infty}f(\tau)g(t-\tau)d\tau \tag{21}</script>

<p>convolution의 특징은 <span style="text-decoration:underline">시스템의 출력으로 ‘시스템의 특성’을 알 수 있다는 점입니다.</span> &lt;그림 13.&gt;에서 시스템의 특성이 담긴 시스템 함수가 위와 같은 모양이라면, 출력도 시스템 함수와 유사한 모양으로 나옵니다.</p>

<p>&lt;그림 14&gt;와 같은 모양을 가진 시스템 함수라면, 출력도 시스템 함수와 유사한 모양이 나옵니다.</p>

<p align="center"><img width="400" src="https://t1.daumcdn.net/cfile/tistory/11297E0F4CFB6C3721" /><figcaption align="center">그림 14. convolution(2)</figcaption></p>

<p>이 개념을 적용한 것이 CNN에서의 filter에 해당되는 것입니다. 이미지 영역내에서의 convolution은 filter라는 시스템에 이미지라는 신호를 입력하여 filter에 해당되는 특징을 출력해 내는 것입니다.</p>

<p align="center"><img width="400" src="https://imgur.com/wvuTh7M" /><figcaption align="center">그림 15. 이미지와 특정 filter의 convolution 연산 수행 후</figcaption></p>

<p>convolution의 기본 개념에 대해서 대략적으로 살펴봤습니다. graph convolution은 그렇다면 우리는 어떤 graph의 특징을 추출해 낼 수 있는 filter를 학습으로 얻고 싶은 것입니다.</p>

<p>convolution theorem은 그럼 어떤 것일까요 ? ‘time 영역에서의 signal과 시스템 함수와의 convolution 연산은 각각 frequency 영역으로 변환한 뒤의 곱과 같다’가 바로 convolution theorem 입니다. ‘time 영역에서의 signal과 시스템 함수와의 convolution 연산은 각각 frequency 영역으로 변환한 뒤의 곱과 같다’가 바로 convolution theorem 입니다.</p>

<blockquote>
  <p>convolution in spatial/time domain is equivalent to multiplication in Fourier domain</p>
</blockquote>

<p align="center"><img src="https://imgur.com/hnw3IeK.png" /><figcaption align="center">그림 16. convolution theorem</figcaption></p>

<p>frequency 영역에서의 convolution은 단순 곱으로 계산될 수 있기 때문에 훨씬 편리합니다. 마찬가지로 graph 영역에서도 graph signal을 fourier transform으로 frequency 도메인으로 바꿔서 계산하면 마찬가지로 편리해집니다. 또한 노드와 가까이 있는 이웃노드에서부터 멀리 떨어져 있는 노드에서 오는 신호까지 모두 고려하여 graph signal의 특징을 추출할 수 있게 되는 것입니다. 이것이 바로 ‘spectral graph convolution’ 입니다.</p>

<blockquote>
  <p>멀리 떨어져 있는 노드에서 오는 신호까지 고려한다는 것이 사실 ‘convolution’ 입니다. convolution은 현재 출력 값은 현재 입력값 뿐만 아니라 이전 입력값에도 영향을 받는다는 것을 고려한 연산입니다. 즉, 멀리 떨어져 있는 노드에서 오는 신호는 비교적 최근에 온 신호가 될 것이고, 이웃노드에서 온 신호가 비교적 이전 시간에서 이미 영향을 준 신호입니다. 그러나 이를 모든 시간에 대해 분해에서 연산하기가 어렵기 때문에(시간 영역에서의 convolution 연산이 어렵기 때문에) frequency 영역으로 바꿔서 간편하게 계산을 하는 것입니다.</p>
</blockquote>

<p>따라서, spectral graph convolution 식은 아래와 같습니다.</p>

<script type="math/tex; mode=display">\mathbf {x} * G \mathbf {g} = \mathcal F^{-1}(\mathcal {F}(\mathbf {x}) \odot \mathcal {F}(\mathbf {g})) = \mathbf {U}(\mathbf {U^{\intercal}x} \odot \mathbf {U^{\intercal}g})</script>

<p>$\mathbf {g}_\theta = diag(\mathbf {U^{\intercal}g})$ 라 가정한다면(학습할 filter가 대각요소만 있다고 가정한다면), 위의 식은 아래와 같이 됩니다.</p>

<script type="math/tex; mode=display">\mathbf {x} * \mathbf {g}_\theta = \mathbf {U} \mathbf {g}_{\theta} \mathbf {U^{\intercal}x}</script>

<hr />

<p>이번 포스팅은 spectral graph convolution 연산을 이해하기 위해서 fourier transform, laplacian operator 와 graph fourier transform을 살펴보고, 마지막으로 spectral graph convolution을 설명하였습니다. <a href="https://ralasun.github.io/deep%20learning/2021/03/06/gcn(2)/">다음 포스팅</a>에서 이어서 spectral-based CNN에 대해서 살펴보도록 하겠습니다.</p>

<p>spectral-based CNN을 깊게 이해하기 위해 리서치를 하고 이해한 내용을 최대한 정리하였으나 전공 분야가 아니라 부족한 부분이 많을 거라고 예상됩니다. 혹시나 내용이 틀렸거나 문의가 있으시면 메일이나 댓글 달아주세요!</p>

<hr />

<ol>
  <li><a href="https://arxiv.org/abs/1609.02907">Kipf, Thomas N., and Max Welling. “Semi-supervised classification with graph convolutional networks.” arXiv preprint arXiv:1609.02907 (2016).</a></li>
  <li><a href="https://arxiv.org/abs/1812.08434">Zhou, Jie, et al. “Graph neural networks: A review of methods and applications.” arXiv preprint arXiv:1812.08434 (2018).</a></li>
  <li><a href="http://dsba.korea.ac.kr/seminar/?mod=document&amp;pageid=1&amp;keyword=spectral&amp;uid=1330">DSBA 연구실 세미나 자료, [Paper Review] MultiSAGE - Spatial GCN with Contextual Embedding</a></li>
  <li>푸리에 변환 참고 페이지
    <ul>
      <li><a href="https://darkpgmr.tistory.com/171">https://darkpgmr.tistory.com/171</a></li>
      <li><a href="https://www.math.ucla.edu/~tao/preprints/fourier.pdf">https://www.math.ucla.edu/~tao/preprints/fourier.pdf</a></li>
      <li><a href="https://angeloyeo.github.io/2019/10/11/Fourier_Phase.html">https://angeloyeo.github.io/2019/10/11/Fourier_Phase.html</a></li>
    </ul>
  </li>
  <li>Laplacian Operator
    <ul>
      <li><a href="https://angeloyeo.github.io/2019/08/25/laplacian.html">https://angeloyeo.github.io/2019/08/28/laplacian.html</a></li>
      <li><a href="https://towardsdatascience.com/spectral-graph-convolution-explained-and-implemented-step-by-step-2e495b57f801">https://towardsdatascience.com/spectral-graph-convolution-explained-and-implemented-step-by-step-2e495b57f801</a></li>
      <li><a href="https://en.wikipedia.org/wiki/Second_derivative">https://en.wikipedia.org/wiki/Second_derivative</a></li>
    </ul>
  </li>
  <li>Graph Fourier Transform
    <ul>
      <li><a href="https://arxiv.org/pdf/1211.0053">Shuman, David I., et al. “The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains.” IEEE signal processing magazine 30.3 (2013): 83-98.</a></li>
    </ul>
  </li>
  <li>spectral graph convolution
    <ul>
      <li><a href="https://trip2ee.tistory.com/101">https://trip2ee.tistory.com/101</a></li>
      <li><a href="https://m.blog.naver.com/PostView.nhn?blogId=namunny&amp;logNo=110183516999&amp;proxyReferer=https:%2F%2Fwww.google.com%2F">https://m.blog.naver.com/PostView.nhn?blogId=namunny&amp;logNo=110183516999&amp;proxyReferer=https:%2F%2Fwww.google.com%2F</a></li>
      <li><a href="https://arxiv.org/pdf/1312.6203.pdf%20http://arxiv.org/abs/1312.6203">Bruna, Joan, et al. “Spectral networks and locally connected networks on graphs.” arXiv preprint arXiv:1312.6203 (2013).</a></li>
    </ul>
  </li>
</ol>


    <article>
    <div class="post-more">
      
      <a href="/deep%20learning/2021/02/15/gcn/#disqus_thread"> <i class="fa fa-comments" aria-hidden="true"></i>Comment</a>&nbsp;
      
      <a href="/deep%20learning/2021/02/15/gcn/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/deep%20learning/2021/02/11/gcn/">
        Introduction to Graph Neural Network - GNN 소개 및 개념
      </a>
    </h1>

    <span class="post-date">11 Feb 2021</span>
     | 
    
    <a href="/blog/tags/#graph-neural-network" class="post-tag">graph-neural-network</a>
    
    

    <article>
      <p>이번 포스팅을 시작으로, Graph Neural Network(GNN)에 대해 본격적으로 다루도록 하겠습니다. 이번 포스팅은 Graph Neural Network가 나온 배경 및 기본적인 구조와 개념에 대해 다루도록 하겠습니다.</p>

<hr />

<p>우리가 흔히 많이 보는 데이터의 종류로는 이미지, 정형 데이터, 텍스트가 있습니다. 이미지는 2-D grid 형식인 격자 형식을 가지며, 정형 테이터는 테이블 형태를 띕니다. 또한 텍스트는 1-D sequence로 생각할 수 있습니다. 즉, 이들 데이터는 ‘격자’의 모양으로 표현할 수 있으며 이는 Euclidean space 상에 있는 것을 뜻합니다.</p>

<p align="center"><img src="https://imgur.com/0bBI5DP.png" /><figcaption align="center">그림 1. Euclidean space vs. Non-Euclidean space</figcaption></p>

<p align="center"><img width="300" src="https://imgur.com/wsEg0pl.png" /><figcaption align="center">그림 2. 3D mesh 이미지</figcaption></p>

<p>그러나, social network 데이터, molecular 데이터, 3D mesh 이미지 데이터(그림 2.)는 ‘비’ Eculidean space 데이터입니다. 그렇다면 기존 CNN과 RNN계열의 모델과 다르게 이런 형태의 데이터를 처리할 수 있는 새로운 모델이 필요합니다. 그것이 바로 Graph Neural Network 입니다.</p>

<h1>What is graph?</h1>

<p>GNN을 본격적으로 시작하기 전에 그래프에 대해서 알아보도록 하겠습니다. 그래프란 $G = (N, E)$ 로 구성된 일종의 자료 구조입니다. V는 노드들의 집합이고, E는 노드 사이를 연결하는 엣지들의 집합입니다. 노드에는 일반적으로 데이터의 정보가 담겨있고, 엣지는 데이터 간의 관계 정보가 포함되어 있습니다. 또한, 아래와 같은 그래프 형태를 ‘undirected graph’ 라고도 합니다.</p>

<p align="center"><img src="https://imgur.com/rRWSycm.png" /><figcaption align="center">그림 3. graph</figcaption></p>

<p><b>directed graph</b><br /></p>

<p align="center"><img width="300" src="https://imgur.com/HO2ho4k.png" /><figcaption align="center">그림 4. directed graph</figcaption></p>

<p>방향 그래프란 엣지가 방향성을 가지는 그래프입니다. 아래 그림에서 $V_2$ 에서 $V_1$ 으로 향하는 엣지 $e_1$ 이 있다면, $V_2$ 를 predecessor, $V_1$ 을 sucessor 라고 부릅니다. 그리고 $e_1$ 을 $V_2$ 의 outgoing edge, $V_1$ 의 incoming edge 라고 합니다.</p>

<p>그렇다면, 이러한 그래프를 네트워크의 인풋으로 넣기 위해선 행렬 형태로 표현해야 합니다. 따라서 그래프를 표현하기 위한 방법으로는 adjacency matrix, degree matrix, laplacian matrix가 있습니다.</p>

<p align="center"><img src="https://imgur.com/bYiaa4S.png" /><figcaption align="center">그림 5. degree vs. adjacency vs. laplacian</figcaption></p>

<p><b>Adjacency matrix</b><br /></p>

<p>adjacency 행렬은 그래프 노드의 개수가 N개라면, NxN 정사각 행렬입니다. i노드와 j노드가 연결되어 있으면 $A_{ij} = 1$ 아니면 $A_{ij} = 0$ 의 성분을 가집니다.</p>

<p><b>Degree matrix</b><br /></p>

<p>Degree 행렬은 그래프 노드의 개수가 N개라면 NxN 크기를 가지는 대각행렬입니다. 각 꼭짓점의 차수에 대한 정보를 포함하고 있는 행렬로, 꼭짓점의 차수란 꼭짓점와 연결된 엣지의 갯수를 말합니다.</p>

<script type="math/tex; mode=display">D_{i,j} = \begin{cases} deg(v_i) \quad if \,\, i=j \\
		                      0 \quad otherwise \end{cases}</script>

<p><b>Laplacian matrix</b><br /></p>

<p>adjacency 행렬은 노드 자신에 대한 정보가 없습니다. 그에 반해 laplacian 행렬은 노드와 연결된 이웃노드와 자기 자신에 대한 정보가 모두 포함된 행렬입니다. laplacian 행렬은 degree 행렬에서 adjacency 행렬을 빼준 것입니다.</p>

<script type="math/tex; mode=display">L = D - A</script>

<script type="math/tex; mode=display">L_{i,j} = \begin{cases}
		deg(v_i) \quad if \,\, i=j \\
		 -1 \quad if \,\, i \neq j \\
		  0 \quad otherwise \end{cases}</script>

<h1>Motivation : GNN $\approx$ CNN</h1>

<p>다시 GNN으로 돌아오겠습니다. GNN의 아이디어는 Convolutional Neural Network(CNN)에서 시작되었습니다. CNN은 아래와 같은 특징을 가지고 있습니다.</p>

<ul><li>local connectivity</li>
<li>shared weights</li>
<li>use of Multi-layer</li></ul>

<p>위와 같은 특징 때문에, CNN은 spatial feature를 계속해서 layer마다 계속해서 추출해 나가면서 고차원적인 특징을 표현할 수 있습니다. 위와 같은 특징은 마찬가지로 graph 영역에도 적용할 수 있습니다.</p>

<p align="center"><img src="https://imgur.com/qa04Jf2.png" /><figcaption align="center">그림 6. GNN $\approx$ CNN</figcaption></p>

<p><b>Local Connectivity</b><br /></p>

<p>&lt;그림 3.&gt; 을 보면, CNN과 GNN의 유사한 점을 확인할 수 있습니다. 먼저, graph도 한 노드와 이웃노드 간의 관계를 local connectivity라 볼 수 있기 때문에, 한 노드의 특징을 뽑기 위해서 local connection에 있는 이웃노드들의 정보만 받아서 특징을 추출할 수 있습니다. 즉, CNN의 filter의 역할과 유사합니다.</p>

<p><b>Shared Weights</b><br /></p>

<p>또한 이렇게 graph 노드의 특징을 추출하는 weight은 다른 노드의 특징을 추출하는데도 동일한 가중치를 사용할 수 있어(shared weight), computational cost를 줄일 수 있습니다.</p>

<p><b>Use of Multi-layer</b><br /></p>

<p>CNN에서 multi layer 구조로 여러 레이어를 쌓게 되면 초반에는 low-level feature위주로 뽑고, 네트워크가 깊어질수록 high level feature를 뽑습니다. <span style="color:red">graph같은 경우에 multi-layer구조로 쌓게되면 초반 layer는 단순히 이웃노드 간의 관계에 대해서만 특징을 추출하지만, 네트워크가 깊어질수록 나와 간접적으로 연결된 노드의 영향력까지 고려된 특징을 추출할 수 있게 됩니다.</span></p>

<p>그렇다면, 위와 같은 특성을 가지려면 GNN은 어떻게 인풋 그래프에 대하여 연산을 해야하는지 알아보도록 하겠습니다.</p>

<h1>Original Graph Neural Network</h1>

<p>graph neural network는 <a href="https://www.infona.pl/resource/bwmeta1.element.ieee-art-000004700287">Scarselli et al.의 The Graph Neural Network Model</a>에서 처음 등장했습니다. GNN의 목적은 결국 이웃노드들 간의 정보를 이용해서 해당 노드를 잘 표현할 수 있는 특징 벡터를 잘 찾아내는 것입니다. 이렇게 찾아낸 특징 벡터를 통해 task를 수행할 수 있습니다(graph classification, node classification 등).</p>

<p align="center"><img width="400" src="https://imgur.com/eDqPQFW.png" /><figcaption align="center">그림 7. GNN</figcaption></p>

<p>GNN의 동작은 따라서 크게 두가지로 생각할 수 있습니다.</p>

<ol>
  <li>propagation step - 이웃노드들의 정보를 받아서 현재 자신 노드의 상태를 업데이트 함</li>
  <li>output step - task 수행을 위해 노드 벡터에서 task output를 출력함</li>
</ol>

<p>이를 수식으로 표현하면 아래와 같습니다.</p>

<script type="math/tex; mode=display">x_n = f_w(l_n, l_{co[n]}, x_{ne[n]}, l_{ne[n]})</script>

<script type="math/tex; mode=display">o_n = g_w(x_n, l_n)</script>

<p>이때, $l_n, l_{co[n]}, x_{ne[n]}, l_{ne[n]}$ 은 각각 n 노드의 라벨, n노드와 연결된 엣지들의 라벨, 이웃노드들의 states, 이웃노드들의 라벨입니다. 또한 $f_w$ 와 $o_w$ 는 각각 propagation function(논문에선 transition function 이라 표현함)와 output function입니다.</p>

<p>propagation function(transition function)은 이웃 노드들의 정보와 노드와 연결된 엣지정보들을 토대로 현재 자신의 노드를 표현합니다. 즉, d-차원의 공간에 이러한 인풋들을 받아서 맵핑하는 과정이라 생각할 수 있습니다. output function은 task 수행을 위해 학습을 통해 얻은 node feature을 입력으로 하여 output을 얻습니다. 예를 들어, node label classification 이라면 node label이 아웃풋이 될 것입니다.</p>

<h3>Learning algorithm : Banach fixed point theorem</h3>

<p>그렇다면 어떻게 학습이 이뤄질까요 ? 위에서 Motivation : GNN $\approx CNN$ 에서 Multi-layer를 GNN에 사용하면 얻는 이점은 layer가 깊어질수록 직접적으로 연결된 이웃 노드 이외에 멀리 있는 노드들의 영향력을 고려하여 현재 노드의 feature를 구성할 수 있다고 하였습니다.</p>

<blockquote>
  <p>이렇게 멀리있는 노드에서부터 현재 노드까지 정보가 전달되는 과정을 message passing이라고 합니다. message passing이란 개념은 GNN이 등장하고 난 이후에, Gilmer et al.의 “Neural message passing for quantumchemistry” 에서 등장하였습니다. 해당 논문은 여러 종류의 GNN 구조를 일반화하는 프레임워크를 message passing 이라는 것으로 제안한 논문입니다.</p>
</blockquote>

<p>하지만, 초기 GNN은 multi-layer 구조가 아니기 때문에 불가능합니다. 따라서, Banach fixed point theorem에 따라, iterative method로 고정된 node feature를 찾습니다.</p>

<p align="center"><img width="400" src="https://imgur.com/UIfPnoL.png" /><figcaption align="center">그림 8. Network obtained by unfolding the encoding network</figcaption></p>

<p>$x_n$ 와 $o_n$ 이 어떤 수렴된 값을 가지려면, Banach fixed point theorem에 의하면 propagation function이 contraction map이어야 합니다.</p>

<blockquote>
  <p>$F_w$ is a <em>contraction map</em> with respect to the state, i.e., there exists $\mu$ , $0 \leq \mu \le 1$ , such that $|F_w(x, l) - F_w(y, l)| \leq \mu |x-y|$ holds for any x, y where $| \cdot |$ denotes a vectorial norm.</p>
</blockquote>

<p><b>Contraction Map에 대한 개인적인 생각</b><br />
사실 contraction map의 수학적인 이해가 완벽하게 되진 않았습니다. 그러나, 제가 생각하는 contraction map은 다음과 같습니다. 선형대수학에서 선형변환을 진행하면, m차원의 벡터가 n차원의 공간으로 맵핑이 됩니다. 이 때, 서로 다른 두 m차원의 벡터가 n차원의 공간으로 맵핑이 되었을 때, 두 벡터 사이의 거리가 줄어드는 방향이라면 이 맵핑 function은 contraction map입니다.</p>

<p>그렇다면 fixed point가 되려면, 즉 수렴된 node feature들은 contraction map에 의해 정의된 공간 안에서 존재하는 것이고, 어떻게 보면 node feature를 서치하는 범위가 작다라고 생각할 수 있습니다.</p>

<p>이러한 문제 때문에 추후 다양한 버전의 GNN은 이러한 제한된 가정을 두지 않고 우리가 딥러닝 네트워크 학습시 사용하는 방식으로 node feature 값을 찾습니다. 즉, node feature의 search space가 훨씬 넓어지는 것입니다.</p>

<p>다시 돌아와서, 그렇다면 iterative method식으로 수식을 전개하면 아래와 같이 전개할 수 있습니다.</p>

<script type="math/tex; mode=display">x_n(t+1) = f_w(l_n, l_{co[n]}, x_{ne[n]}(t), l_{ne[n]})</script>

<script type="math/tex; mode=display">o_n(t) = g_w(x_n(t), l_n), \quad n \in N</script>

<p>fixed 된 $x_n, o_n$ 을 얻으면 아래와 같은 loss를 계산할 수 있고, gradient 계산을 통해 weight을 업데이트합니다. 여기서 weight은 $F_w$ 의 파라미터 입니다. neural network 라면 network의 가중치가 됩니다.</p>

<h1>Variants of GNNs</h1>

<p>Scarselli의 GNN 이후로 여러 변형된 GNN이 많이 등장하였습니다. 초기 GNN은 학습 방식의 단점에 의해 수렴이 잘 되지 않는다는 문제가 있습니다. 이러한 문제를 해결하기 위해 초기 GNN 이후에 다양한 GNN이 등장하였습니다. 대표적으로 Graph Convolutional Network와 Gated Graph Neural Network 등이 있습니다.</p>

<p>다음 포스팅부터는 GNN이 더욱 더 유명해진 계기가 된 Graph Convolutional Network에 대해 다루도록 하겠습니다. 읽어주셔서 감사합니다.</p>

<hr />

<ol>
  <li><a href="https://arxiv.org/abs/1812.08434">Zhou, Jie, et al. “Graph neural networks: A review of methods and applications.” arXiv preprint arXiv:1812.08434 (2018).</a></li>
  <li>What is graph?, <a href="https://ratsgo.github.io/data%20structure&amp;algorithm/2017/11/18/graph/">https://ratsgo.github.io/data%20structure&amp;algorithm/2017/11/18/graph/</a></li>
  <li><a href="https://www.infona.pl/resource/bwmeta1.element.ieee-art-000004700287">Scarselli, F., et al. “The Graph Neural Network Model.” IEEE Transactions on Neural Networks 1.20 (2009): 61-80.</a></li>
</ol>

    <article>
    <div class="post-more">
      
      <a href="/deep%20learning/2021/02/11/gcn/#disqus_thread"> <i class="fa fa-comments" aria-hidden="true"></i>Comment</a>&nbsp;
      
      <a href="/deep%20learning/2021/02/11/gcn/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/deep%20learning/2021/01/18/tada/">
        TADA, Trend Alignment with Dual-Attention Multi-Task Recurrent Neural Networks for Sales Prediction 논문 리뷰
      </a>
    </h1>

    <span class="post-date">18 Jan 2021</span>
     | 
    
    <a href="/blog/tags/#time-series-analysis" class="post-tag">time-series-analysis</a>
    
    

    <article>
      <hr />

<p>다변량 시계열 예측 모델에 관한 논문으로, 다변량 시계열 데이터를 가지고 encoder-decoder RNN 모델 기반으로 dual-attention과 multi-task RNN으로 구성된 모델입니다.</p>

<hr />

<h2>Problem</h2>
<p>다변량 시계열 예측을 위한 여러 통계 기반 모델링이 있으나, 판매량에 영향을 주는 변수들 간의 관계를 파악하기 어렵고, 이 변수들로 부터 의미있는 정보(contextual information)을 추출하는 건 더욱 어렵습니다. 예를 들어, 겨울의복은 날씨에 영향에 두드러지게 받지만, 일반적인 셔츠는 사계절내내 잘 입는 옷이기 때문에 겨울의복보단 계절의 영향을 덜 받습니다. 또한 소비자의 주관적인 선호도(브랜드 선호도, 상품 선호도 등)에 따라 상품 판매는 크게 또한 달라지게 됩니다. 따라서, 본 논문에서 주목하는 다변량 시계열 예측에서의 문제는 아래와 같이 크게 세가지입니다.</p>

<ol><li>how to fully capture the dynamic dependencies among multiple influential factors?<br />
판매에 영향을 주는 여러 변수들 간의 관계는 시간에 따라 변할 가능성이 높습니다. 그렇다면 매 스텝마다 변수들 간의 관계를 어떻게 포착할 수 있을까요 ?</li><li>how can we possibly glean wisdoms form the past to compensate for the unpredictability of influential factors?<br />이 변수들이 미래에 어떻게 변할지는 아무도 모릅니다. 그렇다면 과거 이 변수들의 정보만을 가지고 어떻게 미래를 눈여겨 볼 수 있는 정보를 추출할지는 생각해 봐야 합니다.</li>
<li>how to align the upcoming trend with historical sales trends?<br />현실 시계에서의 판매 트랜드는 전혀 규칙적이지 않습니다. 그렇다면 과거 판매 트렌드를 어떻게 하면 현실 트렌드와 연관지을 수 있을까요 ?</li></ol>

<h2>TADA : Trend Alignment with Dual-Attention Multi-Task RNN</h2>

<h3>Problem Formulation</h3>

<p>본 논문에서 풀고자 하는 다변량 시계열 예측 문제는 아래와 같이 수학적으로 정의됩니다.</p>

<script type="math/tex; mode=display">\{\hat{y_t\}}^{T+\triangle}_{t=T+1} = F({\{\mathbf x_t\}}^{T}_{t=1}, {\{y_t\}}^{T}_{t=1})</script>

<p>$\mathbf x_t$ 는 influential factors로 판매량 이외의 변수(ex. 날씨, 브랜드, 상품인덱스 등)이고, $y_t$ 는 판매량 입니다.</p>

<h3>TADA 모델 개요</h3>
<p align="center"><img src="https://imgur.com/w09ZSHF.png" /><figcaption align="center">그림 1. 모델 개요</figcaption></p>

<p>위의 그림은 본 논문의 모델 개요입니다. 크게 아래와 같이 구성되어 있습니다.</p>
<ul>
  <li>Multi-task based Encoder Structures</li>
  <li>Dual-Attention based Decoder Structures
    <ul>
      <li>Attention got weighted decoder input mapping</li>
      <li>attention for trend alignment</li>
    </ul>
  </li>
</ul>

<h4>Multi-task based Encoder Structures</h4>
<p align="center"><img src="https://imgur.com/leH0yfV.png" /><figcaption align="center">그림 2. Multi-task based Encoder</figcaption></p>

<p>influential factor의 semantic한 특징을 잘 추출한다면 분명 예측에 도움이 될 것입니다. 그러나 매 타임 스텝마다 어떻게 하면 판매량 예측에 도움될 semantic한 특징을 추출할 수 있을까요 ? 본 논문에서는 influential factor를 크게 intrinsic한 속성과 objective한 속성으로 나누어 LSTM을 이용한 인코딩을 각각 따로하였습니다. 이를 통해 각각 두 개의 LSTM(intrinsic LSTM, external LSTM)을 통해 각기 다른 semantic한 특징을 추출할 수 있습니다. 따라서, 위의 문제 정의는 아래와 같이 다시 정의될 수 있습니다.</p>

<script type="math/tex; mode=display">\{\hat{y_t\}}^{T+\triangle}_{t=T+1} = F({\{\mathbf x_t^{int}\}}^{T}_{t=1}, {\{\mathbf x_t^{ext}\}}^{T}_{t=1}, {\{y_t\}}^{T}_{t=1})</script>

<p>intrinsic한 속성이란 브랜드, 카테고리, 가격등 상품과 관련된 것이고, objective한 속성은 날씨, 휴일유무, 할인등과 관련된 속성입니다. 아래 표는 논문에서 실험한 데이터의 intrinsic/objective 속성입니다.</p>

<p>하지만 우리가 구하고 싶은 건 두 가지의 다른 semantic한 feature를 적절하게 결합하여 의미있는 <strong>contextual vector</strong>를 만드는 것입니다. 따라서 또다른 LSTM 네트워크인 Synergic LSTM을 구축하여 joint representation을 학습합니다. 이때, Synergic LSTM에 입력으로 들어가는 건 각 타임스텝에 해당되는 $h_t^{int}$ 와 $h_t^{ext}$ 뿐만 아니라 판매량 $y_t$ 도 같이 joint space가 구축되도록 학습됩니다.</p>

<p>먼저, 두 타입스텝 t에서의 두 개의 hidden state을 $h_t^{int}$ 와 $h_t^{ext}$ 이용하여 Synergic LSTM의 인풋인 $\mathbf X_t^{syn}$ 을 아래와 같이 계산합니다.</p>

<script type="math/tex; mode=display">\mathbf X_t^{syn} = \mathbf W_{syn}[\mathbf h_t^{int};\mathbf h_t^{ext};y_t]</script>

<p>그런 다음, intrinsic LSTM/external LSTM과 동일하게 각 타임스텝마다 두 정보가 결합되어 인코딩된 hidden stated인 $\mathbf h^{con}_t$ 를 계산합니다.</p>

<p align="center"><img src="https://imgur.com/ijwajF4.png" /><figcaption align="center">그림 3. Multi-task based Encoder(2)</figcaption></p>

<h4>Dual-Attention based Decoder Structures</h4>
<p>Multi-task Encoder를 통해 과거 판매량 시계열 데이터를 인코딩하면 contextual vectors인 ${\mathbf h_t^{con}}^T_{t=1}$ 이 계산되어 나옵니다. $h_t^{con}$ 은 타임스텝 t까지의 시계열 데이터에 대한 contextual 정보를 품고 있습니다.</p>

<p>LSTM decoder도 encoder와 유사하게 예측에 필요한 contextual vector $\mathbf d_t^{con}$ 을 생성합니다. 따라서, $T &lt; t \leq T + \Delta$ 에 대해 decoder 수학식은 아래와 같습니다.</p>

<script type="math/tex; mode=display">\mathbf d_t^{con} = LSTM^{dec}(\mathbf x_t^{dec}, \mathbf d^{con}_{t-1})</script>

<p>위 식에서 $\mathbf x_t^{dec}$ 는 attention weighted input입니다. 그러면 contextual vector가 어떻게 만들어지는지 보기 전에 attention weighted input 계산 과정을 살펴봅시다.</p>

<h5>Attention for Weighted Decoder Input Mapping</h5>

<p align="center"><img src="https://imgur.com/NvCkYMs.png" /><figcaption align="center">그림 4. Attention for Weighted Decoder Input</figcaption></p>

<p>Decoder에 입력될 Input은 encoder contextual vector들에서 각 디코터 타임 스텝에 필요한 정보를 적절하게 취하도록 하기 위해 attention 메카니즘을 통해 생성합니다.</p>

<script type="math/tex; mode=display">\mathbf x_t^{dec} = \mathbf W_{dec}\left[\sum_{t'=1}^T \alpha_{tt'}^{int}\mathbf h_{t'}^{int};\sum_{t'=1}^T \alpha_{tt'}^{ext}\mathbf h_{t'}^{ext}\right] + \mathbf b_{dec}</script>

<p>$\alpha_{tt’}^{int}$ 와 $\alpha_{tt’}^{ext}$ 는 어텐션 가중치를 의미합니다. 어텐션 가중치는 아래 과정을 통해 계산됩니다.</p>

<script type="math/tex; mode=display">e^{int}_{tt'} = \mathbf v^{\mathrm T}_{int}tanh(\mathbf M_{int}\mathbf d_{t-1}^{con} + \mathbf H_{int}\mathbf h_{t'}^{int})</script>

<script type="math/tex; mode=display">e^{ext}_{tt'} = \mathbf v^{\mathrm T}_{ext}tanh(\mathbf M_{int}\mathbf d_{t-1}^{con} + \mathbf H_{ext}\mathbf h_{t'}^{ext})</script>

<script type="math/tex; mode=display">\alpha_{tt'}^{int} = \frac{exp(e_{tt'}^{int})}{\sum_{s=1}^{T}exp(e_{ts}^{int})}</script>

<script type="math/tex; mode=display">\alpha_{tt'}^{ext} = \frac{exp(e_{tt'}^{ext})}{\sum_{s=1}^{T}exp(e_{ts}^{ext})}</script>

<p>이때, $\sum_{t’=1}^{T}\alpha_{tt’}^{int} = \sum_{t’=1}^{T}\alpha_{tt’}^{ext} = 1$ 이 여야 합니다.</p>

<h5>Attention for Trend Alignment</h5>

<p align="center"><img src="https://imgur.com/riUczJ9.png" /><figcaption align="center">그림 5. Attention for Trend Alignment</figcaption></p>

<p>미래를 예측하기 위해선 과거의 trend 패턴을 안다면 좀 더 수월할 수 있습니다. 따라서, 미래에 예상되는 패턴과 유사한 패턴을 과거에서 찾는 작업을 attention을 통해 진행하는 과정을 본 논문에서 제안하였습니다. 그러나, 
일반적으로 attention 메카니즘은 현 타임스텝에서 아웃풋을 출력하기 위해 이전 hidden state들중에서 가장 align되는 정보를 선택합니다. 과거 정보들 중에서 <strong>미래의 트렌드와 유사한 트렌드 정보</strong>를 선택적으로 이용하고 싶다면 전통적인 attention 메카니즘을 그대로 사용하기는 어렵습니다. 왜냐하면, 일반적인 데이터에선 trend외에 노이즈도 많이 포함하고 있기 때문입니다. 즉, 전체 데이터에 trend + noise라서 이전 모든 과거들에서 유사한 trend 패턴만을 집중하는 건 힘듭니다. 따라서 논문 저자는 아래와 같은 방법을 고안하였습니다.</p>

<p>먼저, ${\mathbf h_t^{con}}_{t=1}^T$ 를 $\triangle$ 타임 스텝 크기에 해당되는  contextual vector를 이어붙여서 $\triangle$ -step trend vector를 생성합니다.</p>

<p align="center"><img src="https://imgur.com/yUmHRP0.png" /></p>

<p>$\mathbf p_i$ 는 과거 시계열 데이터에서 $\triangle$ 간격에 해당되는 구간의 트렌드를 나타냅니다. $i$ 가 1씩 증가하므로, 마치 슬라이딩 윈도우 1씩 움직이면서 트렌드를 포착하는 것과 유사합니다.</p>

<p>마찬가지 방식으로 decoder hidden state들을 이어 붙여 미래에 예상될 트렌드 정보를 생성합니다.</p>

<p align="center"><img src="https://imgur.com/I9C1wnQ.png" /></p>

<p>따라서, 그림5 처럼 과거에 생성된 트렌드 벡터과 미래 트렌드 벡터를 각각 내적하여 가장 큰 값에 해당되는 인덱스 i를 반환합니다. 내적값이 가장 크다는 것은 가장 유사함을 의미합니다.</p>

<script type="math/tex; mode=display">e_i^{trd} = \mathbf p_i^{\mathrm T} \tilde{\mathbf p}</script>

<script type="math/tex; mode=display">i' = argmax(e_i^{trd} , e_{i+1}^{trd},\dots, e_{T+\triangle -1}^{trd})</script>

<p>그 다음 $\mathbf p_{i’}$ 내의 각 ${\mathbf d_t^{con}}$ 와 $\mathbf h_t^{con}$ 을 아래와 같은 계산과정을 거쳐서 $\tilde {\mathbf d}^{con}$ 을 생성합니다.</p>

<p align="center"><img src="https://imgur.com/haL8Udd.png" /></p>

<p>$\tilde {\mathbf d}^{con}$ 은 타임스텝 t에서의 과거 유사한 트렌드 정보에 집중하여 생성된 aligned contextual vector 입니다.</p>

<h4>Sales Prediction and Model Learning</h4>

<p>위에서 생성된 aligned contextual vector ${\widetilde {\mathbf d_t}^{con}}$ 를 가지고 판매량을 예측합니다.</p>

<script type="math/tex; mode=display">\hat y_t = \mathbf v_y^{\mathrm T} \mathbf {\widetilde d}^{con} + b_y</script>

<p>$\hat y_t , \,\,(T+1 \leq t \leq T+\Delta)$ 는 타임스텝 T에서의 예측된 판매량입니다.</p>

<p>본 논문에서 학습은 L2 regularization과 함께 Mean Squared Error를 minimize하였습니다.</p>

<h2>Experiment and result.</h2>

<p>전체적인 결과에 관한 건 논문을 참고 바랍니다. trend alignment 부분에 대한 결과를 살펴보면 과거 유사하다고 찾은 trend와 예측된 trend는 아래 그래프와 같이 나왔습니다.</p>

<p align="center"><img src="https://imgur.com/F5w1W2o.png" /></p>
<p>보면 어느정도 과거 매치된 트렌드와 유사한 트렌드를 따르는 것을 확인할 수 있었습니다.</p>

<h2>Lessons Learned</h2>
<ul>
  <li>결과를 보면 어느정도 lag가 발생하는 것으로 보입니다.</li>
  <li>trend에 대한 파악을 먼저하고 판매량 데이터 입력을 나중에 하면 어떨까?</li>
  <li>dual-stage attention에서의 input attention 모듈과 multi-tasked encoder를 결합하는 건 어떨까 ??</li>
</ul>

<hr />

<p>이상으로 본 논문 리뷰를 마치겠습니다.</p>

<hr />

<ol>
  <li>Chen, Tong, et al. “Tada: trend alignment with dual-attention multi-task recurrent neural networks for sales prediction.” 2018 IEEE International Conference on Data Mining (ICDM). IEEE, 2018.</li>
</ol>

    <article>
    <div class="post-more">
      
      <a href="/deep%20learning/2021/01/18/tada/#disqus_thread"> <i class="fa fa-comments" aria-hidden="true"></i>Comment</a>&nbsp;
      
      <a href="/deep%20learning/2021/01/18/tada/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/deep%20learning/2021/01/18/dual-stage-attention/">
        A Dual-Stage Attention-Based Recurrent Neural Network for Time-Series Prediction 논문 리뷰
      </a>
    </h1>

    <span class="post-date">18 Jan 2021</span>
     | 
    
    <a href="/blog/tags/#time-series-analysis" class="post-tag">time-series-analysis</a>
    
    

    <article>
      <p>A Dual-Stage Attention-Based Recurrent Neural Network는 다변량 시계열 예측 모델입니다(Multi-Variate Time Series Prediction). Bahdanau et al.의 <a href="https://arxiv.org/abs/1409.0473">Attention network 기반 시퀀스 모델</a> 을 베이스로, 인코더 뿐만 아니라 디코더에도 Attention netowork를 이용해 예측을 위한 다변량 시계열 변수 간 상대적인 중요도와 타임 스텝 간 상대적인 중요도를 모두 고려한 모델입니다.</p>

<h2>Problem</h2>


    <article>
    <div class="post-more">
      
      <a href="/deep%20learning/2021/01/18/dual-stage-attention/#disqus_thread"> <i class="fa fa-comments" aria-hidden="true"></i>Comment</a>&nbsp;
      
      <a href="/deep%20learning/2021/01/18/dual-stage-attention/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/blog/page2">Older</a>
  
  
    <span class="pagination-item newer">Newer</span>
  
</div>


      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script>
      (function(document) {
        var toggle = document.querySelector('.sidebar-toggle');
        var sidebar = document.querySelector('#sidebar');
        var checkbox = document.querySelector('#sidebar-checkbox');

        document.addEventListener('click', function(e) {
          var target = e.target;

          if (target === toggle) {
            checkbox.checked = !checkbox.checked;
            e.preventDefault();
          } else if (checkbox.checked && !sidebar.contains(target)) {
            /* click outside the sidebar when sidebar is open */
            checkbox.checked = false;
          }
        }, false);
      })(document);
    </script>
    
    <script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-166283746-1', 'auto');
ga('send', 'pageview');
    </script>
    
  </body>
  
  <script id="dsq-count-scr" src="//ralasun-github-io.disqus.com/count.js" async></script>
  
</html>
