---
layout : post
title: MDP를 푸는 방법, Dynamic Programming 
category: reinforcement learning
tags: cs234 reinforcement-learning david-silver sutton
---

지난 [MDP 포스팅](https://ralasun.github.io/reinforcement%20learning/2020/07/12/mdp/)에 이어서, 이번 포스팅은 MDP를 iterative하게 푸는 방법 중 하나인 Dynamic Programming(DP)에 대해서 다룹니다. CS234 2강, Deep Mind의 David Silver 강화학습 강의 3강, Richard S. Sutton 교재 Reinforcement Learning: An Introduction의 Chapter 4 기반으로 작성하였습니다. 또한, 대부분 수식 표기법은 Sutton 교재를 따랐습니다.
***

일반적으로 Dynamic Programming란 복잡한 문제를 간단한 여러 개의 문제로 나누어 푸는 방법을 말합니다. 지난 시간에서 벨만 방정식(벨만 기대 방정식, 벨만 최적 방정식)은 recursive한 관계를 가지고 있기 때문에, 벨만 방정식을 풀기 위한 솔루션으로 DP 사용이 적합하다고 할 수 있습니다.
<p align='center'>
<img width='500' src='https://user-images.githubusercontent.com/37501153/87278417-5e998980-c51f-11ea-9be2-b1b55ad4c070.jpeg'>
<figcaption align='center'>그림 1. Dynamic programming 조건</figcaption></p>

따라서, 상태 $s \in S$, 행동 $a \in A$, 보상 $r \in R$ 인 환경 모델 $p(s',s|r,a)$ 을 아는 상황에서, 벨만 기대 방정식과 벨만 최적 방정식의 recursive한 성질을 이용하여 최적 가치 함수 $v_\ast, q_\ast$ 를 구하는 것이 Dynamic Programming을 이용한 MDP 를 푸는 것입니다.
<p align='center'>
<img width='700' src='https://user-images.githubusercontent.com/37501153/87279033-08c5e100-c521-11ea-8ccc-de84ed44d45f.jpeg'>
<figcaption align='center'>그림 2. 벨만 방정식 : Recursive 관계</figcaption></p>
<blockquote>MDP문제는 환경모델을 완벽하게 아는 상황이기 때문에, dynamic programming은 'reinforcement learning'이 아니라 'planning' 방법입니다.</blockquote>
DP설명은 finite MDP에 유한하여 설명하도록 하겠습니다. 일반적으로 continuous MDP문제는 DP방법이 아닌 다른 방법을 이용하여 풀기 때문입니다.

지난 [강화학습 소개[2] 포스팅](https://ralasun.github.io/reinforcement%20learning/2020/07/11/introRL(2))에서, sequential decision making 문제 종류로 evaluation(prediction)과 control을 소개하였습니다. evaluation은 일정 정책 아래, 기대보상을 추정하여 현재 따르는 정책의 좋고/나쁨을 평가하는 것입니다. 즉, 현재 정책의 평가가 되는 것입니다. control은 정책들의 평가를 기반으로 최적의 정책을 찾는 것입니다. evaluation과 control은 독립적인 과정이 아니라 서로 연계되어 있는 과정이라 하였습니다. 마찬가지로 Dynamic Programing도 evaluation에 해당하는 Policy Evaluation과 control에 해당하는 Policy Improvement로 구성됩니다. 각각에 대해 알아봅시다.
<blockquote>DP설명은 finite MDP에 유한하여 설명하도록 하겠습니다. 일반적으로 continuous MDP문제는 DP방법이 아닌 다른 방법을 이용하여 풀기 때문입니다.</blockquote>

<h1>Policy Evaluation</h1>
Policy evaluation은 벨만 기대 방정식을 이용하여 iterative한 방법으로 현 정책 아래의 가치함수를 구하는 과정입니다. 아래 벨만 기대 방정식을 

$$v_\pi(s)=\sum_a\pi(a|s)\sum_{s',r}p(s',r|s,a)[r+\gamma v_\pi(s')]$$

update rule의 관계를 가진 방정식으로 취급한 뒤, 

$$v_{k+1}(s)=\sum_a\pi(a|s)\sum_{s',r}p(s',r|s,a)[r+\gamma v_{k}(s')]$$

k=0부터 수렴할 때까지 반복적으로 계산하는 것입니다. 즉 가치 함수를 초기화한 후, $v_0 \to v_1 \to v_2 \to \cdots \to v_\pi$ 으로 수렴할 때까지 <span style="color:red"><b>모든 상태에 대해서 동시에 업데이트</b></span>하는 것입니다. 
<p align='center'> 
<img width='500' src='https://i.imgur.com/WzCwUj1.jpg'>
<figcaption align='center'>그림 3. Iterative Policy Evaluation</figcaption></p>
이를 back-up diagram으로 다시 표현해 봅시다. 
<p align='center'> 
<img width='500' src='https://imgur.com/OKUnBIF.jpg'>
<figcaption align='center'>그림 4. Back-up diagram for iterative policy evaluation</figcaption></p>
그림 4.를 보시면, k 스텝에서 다음 상태를 이용하여 k+1 스텝의 현재 상태를 업데이트합니다. 또한 업데이트되는 방식은 다음 상태에서 나올 수 있는 누적보상의 가중 평균으로 계산됩니다(벨만 기대 방정식이기 때문입니다).

<blockquote>To produce each successive approximation, $v_{k+1}$ from $v_{k}$, iterative policy evaluation applies the same operation to each state s: it replaces the old value of s with a new value obtained form the old values of the successor states of s and the expected immediate rewards, along all the one-step transitions possible under the policy being evaluated. - Sutton and Barto, Reinforcement Learning : An Introduction</blockquote>

아래 그리드월드 예제로, policy evaluation을 살펴봅시다. 
<p align='center'>
<img width='500' src='https://imgur.com/XQHYowT.jpg'>
<figcaption align='center'>그림 5. 그리드월드 예제</figcaption></p>
처음 시작 상태에서 여러 경로를 다니다가 회색색깔에 도착하면 끝나는 게임이 있다고 합시다. 각 상태마다 받는 보상은 -1이고, 행동 좌,우,위,아래 방향에 대해 갈 확률은 0.25라 한다면, k=0, k=1, k= $\infty$ 을 수렴할 때까지 반복하면 각 $v_k$ 에 대해 각 상태의 가치함수 값은 아래 그림과 같습니다.
<p align='center'>
<img width='400' src='https://i.imgur.com/usVVNHF.jpg'>
<img width='400' src='https://imgur.com/44Y2N5r.jpg'> 
<figcaption align='center'>그림 6. 그리드월드 예제 - policy evaluation</figcaption></p>
왼쪽 행은 가치함수 결과이고, 오른쪽 행은 각 가치함수에서 greedy한 전략을 보여줍니다. 그러나 위 예제같은 경우는 간단한 케이스이어서 빨리 수렴에 도달합니다. 일반적으로 상태 집합의 크기 $|S|$ 가 큰 경우, 수렴할 때까지의 속도가 매우 느릴 수도 있기 때문에, 아래 알고리즘과 같이 어느 정도 수렴조건을 만족하면 다음 스텝으로 넘어가는 방법을 주로 택합니다.
<p align='center'>
<img width='500' src='https://imgur.com/l5QDFUd.jpg'>
<figcaption align='center'>그림 6. Policy evaluation 알고리즘</figcaption></p>

<h1>Policy Improvement</h1>
결국 현재 정책을 평가하는 이유는 더 나은 정책을 찾기 위한 것입니다. 그렇다면 현재 정책 평가한 것을 기반으로 어떻게 더 나은 정책을 찾는지 알아보도록 하겠습니다. 

임의의 정책 $\pi$ 아래 policy evaluation을 통해 $v_\pi$ 를 구했다고 하겠습니다. 그림 6.에서 처럼 수렴된 $v_\pi$ 에 대한 greedy policy가 있을 것입니다. 하지만 그 greedy policy 이외의 다른 행동 $a$ 을 선택하고, 즉, $a \neg \pi(s)$ 하고, 기존 정책 $\pi$ 를 따른다고 했을 때, 

$$q_\pi(s,a) = \sum_{s',r}p(s',r|s,a)[r+\gamma v_\pi(s')]$$

기존 정책에 따른 $v_\pi(s)$ 보다 크다면 새로 선택된 행동 a가 발전된 전략일 것입니다(policy improvement). 

$$q_\pi(s, \pi'(s)) \geq v_\pi(s)$$

<span style='color:gray'>처음에, 이 부분을 혼자 공부할 때, 이해하기 어려웠던 부분이 'greedy policy 이외의 다른 행동 $a$ 를 선택하고 기존 정책을 따른다는 부분'이었습니다. 저는 이 부분은 아래와 같이 이해하였습니다.</span>
<p align='center'>
<img src='https://imgur.com/n7Z1Gy4.jpg'>
<figcaption align='center'>그림 7. </figcaption></p>
그러나, 지난 [MDP 포스팅](https://ralasun.github.io/reinforcement%20learning/2020/07/12/mdp/)에서 더 나은 정책이 되려면 $q_\pi(s, \pi'(s)) \geq v_\pi(s)$ 가 아닌 $v_{\pi}(s) \geq v_{\pi'}(s)$ 를 만족해야 합니다. 이를 유도하는 수학적 증명은 아래와 같습니다.
<p align='center'>
<img src='https://imgur.com/OLBeIIc.jpg'>
<figcaption align='center'>그림 8.</figcaption></p>
따라서, greedy하게 policy improvement하는 방식을 수식으로 깔끔하게 정리하면 

$$\pi'(s) = arg \underset a max\,q_\pi(s,a)$$

$$= arg \underset a max\sum_{s',r}p(s',r|s,a)[r + \gamma v_\pi(s')]$$

입니다. 즉 기존 정책에서 발전된 새로운 정책 $\pi'$ 가 되었습니다.





***

1. [CS234 Winter 2019 course Lecture 2](http://web.stanford.edu/class/cs234/slides/lecture2.pdf)
2. [Richard S. Sutton and Andre G. Barto : Reinforcement Learning : An Introduction](http://incompleteideas.net/book/bookdraft2017nov5.pdf)
3. [David Silver Lecture 3](https://www.davidsilver.uk/wp-content/uploads/2020/03/DP.pdf)
4. [위키백과, 동적계획법](https://ko.wikipedia.org/wiki/동적_계획법)