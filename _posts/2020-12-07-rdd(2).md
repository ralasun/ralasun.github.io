---
layout : post
title: RDD, Resilient Distributed Dataset에 대하여[2]
category: Spark Programming
tags: data-engineering, spark
---

이번 포스팅은 지난 포스팅 <2-1. RDD, Resilient Distributed Dataset> 에 이어서 진행하도록 하겠습니다. 교재는 빅데이터 분석을 위한 스파크2 프로그래밍을 참고하였습니다. 

***

<h1>2. RDD</h1>

<h2>2.1.1 들어가기에 앞서</h2>
지난 포스팅 [2-1. RDD Resilient Distributed Dataset에 대하여](https://ralasun.github.io/spark%20programming/2020/11/20/rdd/) 에서 다뤘습니다. 

<h2>2.1.2. 스파크컨텍스트 생성</h2>
- 스파크 컨텍스트는 스파크 애플리케이션과 클러스터의 연결을 관리하는 객체임
- 따라서, 스파크 애플리케이션을 사용하려면 무조건 스파크 컨텍스트를 생성하고 이용해야 함
- RDD 생성도 스파크컨텍스트를 이용해 생성 가능함

```Scala
val conf = new SparkConf().setMaster(“local[*]”).setAppName(“RDDCreateSample”)
val sc = new SparkContext(conf)
```
<figcaption align='center'>[예] scala - sparkcontext 생성</figcaption> 

```Python
 sc = SparkContext(master=“local[*], appName=“RDDCreateTest”, conf=conf)
```
<figcaption align='center'>[예] python - spark context 생성</figcaption> 

- 스파크 동작에 필요한 여러 설정 정보 지정 가능함
- SparkConf(), conf=conf 부분에서 config을 통과시켜서 지정 가능함
- 지정해야 하는 정보 중에, master 정보와 appName 정보는 필수 지정 정보임
	- master 정보란 ? 스파크가 동작할 클러스터의 마스터 서버를 의미하는 것. 로컬모드에서 local, local[3], local[\*\]와 같이 사용. [\*\]는 쓰레드 개수를 의미하며, *는 사용 가능한 모든 쓰레드를 이용하겠다는 이야기임
	- appName은 애플리케이션 이름으로, 구분하기 위한 목적임. 스파크 UI화면에 사용됨

<h2>2.1.3. RDD생성</h2>
- RDD를 생성하는 방법은 크게 2가지임

<ol><li> 드라이버 프로그램의 컬렉션 객체 이용</li>
<ul><li>
자바 or 파이썬 ? 리스트 이용, 스칼라 ? 시퀀스타입 이용</li>
<li>드라이버 프로그램?<br>
	- 최초로 메인 함수를 실행해 RDD등을 생성하고 각종 연산을 호출하는 프로그램<br>
	- 드라이버 내의 메인 함수는 스파크 애플리케이션과 스파크 컨텍스트 객체를 생성함<br>
	- 스파크 컨텍스트를 통해 RDD의 연산 정보를 DAG스케쥴러에 전달하면 스케쥴러는 이 정보를 가지고 실행 계획을 수립한 후 이를 클러스터 매니저에게 전달함<br></li>

<pre lang='Scala'><code>
val rdd1 = sc.parallelize(List("a","b","c","d","e"))</code></pre>
<figcaption align='center'>[예] scala - rdd 생성</figcaption> 

<p align='center'><img src='https://imgur.com/MxtJV7I.png'><figcaption align='center'>[예] python - 드라이버의 컬렉션 객체를 이용한 RDD 생성</figcaption></p>

<li>
문자열을 포함한 컬렉션 객체 생성 example) python : ['a','b','c','d']</li>
<li>parallelize() 메서드를 이용해 RDD 생성<br> - RDD의 파티션 수를 지정하고 싶을 때,  parallelize() 메서드의 두 번째 매개변수로 파티션 개수 지정 가능</li>

<pre lang='scala'><code>
val rdd1 = sc.parallelize(1 to 1000, 10)
</code></pre></ul>

<li>외부 데이터를 읽어서 새로운 RDD를 생성</li> 
<ul>
<li>기본적으로 하둡의 다루는 모든 입출력 유형 가능</li>
<li>내부적으로 하둡의 입출력을 사용하기 때문임</li></ul>

<p align='center'><img src='https://imgur.com/eByLARc.png'><figcaption align='center'>[예] python - 외부데이터를 이용한 RDD 생성</figcaption></p></ol>

<h2>2.1.4 RDD 기본 액션</h2>
기본 액션 연산의 종류에 대해 알아보도록 하겠습니다.

<h3>1. collect</h3>
- collect은 RDD의 모든 원소를 모아서 배열로 리턴
- <b>반환 타입이 RDD가 아닌 배열</b>이므로 액션 연산
- RDD에 있는 모든 요소들이 서버의 메모리에 수집됨. 즉, 대용량 데이터를 다룰 땐 조심하고, 주로 작은 용량의 데이터 디버깅용으로 사용함

<p align='center'><img src='https://imgur.com/a1uu1V0.png'><figcaption align='center'>[예] python - collect 연산</figcaption></p>

<h3>2. count</h3>
- RDD 구성하는 전체 요소 개수 반환

<h2>2.1.5 RDD 트랜스포메이션</h2>
기존 RDD를 이용해 새로운 RDD를 생성하는 연산입니다.

<h3>1. Map 연산</h3>
- RDD에 속하는 <b>모든 요소에 적용</b>하여 새로운 RDD 생성하는 연산
- RDD의 몇몇 연산은 특정 데이터 타입에만 적용 가능함

<h4>1.1. map</h4>
- <b>하나의 인자를 받는 함수 자체</b>가 map의 인자로 들어감
- 이 함수를 이용해 rdd의 모든 요소에 적용한 뒤 새로운 RDD 리턴
- 
<p align='center'><img src='https://imgur.com/1nFR0IH.png'><figcaption align='center'>[예] python - map 연산</figcaption></p>

- map()에 전달되는 함수의 입력 데이터 타입과 출력 데이터 타입이 일치할 필요 없음. 문자열을 입력받아 정수로 반환하는 함수 사용 가능

<p align='center'><img src='https://imgur.com/lHGsvGD.png'><figcaption align='center'>[예] python - map 연산, 입력/출력 일치하지 않는 경우</figcaption></p>

<h4>1.2. flatMap</h4>
- map()과 마찬가지로, 하나의 인자를 받는 함수가 flatMap의 인자로 들어감
- map()과 차이점은 각 함수의 인자가 반환하는 값의 타입이 다름
- flatMap()에 사용하는 함수 f는 반환값으로 리스트나 시퀀스 같은 여러 개의 값을 담은 (이터레이션이 가능한) 일종의 컬렉션과 유사한 타입의 값을 반환해야 함
	- map[U]\(f:(T) -> U\):RDD\[U\]
	- flatMap[U](f:(T) -> TraversableOnce\[U\]\):RDD\[U\])\
		- TraversableOnce는 이터레이터 타입을 의미

<h5>map()과 flatMap() 차이점 예시</h5>
<p align='center'><img src='https://imgur.com/hQ7sfCG.png'><figcaption align='center'>[예] python - map 연산 vs. flatMap 연산</figcaption></p>

- map연산은 문자열의 배열로 구성된 RDD를 생성함
- 각 요소의 문자열(T)이 단어가 포함된 배열(U)이기 때문임
- 반면, flatMap 연산은 문자열로 구성된 RDD를 생성함
- TraversableOnce(U)이기 때문에 문자열의 배열 내의 요소가 모두 끄집어져 나오는 작업을 하게 됨
- flatMap()은 하나의 입력값("apple, orange")에 대해 출력 값이 여러개인 경우(["apple", "orange"]) 유용하게 사용할 수 있음

<h4>1.3. mapPartitions</h4>
- map과 flatMap은 하나의 인자만을 받는 함수가 인자로 들어가지만, mapPartitions은 여러 인자를 받는 함수가 인자로 들어갈 수 있음 ex) 이터레이터를 인자로 받는 함수
- mapartitions은 인자로 받은 함수가 파티션 단위로 적용하여 새로운 RDD를 생성함. 반면에, map과 flatMap은 인자로 받은 함수가 요소 한개 단위로 적용됨

<p align='center'><img src='https://imgur.com/bAIoEqG.png'><figcaption align='center'>[예] python - mapPartitions</figcaption></p>

- sc.parallelize(range(1,11),3)으로 파티션 3개로 나뉨
- DB 연결!!! 가 세번 출력된 걸 보니 파티션 단위로 처리한 것을 확인할 수 있음
- increase함수는 각 파티션 내의 요소에 대한 이터레이터를 전달받아 함수 내부에서 파티션의 개별 요소에 대한 작업을 처리하고 그 결과를 다시 이터레이터 타입으로 반환

<h4>1.4. mapPartitionsWithIndex</h4>
- mapPartions와 동일하고 다른 점은 인자로 전달되는 함수를 호출할 때 파티션에 속한 요소의 정보뿐만 아니라 해당 파티션의 인덱스 정보도 함께 전달해 준다는 것임

```Python
def IncreaseWithIndex(idx, numbers):
	for i in numbers:
		if(idx == 1):
			yield i+1
```

- mapPartitionswithIndex에 인자로 들어갈 함수는 위와 같이 인덱스 정보도 같이 들어감

<p align='center'><img src='https://imgur.com/wO8pOFo.png'><figcaption align='center'>[예] python - mapPartitionsWithIndex</figcaption></p>

<h4>1.5. mapValues</h4>
- RDD의 모든 요소들이 키와 값의 쌍을 이루고 있는 경우에만 사용 가능한 메서드이며, 인자로 전달받은 함수를 "값"에 해당하는 요소에만 적용하고 그 결과로 구성된 새로운 RDD를 생성

```Python
rdd1 = sc.parallelize(["a","b","c"]).map(lambda v:(v,1)) //(키,값)으로 구성된 rdd생성
rdd2 = rdd1.mapValues(lambda i:i+1)
print(rdd2.collect())
```

<p align='center'><img src='https://imgur.com/aFNZfuf.png'><figcaption align='center'>[예] python - mapValues</figcaption></p>

<h4>1.6. flatMapValues</h4>
- MapValues 처럼 키에 해당되는 값에 함수를 적용하나 flatMap() 연산을 적용할 수 있음

```Python
rdd1 = sc.parallelize([(1, "a,b"),(2, "a,c"),(3, "d,e")])
rdd2 = rdd1.flatMapValues(lambda v:v.split(','))
rdd2.collect()
```

<p align='center'><img src='https://imgur.com/5J8kSE1.png'><figcaption align='center'>[예] python - flatMapValues</figcaption></p>

<h3>2. 그룹화 연산</h3>
- 특정 조건에 따라 요소를 그룹화하거나 특정 함수 적용

<h4>2.1. zip</h4>
- 두 개의 서로 다른 RDD를 각 요소의 인덱스에 따라 첫번째 RDD의 '인덱스'번째를 키로, 두번째 RDD의 '인덱스'번째를 값으로 하는 순서쌍을 생성
- 두 개 RDD는 같은 개수의 파티션과 각 파티션 당 요소개수가 동일해야 함

```Python
rdd1 = sc.parallelize(["a","b","c"])
rdd2 = sc.parallelize([1,2,3])
rdd3 = rdd1.zip(rdd2)
rdd3.collect()
>>> [('a', 1), ('b', 2), ('c', 3)]
```

```Python
rdd1 = sc.parallelize(range(1,10),3)
rdd2 = sc.parallelize(range(11,20),3)
rdd3 = rdd1.zip(rdd2)
rdd3.collect()
>>> [(1, 11), (2, 12), (3, 13), (4, 14), (5, 15), (6, 16), (7, 17), (8, 18), (9, 19)]
```

<h4>2.2. zipPartitions</h4>
- zip()과 다르게 파티션의 개수만 동일하면 됨
- zipPartitions()은 최대 4개 RDD까지 인자로 넣을 수 있음
- 파이썬 사용불가!!

<h4>2.3. groupBy</h4>
- RDD의 요소를 <b>일정한 기준</b>에 따라 그룹을 나누고, 각 그룹으로 구성된 새로운 RDD를 생성함
- 각 그룹은 키와 각 키에 속한 요소의 시퀀스(iterator)로 구성됨
- 인자로 전달하는 함수가 각 그룹의 키를 결정하는 역할을 담당함


```Python
>>> rdd1 = sc.parallelize(range(1,11))
>>> rdd2 = rdd1.groupBy(lambda v: "even" if v%2==0 else "odd") 
/// groupBy에 인자로 전달된 함수에 의해 키(even/odd) 결정

print(rdd2.collect())
>>> [('even', <pyspark.resultiterable.ResultIterable object at 0x7fbc3f090e80>), ('odd', <pyspark.resultiterable.ResultIterable object at 0x7fbc3f090470>)]
/// 각 키에 해당하는 값은 iterator임을 확인할 수 있음

>>> for x in rdd2.collect():
...     print(x[0], list(x[1]))
... 
even [2, 4, 6, 8, 10]
odd [1, 3, 5, 7, 9]
```

<h4>2.3. groupByKey</h4>
- 이미 키와 값의 쌍으로 구성된 RDD에만 적용 가능함

```Python
>>> rdd1 = sc.parallelize(["a","b","c","b","c"]).map(lambda v:(v,1))
>>> rdd1.collect()
[('a', 1), ('b', 1), ('c', 1), ('b', 1), ('c', 1)]
/// (키,값) 쌍으로 구성된 RDD 생성

>>> rdd2 = rdd1.groupByKey()
>>> rdd2.collect()
[('b', <pyspark.resultiterable.ResultIterable object at 0x7fbc3f0ab6a0>), ('c', <pyspark.resultiterable.ResultIterable object at 0x7fbc3f0ab6d8>), ('a', <pyspark.resultiterable.ResultIterable object at 0x7fbc3f0ab0b8>)]
/// 키에 따라 그룹화함. 그 결과 키에 해당하는 시퀀스 생성

>>> for x in rdd2.collect():
...     print(x[0], list(x[1]))
... 
b [1, 1]
c [1, 1]
a [1]
```

<h4>2.4. cogroup</h4>
- 이미 키와 값의 쌍으로 구성된 RDD에만 적용 가능함
- 여러 개의 RDD를 인자로 받음(최대 3개)
- 여러 RDD에서 동일한 키에 해당하는 요소들로 구성된 시퀀스를 만든 후, (키, 시퀀스)의 튜플을 구성. 그 튜플들로 구성된 새로운 RDD를 생성함
- Tuple(키, Tuple(rdd1요소들의 집합, rdd2요소들의 집합, ...))

```Python
>>> rdd1 = sc.parallelize([("k1","v1"),("k2","v2"),("k1","v3")])
>>> rdd2 = sc.parallelize([("k1","v4")])
>>> rdd1.collect()
[('k1', 'v1'), ('k2', 'v2'), ('k1', 'v3')]
>>> rdd2.collect()
[('k1', 'v4')]
/// (키, 값)쌍으로 구성된 RDD 2개 생성

>>> rdd3 = rdd1.cogroup(rdd2)
>>> rdd3.collect()
[('k1', (<pyspark.resultiterable.ResultIterable object at 0x7fbc3f0b2fd0>, <pyspark.resultiterable.ResultIterable object at 0x7fbc3f00a828>)), ('k2', (<pyspark.resultiterable.ResultIterable object at 0x7fbc3f00ac18>, <pyspark.resultiterable.ResultIterable object at 0x7fbc3f00a5f8>))]
///k1 키에 대해 rdd1의 v1과 v3요소를 묶고, rdd2의 v4요소를 묶어서 튜플로 구성

>>> for x in rdd3.collect():
...     print(x[0], list(x[1][0]), list(x[1][1]))
... 
k1 ['v1', 'v3'] ['v4']
k2 ['v2'] []
```

<h3>3. 집합 연산</h3>
- RDD에 포함된 요소를 하나의 집합으로 간주하여 집합 연산을 수행(합/교집합)
- 
<h4>3.1. distinct</h4>
- RDD의 원소에서 중복을 제외한 요소로만 새로운 RDD 구성

```Python
>>> rdd = sc.parallelize([1,2,3,1,2,3,1,2,3])
>>> rdd2 = rdd.distinct()
>>> rdd2.collect()
[1, 2, 3]
```

<h4>3.1. cartesian</h4>
- 두 RDD요소의 카테시안곱을 구하고 그 결과를 요소로 하는 새로운 RDD구성

```Python
>>> rdd1 = sc.parallelize([1,2,3])
>>> rdd2 = sc.parallelize(['a','b','c'])
>>> rdd3 = rdd1.cartesian(rdd2)
>>> rdd3.collect()
[(1, 'a'), (1, 'b'), (1, 'c'), (2, 'a'), (2, 'b'), (2, 'c'), (3, 'a'), (3, 'b'), (3, 'c')]
```

<h4>3.2. subtract</h4>
- rdd1.subtract(rdd2) : (rdd1의 요소집합 - rdd2의 요소집합)의 차집합
- rdd2.subtract(rdd1) : (rdd2의 요소집합 - rdd1의 요소집합)의 차집합

```Python
>>> rdd1 = sc.parallelize(["a", "b","c","d","e"])
>>> rdd2 = sc.parallelize(["d","e"])
>>> rdd3 = rdd1.subtract(rdd2)
>>> rdd3.collect()
['a', 'b', 'c']  
```

<h4>3.2. union</h4>
- 두 RDD요소의 합집합

```Python
>>> rdd1 = sc.parallelize(["a", "b","c","d","e"])
>>> rdd2 = sc.parallelize(["d","e"])
>>> rdd3 = rdd1.union(rdd2)
>>> rdd3.collect()
['a', 'b', 'c', 'd', 'e', 'd', 'e']
```

<h4>3.3. intersection</h4>
- 두 RDD요소의 교집합으로 중복되지 않은 요소로 구성

```Python
>>> rdd1 = sc.parallelize(["a","a","b","c"])
>>> rdd2 = sc.parallelize(["a","a","c","c"])
>>> rdd3 = rdd1.intersection(rdd2)
>>> rdd3.collect()
['a', 'c']
```

<h4>3.4. join</h4>
- RDD의 구성요소가 키와 값의 쌍으로 구성된 경우에 사용할 수 있는 메서드
- 공통된 키에 대해서만 join수행
- join 수행 결과 Tuple(키, Tuple(첫번째 RDD요소, 두번쨰 RDD요소))

```Python
>>> rdd1 = sc.parallelize(["a", "b","c","d","e"]).map(lambda v : (v,1))
>>> rdd1.collect()
[('a', 1), ('b', 1), ('c', 1), ('d', 1), ('e', 1)]
>>> rdd2 = sc.parallelize(["b","c"]).map(lambda v:(v,2))
>>> rdd2.collect()
[('b', 2), ('c', 2)]
>>> rdd3 = rdd1.join(rdd2)
>>> rdd3.collect()
[('b', (1, 2)), ('c', (1, 2))]
```

<h4>3.5. leftOuterJoin, rightOuterJoin</h4>
- 키와 값의 쌍으로 구성된 RDD에 사용가능
- leftjoin, rightjoin을 수행

```Python
>>> rdd1 = sc.parallelize(["a", "b","c","d","e"]).map(lambda v : (v,1))
>>> rdd2 = sc.parallelize(["b","c"]).map(lambda v:(v,2))
>>> rdd3 = rdd1.leftOuterJoin(rdd2)
>>> rdd3.collect()
[('a', (1, None)), ('e', (1, None)), ('b', (1, 2)), ('c', (1, 2)), ('d', (1, None))]
///rdd2에는 a,d,e 키가 없기 때문에 해당 키에 대한 튜플 요소는 (rdd1의 요소, None)으로 구성됨

>>> rdd4 = rdd1.rightOuterJoin(rdd2)
>>> rdd4.collect()
[('b', (1, 2)), ('c', (1, 2))]
```

<h4>3.6. subtractByKey</h4>
- 키와 값의 쌍으로 구성된 RDD에 사용가능
- rdd1의 요소 중에서 rdd2와 겹치지 않는 키로 구성된 새로운 RDD 생성

```Python
>>> rdd1 = sc.parallelize(["a","b"]).map(lambda v:(v,1))
>>> rdd2 = sc.parallelize(["b"]).map(lambda v:(v,1))
>>> rdd3 = rdd1.subtractByKey(rdd2)
>>> rdd3.collect()
[('a', 1)]
```

<h3>4. 집계와 관련된 연산들</h3>

<h4>4.1 reduceByKey</h4>
- 키와 값의 쌍으로 구성된 RDD에서 사용 가능
- RDD 내의 동일한 키를 하나로 병합해 (키,값) 쌍으로 구성된 새로운 RDD 생성
- 함수를 인자로 받음. 
- 왜냐하면, 파티션 별로 연산을 수행했을 때, 항상 같은 순서로 연산이 수행되는 것을 보장 못하므로, 함수가 수행하는 연산은 교환법칙과 결합법칙이 성립해야 함

```Python
>>> rdd = sc.parallelize(['a','b','b']).map(lambda v:(v,1))
>>> rdd.collect()
[('a', 1), ('b', 1), ('b', 1)]
>>> rdd2 = rdd.reduceByKey(lambda v1, v2:(v1+v2))
>>> rdd2.collect()
[('b', 2), ('a', 1)]
```

<blockquote><b>(키,값)쌍으로 하는 RDD를 인자로 받는 트랜스포메이션 메서드</b><br>
- 데이터 처리 과정에서 사용할 파티셔너와 파티션 개수를 지정할 수 있는 옵션이 있음
- 자체적으로 작성한 파티셔너나 파티션 개수를 통해 병렬 처리 수준 변경 가능
</blockquote>

<h4></h4>














