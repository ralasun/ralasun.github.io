---
layout : post
title: Reinforcement Learning 소개[2]
category: Reinforcement Learning
tags: reinforcement-learning
---

이번 포스팅은 [강화학습 소개[1]](https://ralasun.github.io/reinforcement%20learning/2020/07/11/introRL(1)/)에 이어서 진행합니다. CS234 1강, Deep Mind의 David Silver 강화학습 강의 1강, Richard S. Sutton 교재 Reinforcement Learning: An Introduction의 Chapter 1 기반으로 작성하였습니다. 

***

지난 포스팅에서, 강화학습의 특성과 강화학습 문제를 정의하기 위해 필요한 요소, 강화학습 문제의 종류에 대해서 알아봤습니다. 에이전트는 에이전트의 상태를 가지고 어떻게 좋은 행동을 선택할 수 있을까요 ? 이 문제에 대한 답을 하기 전에, 에이전트가 상태 이외에 어떠한 요소를 가지고 있어야 하는지 알아봅시다. 

<h2>Major Components of an RL Agent</h2>
강화학습 에이전트를 구성하는 요소는 크게 정책(policy), 가치 함수(value function), 모델(Model)입니다.

<b>Policy</b><br>
정책은 에이전트의 행동전략(agent's behavior)입니다. 정책은 일종의 함수로, 주체의 상태를 행동으로 맵핑합니다. 정책의 종류는 deterministic policy $a = \pi(s)$ 와 stochastic policy와 $\pi(a|s) = P [A_t=a|S_t=s]$ 가 있습니다.
<blockquote>A policy is a map from state to action</blockquote>

아래와 같이 화성탐사기 예를 들어봅시다.
<p align='center'>
<img width='500' src='https://user-images.githubusercontent.com/37501153/87240302-5c69f900-c453-11ea-87ba-7b34cd3fbdb1.jpg'>
<figcaption align='center'>그림 1. 화성탐사기 예제</figcaption>
</p>
위의 그림과 같이, 화성탐사기가 도달할 수 있는 상태는 총 7가지 상태이고, 각 상태에서 취할 수 있는 행동은 왼쪽/오른쪽 두가지입니다. s1 상태에서 +1 보상을, s7 상태에서 +10 보상을, 나머지 상태에서 0의 보상을 받습니다. 

<p align='center'>
<img width='500' src='https://user-images.githubusercontent.com/37501153/87240430-82dc6400-c454-11ea-9687-455bbccff299.jpg'>
<figcaption align='center'>그림 2. 화성 탐사기 정책 예시</figcaption>
</p>
위 예는 화성 탐사가 가질 수 있는 정책 중 하나입니다(사실 이 예제는 너무 단순해서 이 정책이 최적 정책이긴 합니다). s7상태에서 가장 큰 보상을 받기 때문에 어떤 상태에서 시작하던 간에 오른쪽으로 가는 것이 최고의 정책이죠. 또한 이 정책의 특성은 deterministic입니다. 그 이유는 각 상태에서 나올 수 있는 모든 행동들의 가능성을 보여주는 것이 아니라, 한가지 행동만을 출력하기 때문입니다.

<b>Value Function</b><br>
다음은 가치 함수입니다. 가치 함수는 특정 정책 아래, 현재 에이전트 상태에서 앞으로 받을 미래 보상까지 고려한 누적보상 예측값입니다. 에이전트는 이 가치값을 기반으로 현 상태의 좋고 나쁨을 판단합니다. 또한 정책 간 가치함수를 비교를 통한 행동 선택의 기반이 되기도 합니다.

$$V_{\pi} = E_{\pi}[R_{t+1} + \gamma R_{t+2} + \gamma^2R_{t+3} + \dots|S_t=s]$$

$\gamma$ 는 discount factor로, 현재 보상과 미래 보상간의 중요도 차이를 보여줍니다. 추후에 더 자세히 설명하도록 하겠습니다. 아래는 가치함수의 예입니다.
<p align='center'>
<img width='500' src='https://user-images.githubusercontent.com/37501153/87240661-69d4b280-c456-11ea-8279-8c3fa528358e.jpg'>
<figcaption align='center'>그림 3. 화성 탐사기 가치함수 예시</figcaption>
</p>
화성 탐사 문제를 그림 1. 처럼 정의했을 때, 각 상태에서 가질 수 있는 화성 탐사 에이전트의 가치함수입니다.

<b>Model</b><br>
모델이란 환경(정확히, 주체가 영향을 받고 있는 환경)이 주체의 행동에 따라 어떻게 변하는지에 관한 모델입니다. 즉 환경의 동적모델(dynamic models of the environment)이죠. 
<blockquote>
<ul>
<li>A model predicts what the environment will do next</li>
<li>$P$ predicts the next state</li>
<li>$R$ predicts the next immediate reward</li>
</ul>
</blockquote>

$$P^a_{ss'} = P[S_{t+1}=s'|S_t=s, A_t=a]$$

$$R^a_s = E[R_{t+1}|S_t=s,A_t=a]$$

환경의 모델은 크게 두 가지가 있습니다. 변이모델(transition/dynamics model)과 보상입니다. 변이모델은 에이전트의 행동에 따라 다음 상태에 대한 정보를 알려줍니다. 보상모델은 에이전트가 선택한 행동에 대해 변이모델에 따라 다음 상태에 갔을 때 받는 즉각적인 보상입니다. 아래는 모델에 관한 화성탐사 예시입니다.
<p align='center'>
<img width='500' src='https://user-images.githubusercontent.com/37501153/87240816-e3b96b80-c457-11ea-90ce-96d711a65cc4.jpg'>
<figcaption align='center'>그림 4. 화성 탐사기 모델 예시</figcaption>
</p>

<h2>Categorizing RL Agents</h2>
에이전트를 구성하는 요소로는 모델, 가치함수, 정책임을 알았습니다. 그러나 사실 구성요소를 어떻게 조합하느냐에 따라 강화학습 에이전트의 종류를 몇가지로 나눌 수 있습니다.
<p align='center'>
<img width='300' src='https://user-images.githubusercontent.com/37501153/87240991-88887880-c459-11ea-881b-49b8789bde78.jpg'>
<figcaption align='center'>그림 5. RL Agent Taxonomy</figcaption></p>
<ul>
<li>Value Based</li>
에이전트가 행동을 선택할 때, 가치함수의 결과값이 가장 높은 쪽으로 선택하는 것입니다. 이때, 정책이 명시적으로 표현되지 않고 내재적으로 표현됩니다.
<li>Policy Based</li>
명시적인 정책을 가진 에이전트입니다. 즉, 특성 상태에서 어떤 행동이 확률적으로 높은지를 보고 행동하는 것입니다.
<li>Actor-Critic</li>
위 Value-based와 policy based가 합쳐진 상태입니다.
<li>Model Free</li>
변이확률과 보상함수에 대한 정보가 없는 경우입니다. 에이전트는 환경모델을 알 수 없으나 경험을 해나가면서 환경을 이해해 나가면서 문제를 해결하는 케이스입니다.
<li>Model Based</li>
환경 모델을 구축하여 문제를 푸는 케이스입니다.</ul>

<h2>Key Challenges in Learning to Make Sequences of Good Decisions</h2>
연속적인 의사 결정 문제는 문제의 상태에 따라 다르게 접근해야 합니다. 어떤 문제 같은 경우, 환경의 모델을 완벽하게 아는 경우가 있을 수 있습니다. 예를 들어, 무인 헬리콥터에서, 헬리콥터가 있는 환경에서 바람의 속도, 바람의 방향, 장애물의 위치, 날씨, 온도등 헬리콥터에 영향을 줄 수 있는 모든 요소들을 다 파악할 수 있으면 환경 모델을 완벽하게 아는 경우입니다. 이런 경우, 우리는 헬리콥터를 움직일 때마다 어떠한 결과를 초래하고 헬리콥터 움직임에 좋은지 나쁜지를 일일이 "계산"할 수 있습니다. 이런 경우를 Planning이라 합니다.
<p align='center'>
<img width='500' src='https://user-images.githubusercontent.com/37501153/87241351-46f9cc80-c45d-11ea-8551-ac02e8a60e28.jpg'>
<figcaption align='center'>그림 6. Planning</figcaption>
</p>
하지만, 대부분의 경우, 환경 모델을 완벽하게 아는 것은 불가능합니다. 따라서 환경에 직접 부딪혀 가면서(환경과 상호작용하면서) 경험을 통해 환경모델을 간접적으로 익히는 것입니다. 따라서 에이전트는 환경과의 상호작용을 통해 얻은 경험을 바탕으로 자신만의 전략을 구축해 나가는 것입니다. 포커 게임에서, 상대방이 가지고 있는 패나 전략을 알 순 없지만 상대방과 여러 번 게임을 통해 상대방 전략을 간접적으로 익힐 수 있습니다. 이렇게 푸는 방법이 "Reinforcement Learning"입니다.
<p align='center'>
<img width='500' src='https://user-images.githubusercontent.com/37501153/87241443-12d2db80-c45e-11ea-8d6d-dfda09341bfe.jpg'>
<figcaption align='center'>그림 7. Reinforcement Learning</figcaption>
</p>

<h2>Exploration and Exploitation</h2>
강화학습은 <b>환경과의 상호작용을 통해 경험을 쌓아가면서 에이전트의 전략을 스스로 구축해나가는 것</b>이라 하였습니다. 좋은 전략을 구축하기 위해, 실패도 해보고 성공도 해보면서 배워나가야 합니다. 그러나 어느 정도 경험을 했다면, 이 경험을 바탕으로 대략적인 전략을 구축해 나가야합니다. 이와 관련 강화학습 문제가 exploration과 exploitation입니다. 

exploration이란 추후에 에이전트가 더 좋은 결정을 내릴수도 있기 때문에 새로운 결정을 시도해보는 과정입니다. 반면에, exploitation은 이제까지의 경험을 바탕으로 결정을 내리는 과정입니다. 이 둘 사이는 trade-off관계에 있습니다. exploration을 많이 하면 새로운 시도로 좋은 의사 결정 전략을 구축할 수 있지만 반면에 지금 당장 받을 보상을 희생해야 합니다. 이 둘 사이의 trade-off관계에 관한 예를 들어봅시다. 외식을 하기 위해 식당을 선택하는 경우, 이제까지 경험을 바탕으로 좋아하는 식당을 가는 건 exploitation이고, 새로운 식당을 시도해 보는 건 exploration입니다. 이 외에 다른 예는 아래와 같습니다.
<p align='center'>
<img width='500' src='https://user-images.githubusercontent.com/37501153/87241596-c25c7d80-c45f-11ea-8454-ed31384441ef.jpg'>
<figcaption align='center'>그림 8. exploration and exploitation</figcaption>
</p>

<h2>Evalutation(Prediction) and Control</h2>
에이전트는 좋은 경험을 쌓기 위해서, exploration과 exploitation을 균형있게 활용해야합니다. 하지만  어떻게 활용하면서 경험을 쌓고, 전략을 구축하는 걸까요? 이제 관한 문제가 evalution과 control입니다.

evaluation은 일정 정책 아래, 기대보상을 추정하여 현재 따르는 정책이 좋고/나쁨을 평가하는 것입니다. 기대보상 추정은 결국 가치함수를 구하는 것입니다. 가치함수 $V_{\pi}(s)$ 는 현 정책을 따랐을 때, 상태 $s$ 의 가치입니다. 직접적으로는 현 상태의 가치지만 현 정책 아래에서 계산한 것이기 때문에 현 정책의 가치로도 생각할 수 있습니다. control은 최적정책을 찾는 것입니다. evalutation과정을 통해 정책의 가치를 평가했다면, 이 평가를 기반으로 더 나은 정책이 있는지를 찾는 과정입니다.  

evaluation과 control은 독립적인 과정이 아닙니다. evaluation을 해야 control을 하고, control을 해야 evalutation을 할 수 있습니다. 즉 서로 맞물려서 최적의 정책을 찾아 나가는 것입니다. 아래 그리드월드 예제를 들어봅시다.
<p align='center'>
<img width='500' src='https://user-images.githubusercontent.com/37501153/87241960-4b28e880-c463-11ea-9aea-94314e98f3e9.jpg'>
<figcaption align='center'>그림 9. evalutation(prediction)</figcaption></p>
<p align='center'>
<img width='500' src='https://user-images.githubusercontent.com/37501153/87241958-46643480-c463-11ea-8d86-772141a0e043.jpg'>
<figcaption align='center'>그림 10. control</figcaption>
</p>

그림 9에서 주어진 정책은 모든 행동(좌, 우, 위, 아래)으로 갈 확률이 0.25로 동일합니다. 이 정책 아래 각 상태(그리드월드 한 칸)의 가치를 평가하면 오른쪽 숫자로 채워진 테이블이 됩니다. 최적 정책을 최적 가치함수 기반으로 찾을 수 있습니다. 그림 10처럼 최적 가치함수가 가운데 표처럼 구해졌다고 합시다. 이 기반으로 구한 최적 정책은 오른쪽 표와 같습니다. A'상태에서의 최적 정책은 위로 올라가는 것입니다. 왜냐하면 A' 주변 가치함수 값들은 14.4, 17.8, 14.4입니다. A'에서 가장 높은 17.8의 가치를 가진 상태로 가는 것이 최적이기 때문입니다. 

***

이상으로, Reinforcement Learning 소개[2] 포스팅을 마치겠습니다. 다음 포스팅은 [Markov Decision Process](https://ralasun.github.io/reinforcement%20learning/2020/07/12/mdp/)을 알아보도록 하겠습니다.

***
1. [CS234 Winter 2019 course Lecture 1](http://web.stanford.edu/class/cs234/slides/lecture1.pdf)
2. [Richard S. Sutton and Andre G. Barto : Reinforcement Learning : An Introduction](http://incompleteideas.net/book/bookdraft2017nov5.pdf)
3. [David Silver Lecture 1](https://www.davidsilver.uk/wp-content/uploads/2020/03/intro_RL.pdf)





